-DOCSTART- O
With O
such O
a O
novel O
design O
, O
our O
model O
is O
able O
to O
deal O
with O
the O
problems O
of O
saliency O
and O
redundancy O
explicitly O
. O

We O
also O
introduce O
a O
decoder O
with O
a O
two O
- O
level O
attention O
mechanism O
, O
which O
firstly O
attends O
to O
the O
entity O
nodes O
, O
where O
the O
attention O
weights O
are O
then O
subsequently O
utilized O
to O
guide O
the O
attention O
to O
the O
text O
units O
. O

We O
introduce O
entity O
nodes O
in O
addition O
to O
text O
unit O
nodes O
to O
construct O
a O
heterogeneous O
graph O
, O
helping O
our O
model O
capture O
complicated O
relations O
between O
text O
units O
. O

Conclusion O
In O
this O
paper O
, O
we O
propose O
an O
entity O
- O
aware O
multidocument B-TaskName
summarization I-TaskName
model O
. O

Part O
of O
this O
work O
was O
done O
when O
Wei O
Lu O
was O
a O
visiting O
Professor O
at O
SJTU O
. O

This O
research O
work O
was O
supported O
by O
the O
National O
Natural O
Science O
Foundation O
of O
China O
( O
Grant O
No.61772337 O
, O
U1736207 O
) O
. O

5 O
Acknowledgments O
We O
thank O
all O
the O
anonymous O
reviewers O
for O
their O
valuable O
suggestions O
. O

These O
results O
show O
that O
our O
EMSum B-MethodName
model O
is O
able O
to O
generate O
summaries O
of O
higher O
quality O
than O
other O
models O
and O
further O
show O
the O
effectiveness O
of O
our O
proposed O
approach O
. O

The O
results O
are O
shown O
in O
Table O
5 O
. O

On O
the O
MultiNews B-DatasetName
dataset O
, O
we O
choose O
FT B-MethodName
, O
T B-MethodName
- I-MethodName
DMCA I-MethodName
, O
HS B-MethodName
, O
together O
with O
EMSum B-MethodName
. O

On O
the O
WikiSum B-DatasetName
dataset O
, O
we O
choose O
FT B-MethodName
, O
TDMCA B-MethodName
, O
HT B-MethodName
, O
EMSum B-MethodName
and O
conduct O
human B-MetricName
evaluation I-MetricName
to O
compare O
their O
performance O
. O

Ratings O
range O
from O
-1 B-MetricValue
( O
worst B-MetricValue
) O
to O
1 B-MetricValue
( O
best B-MetricValue
) O
. O

The O
rating O
of O
each O
system O
was O
computed O
as O
the O
percentage O
of O
times O
it O
was O
chosen O
as O
best O
minus O
the O
times O
it O
was O
selected O
as O
worst O
. O

Annotators O
are O
presented O
with O
the O
gold O
summary O
and O
summaries O
generated O
from O
3 O
out O
of O
4 O
systems O
and O
decide O
which O
summary O
is O
the O
best O
and O
which O
is O
the O
worst O
based O
on O
the O
criteria O
mentioned O
above O
. O

We O
used O
BestWorst B-MetricName
Scaling I-MetricName
( O
Louviere O
et O
al. O
, O
2015 O
) O
because O
it O
has O
been O
shown O
to O
produce O
more O
reliable O
results O
than O
rating O
scales O
( O
Kiritchenko O
and O
Mohammad O
, O
2017 O
) O
. O

( O
3 O
) O
Succinctness O
: O
does O
redundancy O
occur O
in O
the O
summary O
? O

( O
2 O
) O
Fluency O
: O
Is O
the O
summary O
fluent O
and O
grammatical O
? O

parts O
of O
the O
input O
? O

We O
would O
also O
like O
to O
apply O
our O
method O
to O
other O
tasks O
such O
as O
multidocument O
question O
answering O
( O
Joshi O
et O
al. O
, O
2017 O
) O
. O

In O
the O
future O
, O
we O
would O
like O
to O
explore O
other O
approaches O
such O
as O
reinforcement O
learning O
based O
methods O
( O
Sharma O
et O
al. O
, O
2019 O
) O
to O
further O
improve O
the O
summary O
quality O
in O
the O
context O
of O
multidocument B-TaskName
summarization I-TaskName
. O

Experiments O
on O
standard O
datasets O
show O
the O
effectiveness O
of O
our O
model O
. O

358 O

Following O
criteria O
used O
by O
previous O
work O
( O
Liu O
and O
Lapata O
, O
2019a O
) O
, O
the O
evaluation O
score O
takes O
three O
aspects O
into O
account O
: O
( O
1 O
) O
Informativeness O
: O
does O
the O
summary O
include O
salient O

We O
randomly O
sampled O
20 O
documents O
- O
summary O
pairs O
from O
the O
WikiSum B-DatasetName
test O
set O
and O
20 O
from O
the O
MultiNews B-DatasetName
test O
set O
, O
and O
invited O
3 O
participants O
to O
assess O
the O
outputs O
of O
different O
models O
independently O
. O

4.5 O
Human B-MetricName
Evaluation I-MetricName
We O
further O
employ O
human B-MetricName
evaluation I-MetricName
to O
assess O
model O
performance O
. O

Incorporating O
entity O
information O
to O
construct O
a O
heterogeneous O
graph O
network O
enables O
better O
information O
flowing O
between O
text O
nodes O
, O
and O
our O
design O
of O
the O
novel O
two O
- O
level O
attention O
mechanism O
in O
this O
task O
is O
indeed O
playing O
an O
important O
role O
towards O
the O
overall O
effectiveness O
of O
our O
approach O
. O

The O
results O
show O
the O
effectiveness O
of O
our O
new O
introduced O
module O
. O

Table O
4 O
shows O
the O
results O
. O

For O
experiments O
without O
two O
- O
level O
attention O
, O
we O
apply O
token O
- O
level O
attention O
directly O
, O
but O
attend O
to O
the O
entity O
cluster O
representation O
additionally O
, O
which O
is O
a O
naive O
way O
to O
incorporate O
entity O
information O
. O

For O
experiments O
without O
graph O
encoder O
module O
, O
we O
simply O
fix O
the O
entity O
cluster O
representation O
and O
paragraph O
representation O
after O
the O
multi O
- O
head O
pooling O
layer O
. O

Ablation O
Study O
To O
validate O
the O
effectiveness O
of O
individual O
components O
such O
as O
graph O
encoder O
module O
and O
two O
- O
level O
attention O
module O
, O
we O
conduct O
experiments O
of O
ablation O
studies O
. O

Finally O
, O
we O
choose O
k B-HyperparameterName
= O
10 B-HyperparameterValue
because O
it O
performs O
the O
best O
. O

We O
also O
conduct O
ablation O
studies O
to O
validate O
the O
effectiveness O
of O
different O
components O
of O
our O
model O
. O

We O
further O
conduct O
experiments O
to O
analyze O
the O
effects O
of O
the O
number B-HyperparameterName
of I-HyperparameterName
iterations I-HyperparameterName
and O
the O
number B-HyperparameterName
of I-HyperparameterName
paragraphs I-HyperparameterName
selected O
for O
attention O
. O

However O
, O
the O
results O
show O
us O
that O
t B-HyperparameterName
= O
3 B-HyperparameterValue
, O
4 B-HyperparameterValue
outperforms O
t B-HyperparameterName
= O
2 B-HyperparameterValue
on O
ROUGE B-MetricName
- I-MetricName
L I-MetricName
and O
the O
overall O
performance O
R̃ B-MetricName
fluctuates O
very O
little O
. O

R̃ B-MetricName
is O
the O
mean O
of O
R-1 B-MetricName
, O
R-2 B-MetricName
and O
R B-MetricName
- I-MetricName
L. I-MetricName
Model O
EMSum B-MethodName
w/o O
graph O
enc O
w/o O
two O
- O
level O
attn O
Analysis O

When O
k B-HyperparameterName
= O
20 B-HyperparameterValue
, O
that O
means O
we O
do O
not O
perform O
any O
cut O
- O
off O
but O
only O
modify O
the O
paragraph O
attention O
weights O
with O
the O
entity O
attention O
weights O
, O
so O
the O
performance O
is O
also O
reduced O
. O

As O
the O
results O
in O
the O
second O
block O
of O
Table O
3 O
show O
, O
when O
k B-HyperparameterName
= O
5 B-HyperparameterValue
, O
the O
number B-HyperparameterName
of I-HyperparameterName
attended I-HyperparameterName
paragraphs I-HyperparameterName
is O
relatively O
small O
thereby O
degrading O
performance O
heavily O
. O

We O
conduct O
experiments O
on O
WikiSum B-DatasetName
dataset O
when O
k B-HyperparameterName
= O
5 B-HyperparameterValue
, O
10 B-HyperparameterValue
, O
15 B-HyperparameterValue
, O
20 B-HyperparameterValue
. O

So O
we O
need O
to O
figure O
out O
how O
much O
salient O
information O
is O
enough O
for O
our O
model O
, O
namely O
the O
proper O
value O
of O
k. B-HyperparameterName

The O
attention O
weights O
over O
the O
entire O
long O
token O
sequence O
may O
be O
sparse O
. O

The O
Number B-HyperparameterName
of I-HyperparameterName
Paragraphs I-HyperparameterName
Selected O
for O
Attention O
At O
each O
decoding O
step O
, O
our O
two O
- O
level O
attention O
mechanism O
firstly O
computes O
weights O
over O
entity O
nodes O
to O
identify O
the O
most O
salient O
parts O
of O
source O
documents O
. O

Therefore O
we O
choose O
t B-HyperparameterName
= O
2 B-HyperparameterValue
finally O
. O

We O
argue O
the O
performance O
is O
limited O
by O
the O
number O
of O
introduced O
parameters O
. O

Intuitively O
, O
the O
more O
iterations O
the O
graph O
is O
updated O
, O
the O
more O
information O
is O
flowed O
across O
the O
nodes O
. O

The O
first O
block O
in O
Table O
3 O
shows O
the O
results O
. O

To O
this O
end O
, O
we O
conduct O
experiments O
on O
WikiSum B-DatasetName
dataset O
when O
t B-HyperparameterName
= O
1 B-HyperparameterValue
, O
2 B-HyperparameterValue
, O
3 B-HyperparameterValue
, O
4 B-HyperparameterValue
. O

The O
Number B-HyperparameterName
of I-HyperparameterName
Iterations I-HyperparameterName
We O
investigate O
how O
the O
number B-HyperparameterName
of I-HyperparameterName
iterations I-HyperparameterName
t B-HyperparameterName
influences O
the O
performance O
of O
our O
model O
. O

Overall O
, O
the O
results O
demonstrate O
the O
effectiveness O
of O
our O
model O
on O
different O
types O
of O
corpora O
. O

However O
, O
our O
model O
still O
achieves O
higher O
ROUGE-1 B-MetricName
and O
ROUGE-2 B-MetricName
scores O
than O
HeterSumGraph B-MethodName
. O

HeterSumGraph B-MethodName
is O
a O
extractive O
method O
so O
it O
achieves O
better O
ROUGE B-MetricName
- I-MetricName
L I-MetricName
score O
. O

We O
can O
see O
that O
EMSum B-MethodName
outperforms O
GraphSum B-MethodName
and O
EMSum+RoBERTa B-MethodName
outperforms O
GraphSum+RoBERTa B-MethodName
. O

The O
last O
block O
shows O
the O
results O
of O
our O
models O
. O

We O
report O
the O
results O
of O
FT B-MethodName
, O
HT B-MethodName
and O
GraphSum B-MethodName
following O
Li O
et O
al. O
( O
2020 O
) O
. O

The O
second O
block O
shows O
the O
abstractive O
methods O
. O

Similarly O
, O
the O
first O
block O
shows O
two O
extractive O
baselines O
LexRank B-MethodName
, O
and O
HeterSumGraph B-MethodName
. O

Results O
on O
MultiNews B-DatasetName
Table O
2 O
summarizes O
the O
evaluation O
results O
on O
the O
MultiNews B-DatasetName
dataset O
. O


The O
improvements O
over O
GraphSum+RoBERTa B-MethodName
are O
0.28 B-MetricValue
on O
ROUGE-2 B-MetricName
and O
0.83 B-MetricValue
on O
ROUGE B-MetricName
- I-MetricName
L I-MetricName
, O
also O
showing O
the O
effectiveness O
of O
our O
model O
even O
in O
the O
presence O
of O
pre O
- O
trained O
LMs O
. O

For O
models O
combined O
with O
pre O
- O
trained O
LMs O
, O
the O
results O
show O
that O
EMSum+RoBERTa B-MethodName
further O
improves O
the O
summarization B-TaskName
performance O
on O
all O
metrics O
over O
EMSum B-MethodName
. O

357 O

Considering O
all O
these O
three O
metrics O
together O
, O
the O
results O
show O
the O
effectiveness O
of O
our O
model O
. O

The O
gap O
between O
EMSum B-MethodName
and O
GraphSum B-MethodName
on O
ROUGE-1 B-MetricName
score O
is O
0.23 B-MetricValue
( O
42.40 B-MetricValue
vs O
42.63 B-MetricValue
) O
. O

Compared O
to O
the O
reported O
results O
of O
GraphSum B-MethodName
( O
which O
used O
top O
40 O
documents O
) O
, O
EMSum B-MethodName
achieves O
improvements O
on O
ROUGE-2 B-MetricName
and O
ROUGE B-MetricName
- I-MetricName
L I-MetricName
, O
even O
though O
EMSum B-MethodName
takes O
shorter O
source O
documents O
as O
input O
. O

Our O
model O
EMSum B-MethodName
performs O
the O
best O
under O
the O
top-20 O
setting O
. O

We O
believe O
this O
is O
because O
the O
lower O
- O
ranked O
paragraphs O
can O
still O
provide O
information O
anyway O
. O

The O
results O
show O
that O
if O
we O
limit O
the O
number O
of O
input O
paragraphs O
to O
20 O
, O
ROUGE B-MetricName
score O
of O
all O
models O
will O
drop O
by O
about O
2 B-MetricValue
points I-MetricValue
. O

The O
last O
block O
shows O
the O
results O
of O
some O
abstractive O
models O
and O
our O
model O
, O
but O
such O
models O
are O
fed O
with O
20 O
top O
- O
ranked O
paragraphs O
as O
input O
. O

We O
report O
their O
results O
following O
Li O
et O
al. O
( O
2020 O
) O
. O

The O
second O
block O
shows O
the O
results O
of O
abstractive O
models O
introduced O
in O
Section O
4.2 O
. O

The O
first O
block O
shows O
the O
baseline O
model O
Lead B-MethodName
and O
LexRank B-MethodName
( O
Erkan O
and O
Radev O
, O
2004 O
) O
, O
which O
are O
extractive O
methods O
. O

Results O
Results O
on O
WikiSum B-DatasetName
Table O
1 O
summarizes O
the O
evaluation O
results O
on O
the O
WikiSum B-DatasetName
dataset O
. O

Moreover O
, O
we O
choose O
Hierarchical B-MethodName
Transformer I-MethodName
( O
HT B-MethodName
) O
proposed O
by O
Liu O
and O
Lapata O
( O
2019a O
) O
, O
GraphSum B-MethodName
proposed O
by O
Li O
et O
al. O
( O
2020 O
) O
, O
and O
HeterSumGraph B-MethodName
proposed O
by O
Wang O
et O
al. O
( O
2020a O
) O
for O
comparisons O
. O

They O
use O
a O
Transformer O
decoder O
but O
apply O
a O
convolutional O
layer O
to O
compress O
the O
key O
and O
value O
in O
self O
- O
attention O
. O

Transformer B-MethodName
Decoder I-MethodName
with I-MethodName
Memory I-MethodName
Compressed I-MethodName
Attention I-MethodName
model O
( O
T B-MethodName
- I-MethodName
DMCA I-MethodName
) O
is O
proposed O
by O
Liu O
et O
al. O
( O
2018 O
) O
with O
the O
WikiSum B-DatasetName
dataset O
. O

Flat B-MethodName
Transformer I-MethodName
( O
FT B-MethodName
) O
is O
a O
6 B-HyperparameterValue
- O
layer B-HyperparameterName
encoder O
- O
decoder O
model O
. O

The O
title O
and O
ranked O
paragraphs O
were O
concatenated O
and O
truncated O
to O
800 B-HyperparameterValue
tokens O
. O

We O
choose O
a O
series O
of O
Transformer O
- O
based O
models O
for O
comparison O
due O
to O
their O
excellent O
performance O
. O

For O
the O
pre O
- O
trained O
part O
, O
the O
learning B-HyperparameterName
rate I-HyperparameterName
and O
warmup B-HyperparameterName
steps I-HyperparameterName
are O
set O
as O
0.002 B-HyperparameterValue
and O
20,000 B-HyperparameterValue
, O
while O
for O
other O
parts O
are O
0.2 B-HyperparameterValue
and O
8,000 B-HyperparameterValue
, O
respectively O
. O

We O
follow O
Liu O
and O
Lapata O
( O
2019b O
) O
, O
employing O
two O
Adam B-HyperparameterValue
optimizers B-HyperparameterName
( O
Kingma O
and O
Ba O
, O
2015 O
) O
for O
the O
pre O
- O
trained O
part O
and O
other O
parts O
, O
with O
β1 B-HyperparameterName
= O
0.9 B-HyperparameterValue
, O
β2 B-HyperparameterName
= O
0.998 B-HyperparameterValue
. O

For O
models O
with O
pre O
- O
trained O
LMs O
, O
we O
choose O
the O
base O
version O
of O
RoBERTa O
. O

During O
decoding O
we O
apply O
beam O
search O
with O
beam B-HyperparameterName
size I-HyperparameterName
5 B-HyperparameterValue
and O
length B-HyperparameterName
penalty I-HyperparameterName
( O
Wu O
et O
al. O
, O
2016 O
) O
with O
factor O
0.4 B-HyperparameterValue
. O

We O
train O
our O
model O
for O
200,000 B-HyperparameterValue
steps B-HyperparameterName
with O
gradient O
accumulation O
every O
four O
steps O
. O

We O
use O
dropout B-HyperparameterName
with I-HyperparameterName
probability I-HyperparameterName
0.1 B-HyperparameterValue
before O
all O
linear O
layers O
and O
label O
smoothing O
( O
Szegedy O
et O
al. O
, O
2016 O
) O
with O
smoothing B-HyperparameterName
factor I-HyperparameterName
0.1 B-HyperparameterValue
. O

4.3 O
number B-HyperparameterName
of I-HyperparameterName
iterations I-HyperparameterName
t B-HyperparameterName
= O
2 B-HyperparameterValue
based O
on O
the O
performance O
. O

We O
have O
introduced O
them O
in O
Section O
2 O
. O

We O
select O
the O

In O
the O
graph O
encoding O
process O
, O
each O
layer O
has O
8 B-HyperparameterValue
heads B-HyperparameterName
and O
the O
hidden B-HyperparameterName
size I-HyperparameterName
is O
256 B-HyperparameterValue
. O

In O
the O
multi O
- O
head O
pooling O
layer O
, O
the O
number B-HyperparameterName
of I-HyperparameterName
heads I-HyperparameterName
is O
8 B-HyperparameterValue
. O

We O
truncate O
the O
length B-HyperparameterName
of I-HyperparameterName
input I-HyperparameterName
paragraphs I-HyperparameterName
and O
entity O
clusters O
to O
100 B-HyperparameterValue
and O
50 B-HyperparameterValue
tokens O
, O
respectively O
. O

We O
apply O
the O
graph O
attention O
network O
( O
GAT O
) O
( O
Veličković O
et O
al. O
, O
2017 O
) O
to O
enable O
information O
flow O
between O
nodes O
and O
iteratively O
update O
the O
node O
representations O
. O

Our O
model O
augments O
the O
classical O
Transformer B-MethodName
- O
based O
encoder O
- O
decoder O
framework O
with O
a O
heterogeneous O
graph O
consisting O
of O
text O
units O
and O
entities O
as O
nodes O
, O
which O
allows O
rich O
cross O
- O
document O
information O
to O
be O
captured O
. O

Entity O
- O
Aware O
Abstractive O
Multi B-TaskName
- I-TaskName
Document I-TaskName
Summarization I-TaskName
Hao O
Zhou1 O
, O
Weidong O
Ren1 O
, O
Gongshen O
Liu1∗ O
, O
Bo O
Su1 O
, O
Wei O
Lu2 O
1 O
School O
of O
Electronic O
Information O
and O
Electrical O
Engineering O
, O
Shanghai O
Jiao O
Tong O
University O
2 O
StatNLP O
Research O
Group O
, O
Singapore O
University O
of O
Technology O
and O
Design O
{ O
zhou1998,renweidong1997,lgshen,subo}@sjtu.edu.cn O
luwei@sutd.edu.sg O
Abstract O
Entities O
and O
their O
mentions O
convey O
significant O
semantic O
information O
in O
documents O
. O

Hyperparameters O
We O
set O
the O
number O
of O
our O
vanilla O
Transformer O
encoding O
layers O
as O
6 B-HyperparameterValue
, O
the O
hidden O
size O
as O
256 B-HyperparameterValue
and O
the O
number O
of O
heads O
as O
8 B-HyperparameterValue
, O
while O
the O
hidden B-HyperparameterName
size I-HyperparameterName
of I-HyperparameterName
feed I-HyperparameterName
- I-HyperparameterName
forward I-HyperparameterName
layers I-HyperparameterName
is O
1,024 B-HyperparameterValue
. O

356 O

We O
report O
different O
versions O
of O
the O
metric O
, O
based O
on O
overlaps O
of O
unigrams O
( O
ROUGE-1 B-MetricName
, O
R-1 B-MetricName
) O
, O
bigrams O
( O
ROUGE-2 B-MetricName
, O
R-2 B-MetricName
) O
and O
the O
longest O
common O
subsequences O
( O
ROUGE B-MetricName
- I-MetricName
L I-MetricName
, O
R B-MetricName
- I-MetricName
L I-MetricName
) O
. O

Experiments O
We O
use O
ROUGE B-MetricName
scores O
to O
evaluate O
summarization B-TaskName
quality O
automatically O
( O
Lin O
, O
2004 O
) O
. O

For O
each O
instance O
, O
we O
get O
13.3 O
clusters O
on O
average O
and O
each O
cluster O
has O
9.9 O
tokens O
on O
average O
. O

Different O
from O
the O
WikiSum B-DatasetName
dataset O
, O
each O
source O
article O
only O
contains O
2.8 O
paragraphs O
and O
21.6 O
sentences O
on O
average O
, O
thus O
we O
choose O
to O
build O
graph O
on O
sentence O
level O
rather O
than O
paragraph O
level O
for O
this O
dataset O
. O

Following O
their O
experimental O
settings O
, O
we O
get O
44,972 O
instances O
for O
training O
, O
5,622 O
for O
validation O
and O
5,622 O
for O
test O
. O

The O
source O
articles O
come O
from O
a O
diverse O
set O
of O
news O
sources O
, O
over O
1,500 O
sites O
. O

MultiNews B-DatasetName
Dataset O
Introduced O
by O
Fabbri O
et O
al. O
( O
2019 O
) O
, O
MultiNews B-DatasetName
consists O
of O
news O
articles O
and O
hand O
- O
written O
summaries O
. O

For O
each O
instance O
, O
we O
get O
23.7 O
clusters O
on O
average O
and O
each O
cluster O
has O
10.2 O
tokens O
on O
average O
. O

We O
then O
perform O
entity O
cluster O
extraction O
on O
the O
top-20 O
WikiSum B-DatasetName
dataset O
. O

On O
average O
, O
each O
paragraph O
has O
70.1 O
tokens O
, O
and O
target O
sumamry O
has O
139.4 O
tokens O
. O

We O
get O
300,000 O
instances O
for O
training O
, O
38,144 O
for O
validation O
and O
38,205 O
for O
test O
. O

So O
we O
choose O
to O
use O
the O
top-20 O
version O
of O
WikiSum B-DatasetName
dataset O
in O
order O
to O
find O
a O
balance O
between O
computational O
cost O
and O
the O
coverage O
of O
input O
content O
. O

Experiment O
shows O
that O
the O
ROUGE O
- O
L O
recall O
of O
top-20 O
paragraphs O
against O
the O
gold O
target O
text O
is O
53.84 O
, O
and O
top-40 O
is O
60.42 O
. O

However O
, O
the O
top-40 O
dataset O
is O
quite O
heavy O
for O
entity O
extraction O
and O
co O
- O
reference O
resolution O
. O

They O
further O
split O
the O
long O
and O
messy O
source O
documents O
into O
multiple O
paragraphs O
by O
line O
- O
breaks O
and O
select O
the O
top-40 O
paragraphs O
as O
input O
for O
summarization O
systems O
. O

Liu O
and O
Lapata O
( O
2019a O
) O
crawled O
Wikipedia O
articles O
and O
source O
reference O
documents O
through O
the O
provided O
urls O
. O

WikiSum B-DatasetName
Dataset O
Liu O
et O
al. O
( O
2018 O
) O
treat O
the O
generation O
of O
Wikipedia O
section O
titles O
as O
a O
supervised O
multi B-TaskName
- I-TaskName
document I-TaskName
summarization I-TaskName
task O
. O

4 O
4.1 O
Experimental O
Setup O
We O
conduct O
experiments O
on O
two O
major O
datasets O
used O
in O
the O
literature O
of O
multi B-TaskName
- I-TaskName
document I-TaskName
summarization I-TaskName
, O
namely O
WikiSum B-DatasetName
( O
Liu O
et O
al. O
, O
2018 O
) O
and O
MultiNews B-DatasetName
( O
Fabbri O
et O
al. O
, O
2019 O
) O
. O

Finally O
, O
we O
perform O
the O
same O
multi O
- O
head O
pooling O
strategy O
to O
obtain O
paragraph O
representations O
. O

Then O
a O
single O
- O
layer O
bidirectional O
LSTM O
is O
employed O
over O
token O
embeddings O
, O
producing O
token O
features O
. O

We O
feed O
input O
tokens O
to O
a O
pre O
- O
trained O
language O
model O
and O
take O
the O
last O
layer O
output O
as O
token O
embeddings O
. O

Pre O
- O
trained O
language O
models O
can O
be O
more O
effective O
on O
short O
inputs O
than O
training O
stacked O
transformer O
layers O
from O
scratch O
. O

3.7 O
Pre O
- O
trained O
LMs O
as O
Document O
Encoder O
Our O
document O
encoder O
illustrated O
in O
section O
3.3 O
can O
be O
replaced O
by O
a O
pre O
- O
trained O
language O
model O
such O
as O
BERT O
( O
Devlin O
et O
al. O
, O
2019 O
) O
and O
RoBERTa O
( O
Liu O
et O
al. O
, O
2019 O
) O
. O

3.6 O
Training O
Our O
training O
process O
follows O
that O
of O
the O
traditional O
sequence O
- O
to O
- O
sequence O
modeling O
, O
with O
maximum O
likelihood O
estimation O
that O
minimizes O
: O
1 O
X O
Lseq O
= O
− O
log O
p(y|x O
; O
θ O
) O
( O
17 O
) O
|D| O
( O
y O
, O
x)∈D O
where O
x O
and O
y O
are O
document O
- O
summary O
pairs O
from O
training O
set O
D O
, O
and O
θ O
are O
parameters O
to O
be O
learned O
. O

We O
further O
add O
a O
copy O
mechanism O
as O
proposed O
by O
See O
et O
al. O
( O
2017 O
) O
. O

We O
use O
the O
weight O
- O
sharing O
strategy O
between O
the O
input O
embedding O
matrix O
and O
the O
matrix O
Wo O
to O
reuse O
linguistic O
knowledge O
( O
Paulus O
et O
al. O
, O
2018 O
) O
. O

Then O
the O
context O
vector O
vt O
can O
be O
computed O
by O
: O
X O
vt O
= O
γ̂wi O
h̃wi O
( O
15 O
) O
i O
Token O
Prediction O
Context O
vectors O
, O
treated O
as O
salient O
contents O
summarized O
from O
sources O
, O
are O
concatenated O
with O
the O
decoder O
hidden O
state O
st O
to O
produce O
the O
vocabulary O
distribution O
: O

For O
token O
wi O
in O
paragraph O
Pj O
, O
we O
further O
modify O
γwi O
by O
γ̂wi O
= O
βj O
× O
γwi O
( O
14 O
) O

Then O
we O
apply O
the O
attention O
mechanism O
over O
the O
Tw O
tokens O
in O
the O
selected O
paragraphs O
. O

Attending O
the O
Paragraph O
Tokens O
We O
select O
the O
top O
- O
k O
paragraph O
nodes O
with O
the O
highest O
attention O
score O
βj O
. O

We O
incorporate O
zi O
with O
edge O
weights O
ẽij O
to O
enable O
the O
information O
flow O
between O
entity O
nodes O
and O
paragraph O
nodes O
by O
: O
( O
8) O
m O
X O
zi O
× O

The O
entity O
nodes O
act O
as O
the O
intermediary O
between O
paragraph O
nodes O
. O

We O
also O
add O
a O
residual O
connection O
to O
avoid O
gradient O
vanishing O
after O
several O
iterations O
: O
h̃i O
= O
hi O
+ O
ui O
Entity O
- O
Aware O
Decoder O
with O
Two O
- O
level O
Attention O
( O
9 O
) O

We O
combine O
GAT O
with O
multi O
- O
head O
operation O
. O

Therefore O
, O
we O
directly O
incorporate O
the O
raw O
TF O
- O
IDF O
information O
into O
the O
GAT O
mechanism O
by O
modifying O
the O
attention O
weights O
using O
Equation O
5 O
. O

However O
, O
we O
argue O
that O
TF O
- O
IDF O
values O
themselves O
indicate O
the O
closeness O
between O
an O
entity O
cluster O
and O
a O
paragraph O
. O

In O
this O
way O
, O
the O
information O
contained O
in O
the O
values O
needs O
to O
be O
learned O
by O
an O
additional O
embedding O
matrix O
. O

That O
is O
how O
they O
map O
the O
weights O
to O
the O
multi O
- O
dimensional O
embedding O
space O
eij O
∈ O
Rde O
. O

They O
infuse O
the O
scalar O
edge O
weight O
ẽij O
by O
simply O
discretizing O
the O
real O
values O
into O
integers O
, O
and O
then O
learn O
embeddings O
for O
such O
integers O
. O

j∈Ni O
where O
Wa O
, O
Wq O
, O
Wk O
, O
Wv O
are O
trainable O
weights O
, O
σ O
is O
the O
sigmoid O
function O
, O
ẽij O
is O
the O
edge O
weight O
derived O
from O
TF O
- O
IDF O
value O
matrix O
Ẽ. O
We O
basically O
follow O
Wang O
et O
al. O
( O
2020a O
) O
to O
iteratively O
update O
node O
representations O
. O

The O
GAT O
layer O
is O
designed O
as O
follows O
: O

Rdh O
to O
denote O
the O
node O
representations O
, O
and O
use O
Ni O
to O
denote O
the O
set O
of O
neighboring O
nodes O
of O
node O
i. O

We O
use O
i O
, O
j O
∈ O
{ O
1 O
, O
... O
, O
( O
m O
+ O
n O
) O
} O
to O
denote O
an O
arbitrary O
node O
in O
graph O
, O
use O
hi O
, O
hj O
∈ O

We O
use O
graph O
attention O
networks O
( O
GAT O
) O
( O
Veličković O
et O
al. O
, O
2017 O
) O
to O
update O
the O
representations O
of O
semantic O
nodes O
. O

Graph O
Encoder O

Different O
from O
Section O
3.4 O
, O
we O
use O
i O
and O
j O
to O
denote O
the O
entity O
node O
and O
paragraph O
node O
, O
respectively O
. O

The O
indicator O
restricts O
the O
token O
- O
level O
attention O
only O
to O
some O
of O
the O
paragraphs O
, O
which O
can O
further O
reduce O
redundancy O
than O
naively O
attending O
to O
all O
tokens O
. O

Our O
two O
- O
level O
decoding O
process O
firstly O
focuses O
on O
several O
centering O
entity O
cluster O
nodes O
, O
which O
can O
be O
regarded O
as O
indicators O
of O
saliency O
. O

If O
the O
decoder O
needs O
to O
compute O
attention O
weights O
over O
all O
tokens O
, O
the O
cost O
would O
be O
very O
high O
and O
the O
attention O
could O
be O
dispersed O
. O

Under O
the O
setting O
of O
multi B-TaskName
- I-TaskName
document I-TaskName
summarization I-TaskName
, O
the O
input O
source O
documents O
may O
involve O
an O
extremely O
large O
number O
of O
word O
tokens O
. O

3.5 O

Rnw O
×(dw O
+ O
dh B-HyperparameterName
) O
. O

After O
iterating O
for O
t O
times O
, O
we O
concatenate O
H̃p O
to O
each O
corresponding O
input O
token O
vector O
, O
arriving O
at O
H̃pw O
∈ O

Each O
iteration O
contains O
a O
paragraph O
- O
to O
- O
entity O
and O
a O
entity O
- O
to O
- O
paragraph O
updating O
process O
. O

3.4 O
We O
use O
the O
above O
GAT O
layer O
and O
positionwise O
feed O
- O
forward O
layer O
to O
iteratively O
update O
the O
node O
representations O
. O

Rmw O
×dw O
and O
Hc O
∈ O
Rm×dh O
to O
denote O
the O
token O
level O
feature O
matrix O
and O
cluster O
level O
feature O
matrix O
, O
respectively O
. O

We O
use O
Hcw O
∈ O

Note O
that O
we O
firstly O
remove O
pronouns O
and O
stopwords O
in O
entity O
mention O
clusters O
, O
which O
are O
common O
in O
co O
- O
reference O
resolution O
results O
but O
render O
little O
benefit O
for O
our O
semantic O
modeling O
. O

this O
method O
rather O
than O
additional O
entity O
embedding O
methods O
because O
we O
seek O
to O
model O
the O
relationship O
between O
paragraphs O
and O
entities O
in O
a O
unified O
semantic O
space O
. O

We O
choose O

We O
perform O
the O
same O
encoding O
process O
as O
the O
paragraph O
encoder O
to O
get O
entity O
clusters O
’ O
representation O
, O
but O
without O
sharing O
parameters O
between O
the O
two O
encoders O
. O

Entity O
Cluster O
Encoder O

We O
use O
Hp O
∈ O
Rn×dh O
to O
denote O
the O
paragraph O
level O
feature O
matrix O
, O
where O
n O
is O
the O
number O
of O
paragraphs O
, O
dh B-HyperparameterName
is O
the O
hidden O
size O
. O

For O
the O
l O
- O
th O
transformer O
layer O
, O
l−1 O
, O
the O
hidden O
state O
is O
hl O
, O
and O
the O
the O
input O
is O
xw O
w O
354 O
( O
3 O
) O

Let O
x0w O
be O
the O
input O
token O
vector O
. O

The O
transformer O
layer O
is O
the O
same O
as O
the O
vanilla O
transformer O
layer O
( O
Vaswani O
et O
al. O
, O
2017 O
) O
. O

Document O
Encoder O
Paragraph O
Encoder O
Several O
token O
- O
level O
transformer O
encoding O
layers O
are O
stacked O
to O
encode O
contextual O
information O
within O
each O
paragraph O
. O

hp O
= O
MHPool(hw1 O
, O
hw2 O
, O
... O
) O

The O
multi O
- O
head O
pooling O
mechanism O
calculates O
the O
weight O
distributions O
over O
tokens O
, O
allowing O
the O
model O
to O
flexibly O
encode O
paragraphs O
in O
different O
representational O
subspace O
by O
different O
head O
. O

Multi O
- O
Head O
Pooling O
To O
obtain O
fixed O
length O
paragraph O
representations O
, O
we O
follow O
Liu O
and O
Lapata O
( O
2019a O
) O
to O
apply O
a O
weighted O
- O
pooling O
operation O
. O

Rnw O
×dw O
to O
denote O
the O
token O
- O
level O
feature O
matrix O
, O
where O
nw O
is O
the O
total O
number O
of O
tokens O
in O
all O
paragraphs O
and O
dw B-HyperparameterName
is O
the O
dimension O
of O
token O
embedding O
. O

We O
use O
Hpw O
∈ O

We O
take O
the O
output O
of O
last O
layer O
as O
token O
- O
level O
features O
. O

FFN O
is O
a O
feed O
- O
forward O
network O
with O
ReLU O
as O
activation O
function O
. O

MHAttn O
is O
multi O
- O
head O
attention O
from O
Vaswani O
et O
al. O
( O
2017 O
) O
. O

3.3 O
xlw O
= O
LayerNorm(hlw O
+ O
FFN(hlw O
) O
) O
( O
2 O
) O
LayerNorm O
is O
the O
layer O
normalization O
operation O
( O
Ba O
et O
al. O
, O
2016 O
) O
. O

Rm×n O
to O
model O
the O
importance O
of O
relationships O
between O
entity O
clusters O
and O
paragraphs O
. O

Based O
on O
E O
, O
we O
further O
calculate O
the O
TF O
- O
IDF O
value O
matrix O
Ẽ O
∈ O

We O
get O
an O
occurrence O
matrix O
E O
∈ O
Rm×n O
after O
extraction O
, O
where O
eij O
6= O
0 O
indicates O
Pi O
contains O
entity O
mentions O
in O
Cj O
for O
eij O
times O
. O

We O
would O
like O
to O
include O
more O
information O
in O
the O
graph O
. O

An O
edge O
which O
connects O
Pi O
and O
Cj O
means O
paragraph O
Pi O
contains O
an O
entity O
mention O
in O
Cj O
. O

There O
exists O
no O
edge O
inside O
paragraph O
nodes O
or O
entity O
cluster O
nodes O
, O
but O
only O
between O
them O
. O

E O
represents O
undirected O
edges O
between O
nodes O
. O

V O
includes O
paragraph O
nodes O
Vp O
and O
entity O
cluster O
nodes O
Vc O
. O

We O
then O
construct O
a O
heterogeneous O
graph O
G O
= O
( O
V O
, O
E O
) O
. O

, O
Pn O
} O
, O
such O
as O
paragraphs O
and O
sentences O
, O
depending O
on O
the O
characteristics O
of O
datasets O
. O

Given O
a O
source O
document O
cluster O
D O
, O
we O
firstly O
divide O
them O
into O
smaller O
semantic O
units O
P O
= O
{ O
P1 O
, O
P2 O
, O
. O
. O
. O

3.2 O
l−1 O
hlw O
= O
LayerNorm(xl−1 O
w O
+ O
MHAttn(xw O
) O
) O
( O
1 O
) O
Graph O
Construction O

the O
number O
of O
entity O
mentions O
in O
cluster O
Ci O
. O

output O
is O
xlw O
. O

Overall O
architecture O
of O
our O
model O
. O

Figure O
2 O
: O

, O
mentionl O
} O
, O
and O
l O
is O
353 O

, O
Cm O
} O
, O
where O
Ci O
= O
{ O
mention1 O
, O
mention2 O
, O
. O
. O
. O

We O
denote O
the O
extracted O
entity O
clusters O
as O
C O
= O
{ O
C1 O
, O
C2 O
, O
. O
. O
. O

Note O
that O
we O
perform O
extraction O
globally O
, O
which O
means O
we O
concatenate O
all O
the O
documents O
into O
one O
long O
document O
. O

We O
utilize O
the O
co O
- O
reference O
resolution O
tool O
( O
Lee O
et O
al. O
, O
2017 O
) O
from O
AllenNLP O
( O
Gardner O
et O
al. O
, O
2018 O
) O
to O
extract O
entity O
clusters O
. O

Therefore O
, O
we O
use O
entity O
clusters O
as O
more O
advanced O
semantic O
units O
. O

The O
total O
number O
of O
words O
will O
be O
vast O
, O
which O
further O
causes O
a O
hindrance O
for O
the O
graph O
construction O
and O
message O
passing O
process O
. O

For O
multi B-TaskName
- I-TaskName
document I-TaskName
summarization I-TaskName
, O
models O
are O
usually O
required O
to O
process O
tens O
of O
documents O
. O

However O
, O
we O
argue O
that O
word O
- O
level O
semantic O
units O
are O
too O
fine O
and O
will O
bring O
huge O
computational O
costs O
. O

3.1 O
Entity O
Cluster O
Extraction O
Wang O
et O
al. O
( O
2020a O
) O
use O
words O
as O
semantic O
units O
in O
addition O
to O
sentence O
nodes O
, O
acting O
as O
the O
intermediary O
to O
enrich O
the O
relationships O
between O
sentences O
. O

We O
design O
a O
novel O
two O
- O
level O
decoding O
process O
to O
explicitly O
deal O
with O
the O
problem O
of O
saliency O
and O
redundancy O
. O

We O
modify O
the O
encoder O
with O
graph O
neural O
networks O
, O
so O
we O
can O
incorporate O
entity O
information O
and O
graph O
representations O
at O
the O
same O
time O
. O

3 O
Model O
Our O
model O
is O
illustrated O
in O
Figure O
2 O
, O
which O
follows O
the O
transformer O
- O
based O
encoder O
- O
decoder O
architecture O
( O
Vaswani O
et O
al. O
, O
2017 O
) O
. O

Wang O
et O
al. O
( O
2020b O
) O
rearrange O
and O
further O
explore O
the O
semantics O
of O
the O
topic O
model O
and O
develope O
a O
friendly O
topic O
assistant O
for O
transfomer O
- O
based O
abstractive O
summarization O
models O
. O

Perez O
- O
Beltrachini O
et O
al. O
( O
2019 O
) O
explicitly O
model O
the O
topic O
structure O
of O
summaries O
, O
and O
utilize O
it O
to O
guide O
a O
structured O
convolutional O
decoder O
. O

In O
their O
work O
, O
sentence O
salience O
is O
estimated O
in O
a O
hierarchical O
way O
with O
subtopic O
salience O
and O
relative O
sentence O
salience O
. O

Zheng O
et O
al. O
( O
2019 O
) O
propose O
to O
mine O
cross O
- O
document O
subtopics O
. O

Narayan O
et O
al. O
( O
2018 O
) O
recommend O
an O
encoder O
associating O
each O
word O
with O
a O
topic O
vector O
capturing O
whether O
it O
is O
representative O
of O
the O
document O
’s O
content O
, O
and O
a O
decoder O
where O
each O
word O
prediction O
is O
conditioned O
on O
a O
document O
topic O
vector O
. O

Apart O
from O
entity O
or O
fact O
information O
, O
there O
are O
several O
works O
that O
incorporate O
topic O
information O
into O
summarization O
model O
. O

Zhu O
et O
al. O
( O
2020 O
) O
extract O
factual O
relations O
from O
the O
source O
texts O
to O
build O
a O
local O
knowledge O
graph O
and O
integrated O
it O
into O
the O
transformer O
- O
based O
model O
. O

Gunel O
et O
al. O
( O
2020 O
) O
inject O
structural O
world O
knowledge O
from O
Wikidata O
to O
a O
transformer O
- O
based O
model O
, O
enabling O
the O
model O
to O
be O
more O
fact O
- O
aware O
. O

By O
contrast O
, O
our O
EMSum B-MethodName
model O
is O
an O
end O
- O
to O
- O
end O
method O
for O
multi B-TaskName
- I-TaskName
document I-TaskName
summarization I-TaskName
. O

Sharma O
et O
al. O
( O
2019 O
) O
take O
a O
pipeline O
method O
for O
single O
- O
document O
summarization O
which O
is O
composed O
of O
an O
entity O
- O
aware O
content O
selection O
module O
and O
a O
summary O
generation O
module O
. O

Cao O
et O
al. O
( O
2018 O
) O
extract O
actual O
fact O
descriptions O
from O
the O
source O
text O
and O
propose O
a O
dual O
- O
attention O
mechanism O
to O
force O
the O
generation O
conditioned O
on O
both O
the O
source O
text O
and O
the O
extracted O
fact O
descriptions O
. O

In O
addition O
to O
the O
direct O
application O
of O
the O
general O
sequence O
- O
to O
- O
sequence O
framework O
, O
researchers O
attempted O
to O
incorporate O
various O
features O
into O
summarization O
. O

2.3 O
Summarization O
with O
Additional O
Features O

Our O
work O
is O
partly O
similar O
to O
theirs O
, O
but O
we O
construct O
heterogeneous O
graphs O
composed O
of O
text O
unit O
nodes O
and O
entity O
nodes O
for O
abstractive O
multi B-TaskName
- I-TaskName
document I-TaskName
summarization I-TaskName
. O

However O
, O
Wang O
et O
al. O
( O
2020a O
) O
are O
the O
first O
to O
introduce O
different O
granularity O
levels O
of O
text O
nodes O
to O
construct O
heterogeneous O
graphs O
for O
extractive O
summarization O
. O

Li O
et O
al. O
( O
2020 O
) O
utilize O
homogeneous O
graphs O
to O
capture O
cross O
- O
document O
relations O
and O
guide O
the O
summary O
generation O
process O
. O

Huang O
et O
al. O
( O
2020 O
) O
further O
design O
a O
graph O
encoder O
, O
which O
improves O
upon O
graph O
attention O
networks O
, O
to O
maintain O
the O
global O
context O
and O
local O
entities O
complementing O
each O
other O
. O

Fan O
et O
al. O
( O
2019 O
) O
construct O
a O
local O
knowledge O
graph O
, O
which O
is O
then O
linearized O
into O
a O
structured O
input O
sequence O
so O
that O
models O
can O
encode O
within O
the O
sequence O
- O
to O
- O
sequence O
setting O
. O

Yasunaga O
et O
al. O
( O
2017 O
) O
construct O
an O
approximate O
discourse O
graph O
based O
on O
discourse O
markers O
and O
entity O
links O
, O
then O
apply O
graph O
convolutional O
networks O
over O
the O
relation O
graph O
. O

For O
recent O
methods O
based O
on O
graph O
neural O
networks O
, O
Tan O
et O
al. O
( O
2017 O
) O
propose O
a O
graph O
- O
based O
attention O
mechanism O
to O
identify O
salient O
sentences O
. O

Christensen O
et O
al. O
( O
2013 O
) O
build O
multi O
- O
document O
graphs O
to O
approximate O
the O
discourse O
relations O
across O
sentences O
based O
on O
indicators O
including O
discourse O
cues O
, O
deverbal O
nouns O
, O
co O
- O
reference O
and O
more O
. O

Wan O
( O
2008 O
) O
further O
incorporate O
the O
document O
- O
level O
information O
and O
the O
sentence O
- O
to O
- O
document O
relationship O
into O
the O
graph O
- O
based O
ranking O
process O
. O

the O
eigenvector O
centrality O
in O
the O
connectivity O
graph O
of O
inter O
- O
sentence O
cosine O
similarity O
. O

LexRank B-MethodName
( O
Erkan O
and O
Radev O
, O
2004 O
) O
computes O
sentence O
salience O
based O
on O

Text O
units O
on O
graphs O
are O
ranked O
and O
selected O
as O
the O
most O
salient O
ones O
to O
be O
included O
in O
the O
summary O
. O

Our O
code O
is O
at O
https://github.com/Oceandam/EMSum O
352 O
2.2 O
Graph O
- O
based O
Document O
Summarization O
Graph O
- O
based O
methods O
have O
long O
been O
utilized O
for O
extractive O
summarization O
. O

Zou O
et O
al. O
( O
2020 O
) O
present O
three O
sequence O
- O
to O
- O
sequence O
pre O
- O
training O
objectives O
by O
reinstating O
source O
text O
for O
abstractive O
summarization O
. O

Moreover O
, O
several O
general O
purpose O
sequence O
- O
to O
- O
sequence O
pre O
- O
trained O
models O
are O
proposed O
, O
such O
as O
T5 O
( O
Raffel O
et O
al. O
, O
2020 O
) O
and O
BART O
( O
Lewis O
et O
al. O
, O
2019 O
) O
. O

They O
are O
further O
finetuned O
for O
the O
summarization O
task O
. O

Zhang O
et O
al. O
( O
2020 O
) O
propose O
PEGASUS O
, O
in O
which O
they O
design O
a O
pre O
- O
training O
objective O
tailored O
for O
abstractive O
text O
summarization O
. O

Zhang O
et O
al. O
( O
2019 O
) O
build O
low O
- O
level O
and O
high O
- O
level O
Berts O
for O
sentence O
and O
document O
understanding O
, O
respectively O
. O

Liu O
and O
Lapata O
( O
2019b O
) O
propose O
BertSUM B-MethodName
for O
both O
extractive O
and O
abstractive O
summarization O
. O

More O
recently O
, O
due O
to O
the O
excellent O
performance O
on O
various O
text O
generation O
tasks O
, O
transformer O
- O
based O
methods O
become O
the O
mainstream O
approach O
for O
abstractive O
multi B-TaskName
- I-TaskName
document I-TaskName
summarization I-TaskName
, O
as O
well O
as O
pre O
- O
trained O
language O
models O
. O

Traditional O
approaches O
to O
abstractive O
summarization O
can O
be O
divided O
into O
sentence O
fusion O
- O
based O
( O
Barzilay O
and O
McKeown O
, O
2005 O
; O
Filippova O
and O
Strube O
, O
2008 O
; O
Banerjee O
et O
al. O
, O
2015 O
) O
, O
paraphrasing O
- O
based O
( O
Bing O
et O
al. O
, O
2015 O
; O
Cohn O
and O
Lapata O
, O
2009 O
) O
and O
information O
extraction O
- O
based O
( O
Li O
, O
2015 O
; O
Wang O
and O
Cardie O
, O
2013 O
; O
Pighin O
et O
al. O
, O
2014 O
) O
. O

With O
the O
development O
of O
neural O
- O
based O
methods O
, O
abstractive O
methods O
achieved O
promising O
results O
on O
single O
document O
summarization O
( O
See O
et O
al. O
, O
2017 O
; O
Paulus O
et O
al. O
, O
2018 O
; O
Gehrmann O
et O
al. O
, O
2018 O
; O
Li O
et O
al. O
, O
2018 O
) O
. O

By O
contrast O
, O
the O
process O
of O
abstractive O
summarization O
is O
more O
similar O
to O
the O
human O
summarization O
process O
and O
requires O
more O
sophisticated O
natural O
language O
understanding O
and O
generation O
techniques O
. O

However O
, O
sentence O
- O
level O
extraction O
lacks O
flexibility O
and O
tends O
to O
produce O
redundant O
information O
. O

• O
We O
propose O
a O
novel O
two O
- O
level O
attention O
mechanism O
during O
the O
decoding O
process O
, O
solving O
the O
issues O
of O
saliency O
and O
redundancy O
explicitly O
. O

Thus O
, O
they O
may O
be O
able O
to O
achieve O
relatively O
high O
ROUGE B-MetricName
scores O
( O
Lin O
, O
2004 O
) O
. O

• O
Our O
model O
achieves O
state O
- O
of O
- O
the O
- O
art O
results O
on O
WikiSum B-DatasetName
and O
MultiNews B-DatasetName
. O

The O
mechanism O
can O
also O
reduce O
the O
computational O
cost O
, O
making O
it O
easier O
to O
process O
long O
inputs O
. O

Experiments O
show O
that O
exploiting O
entity O
nodes O
as O
the O
intermediary O
between O
documents O
can O
be O
more O
effective O
than O
exploiting O
other O
semantic O
units O
( O
e.g. O
, O
words O
) O
. O

To O
the O
best O
of O
our O
knowledge O
, O
we O
are O
the O
first O
to O
model O
the O
relations O
between O
documents O
and O
entities O
in O
one O
heterogeneous O
graph O
. O

The O
graph O
consists O
of O
document O
- O
level O
and O
entitylevel O
nodes O
. O

Our O
contributions O
are O
as O
follows O
: O
• O
We O
construct O
a O
heterogeneous O
graph O
network O
for O
multi B-TaskName
- I-TaskName
document I-TaskName
summarization I-TaskName
. O

Further O
improvements O
can O
be O
made O
when O
our O
model O
is O
used O
together O
with O
the O
pre O
- O
trained O
language O
models O
. O

Experiments O
show O
that O
our O
model O
significantly O
improves O
the O
performance O
on O
several O
multi O
- O
document O
datasets O
. O

By O
considering O
the O
global O
interactions O
between O
entities O
and O
documents O
in O
the O
graph O
, O
the O
second O
stage O
is O
able O
to O
handle O
the O
redundancy O
issue O
. O

Intuitively O
, O
the O
first O
stage O
indentifies O
the O
salient O
content O
in O
each O
decoding O
step O
. O

Next O
, O
the O
attention O
weights O
of O
entities O
are O
incorporated O
with O
graph O
edge O
weights O
to O
guide O
the O
attention O
to O
the O
documents O
. O

The O
decoder O
first O
attends O
to O
the O
entities O
. O

In O
the O
decoding O
process O
, O
we O
design O
a O
novel O
two O
- O
level O
attention O
mechanism O
. O

The O
entity O
nodes O
can O
serve O
as O
bridges O
that O
connect O
different O
documents O
– O
we O
can O
model O
the O
relations O
across O
documents O
through O
entity O
clusters O
. O

Inspired O
by O
Wang O
et O
al. O
( O
2020a O
) O
, O
we O
build O
a O
heterogeneous O
graph O
that O
consists O
of O
nodes O
that O
represent O
documents O
and O
entities O
. O

This O
motivates O
us O
to O
propose O
an O
entity O
- O
aware O
abstractive O
multi B-TaskName
- I-TaskName
document I-TaskName
summarization I-TaskName
model O
that O
effectively O
encodes O
relations O
across O
documents O
with O
the O
help O
of O
entities O
, O
and O
explicitly O
solve O
the O
issues O
of O
saliency O
and O
redundancy O
. O

as O
the O
indicator O
of O
saliency O
and O
can O
be O
used O
to O
reduce O
redundancy O
. O

As O
shown O
in O
Figure O
1 O
, O
entity O
mentions O
frequently O
appear O
in O
the O
input O
article O
, O
and O
are O
playing O
unique O
roles O
that O
contribute O
towards O
the O
coherence O
and O
conciseness O
of O
the O
text O
. O

However O
, O
while O
effective O
empirically O
, O
such O
approaches O
do O
not O
focus O
on O
explicitly O
modeling O
the O
underlying O
semantic O
information O
across O
documents O
. O

Several O
previous O
research O
efforts O
have O
shown O
that O
modeling O
cross O
- O
document O
relations O
is O
essential O
in O
multi B-TaskName
- I-TaskName
document I-TaskName
summarization I-TaskName
( O
Liu O
and O
Lapata O
, O
2019a O
; O
Li O
et O
al. O
, O
2020 O
) O
. O

that O
were O
shown O
effective O
for O
single O
- O
document O
summarization O
to O
the O
multi O
- O
document O
setup O
may O
not O
lead O
to O
ideal O
results O
( O
Lebanoff O
et O
al. O
, O
2018 O
; O
Zhang O
et O
al. O
, O
2018 O
; O
Baumel O
et O
al. O
, O
2018 O
) O
. O

Thus O
, O
simply O
adopting O
models O
∗ O

While O
significant O
progress O
has O
been O
made O
in O
single O
- O
document O
summarization O
, O
the O
mainstream O
sequence O
- O
to O
- O
sequence O
models O
, O
which O
can O
perform O
well O
on O
single O
- O
document O
summarization O
, O
often O
struggle O
with O
extracting O
salient O
information O
and O
handling O
redundancy O
in O
the O
presence O
of O
multiple O
, O
long O
documents O
. O

It O
is O
a O
task O
that O
can O
be O
more O
challenging O
than O
single O
- O
document O
summarization O
due O
to O
the O
presence O
of O
diverse O
and O
potentially O
conflicting O
information O
( O
Ma O
et O
al. O
, O
2020 O
) O
. O

Introduction O
Multi B-TaskName
- I-TaskName
document I-TaskName
summarization I-TaskName
aims O
at O
generating O
a O
short O
and O
informative O
summary O
across O
a O
set O
of O
topic O
- O
related O
documents O
. O

We O
conduct O
comprehensive O
experiments O
on O
the O
standard O
datasets O
and O
the O
results O
show O
the O
effectiveness O
of O
our O
approach O
. O

Our O
model O
can O
also O
be O
used O
together O
with O
pre O
- O
trained O
language O
models O
, O
arriving O
at O
improved O
performance O
. O

In O
the O
decoding O
process O
, O
we O
design O
a O
novel O
two O
- O
level O
attention O
mechanism O
, O
allowing O
the O
model O
to O
deal O
with O
saliency O
and O
redundancy O
issues O
explicitly O
. O

In O
this O
paper O
, O
we O
present O
EMSum B-MethodName
, O
an O
entityaware O
model O
for O
abstractive O
multi B-TaskName
- I-TaskName
document I-TaskName
summarization I-TaskName
. O

Capturing O
such O
cross O
- O
document O
entity O
information O
can O
be O
beneficial O
– O
intuitively O
, O
it O
allows O
the O
system O
to O
aggregate O
diverse O
useful O
information O
around O
the O
same O
entity O
for O
better O
summarization B-TaskName
. O

In O
multidocument B-TaskName
summarization I-TaskName
, O
the O
same O
entity O
may O
appear O
across O
different O
documents O
. O

