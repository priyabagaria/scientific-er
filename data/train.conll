-DOCSTART- O
In O
this O
work O
we O
focus O
on O
a O
science O
domain O
( O
chemistry O
) O
and O
demonstrate O
the O
value O
and O
limitations O
of O
large O
- O
scale O
language O
models O
evaluated O
across O
a O
wide O
range O
of O
in O
- O
domain O
( O
science O
- O
focused O
) O
and O
out O
- O
of O
- O
domain O
tasks O
. O

arXiv O
preprint O
arXiv:1909.06146 O
. O

A O
dataset O
for O
biomedical O
research O
question O
answering O
. O

Pubmedqa O
: O

2019 O
. O

Qiao O
Jin O
, O
Bhuwan O
Dhingra O
, O
Zhengping O
Liu O
, O
William O
W O
Cohen O
, O
and O
Xinghua O
Lu O
. O

PMLR O
. O

In O
International O
Conference O
on O
Machine O
Learning O
, O
pages O
4651–4664 O
. O

Perceiver O
: O
General O
perception O
with O
iterative O
attention O
. O

Andrew O
Jaegle O
, O
Felix O
Gimeno O
, O
Andy O
Brock O
, O
Oriol O
Vinyals O
, O
Andrew O
Zisserman O
, O
and O
Joao O
Carreira O
. O
2021 O
. O

arXiv O
preprint O
arXiv:2009.03300 O
. O

Measuring O
massive O
multitask O
language O
understanding O
. O

Dan O
Hendrycks O
, O
Collin O
Burns O
, O
Steven O
Basart O
, O
Andy O
Zou O
, O
Mantas O
Mazeika O
, O
Dawn O
Song O
, O
and O
Jacob O
Steinhardt O
. O
2020 O
. O

PMID O
: O
34115937 O
. O

Journal O
of O
Chemical O
Information O
and O
Modeling O
, O
0(0):null O
. O

Automated O
chemical O
reaction O
extraction O
from O
scientific O
literature O
. O

2021 O
. O

Jiang O
Guo O
, O
A. O
Santiago O
Ibanez O
- O
Lopez O
, O
Hanyu O
Gao O
, O
Victor O
Quach O
, O
Connor O
W. O
Coley O
, O
Klavs O
F. O
Jensen O
, O
and O
Regina O
Barzilay O
. O

ACM O
Transactions O
on O
Computing O
for O
Healthcare O
( O
HEALTH O
) O
, O
3(1):1–23 O
. O

Domain O
- O
specific O
language O
model O
pretraining O
for O
biomedical O
natural O
language O
processing O
. O

A O
framework O
for O
few O
- O
shot O
language O
model O
evaluation O
. O

Yu O
Gu O
, O
Robert O
Tinn O
, O
Hao O
Cheng O
, O
Michael O
Lucas O
, O
Naoto O
Usuyama O
, O
Xiaodong O
Liu O
, O
Tristan O
Naumann O
, O
Jianfeng O
Gao O
, O
and O
Hoifung O
Poon O
. O
2021 O
. O

Leo O
Gao O
, O
Jonathan O
Tow O
, O
Stella O
Biderman O
, O
Sid O
Black O
, O
Anthony O
DiPofi O
, O
Charles O
Foster O
, O
Laurence O
Golding O
, O
Jeffrey O
Hsu O
, O
Kyle O
McDonell O
, O
Niklas O
Muennighoff O
, O
Jason O
Phang O
, O
Laria O
Reynolds O
, O
Eric O
Tang O
, O
Anish O
Thite O
, O
Ben O
Wang O
, O
Kevin O
Wang O
, O
and O
Andy O
Zou O
. O
2021 O
. O

arXiv O
preprint O
arXiv:2101.00027 O
. O

The O
pile O
: O
An O
800 O
gb O
dataset O
of O
diverse O
text O
for O
language O
modeling O
. O

Leo O
Gao O
, O
Stella O
Biderman O
, O
Sid O
Black O
, O
Laurence O
Golding O
, O
Travis O
Hoppe O
, O
Charles O
Foster O
, O
Jason O
Phang O
, O
Horace O
He O
, O
Anish O
Thite O
, O
Noa O
Nabeshima O
, O
et O
al. O
2020 O
. O

Transactions O
of O
the O
Association O
for O
Computational O
Linguistics O
, O
5:529–542 O
. O

Topic O
modeling O
with O
minimal O
domain O
knowledge O
. O

Anchored O
correlation O
explanation O
: O

Ryan O
J O
Gallagher O
, O
Kyle O
Reing O
, O
David O
Kale O
, O
and O
Greg O
Ver O
Steeg O
. O
2017 O
. O

arXiv O
preprint O
arXiv:1810.04805 O
. O

Bert O
: O
Pre O
- O
training O
of O
deep O
bidirectional O
transformers O
for O
language O
understanding O
. O

Jacob O
Devlin O
, O
Ming O
- O
Wei O
Chang O
, O
Kenton O
Lee O
, O
and O
Kristina O
Toutanova O
. O
2018 O
. O

In O
proceedings O
of O
Sinn O
und O
Bedeutung O
, O
volume O
23 O
, O
pages O
107–124 O
. O

The O
commitmentbank O
: O
Investigating O
projection O
in O
naturally O
occurring O
discourse O
. O

Marie O
- O
Catherine O
De O
Marneffe O
, O
Mandy O
Simons O
, O
and O
Judith O
Tonhauser O
. O
2019 O
. O

Cord19 O
. O
https://www.semanticscholar.org/cord19/download O
. O

arXiv O
preprint O
arXiv:1803.05457 O
. O

try O
arc O
, O
the O
ai2 O
reasoning O
challenge O
. O

Think O
you O
have O
solved O
question O
answering O
? O

2018 O
. O

Peter O
Clark O
, O
Isaac O
Cowhey O
, O
Oren O
Etzioni O
, O
Tushar O
Khot O
, O
Ashish O
Sabharwal O
, O
Carissa O
Schoenick O
, O
and O
Oyvind O
Tafjord O
. O

In O
NAACL O
. O

Boolq O
: O
Exploring O
the O
surprising O
difficulty O
of O
natural O
yes O
/ O
no O
questions O
. O

Christopher O
Clark O
, O
Kenton O
Lee O
, O
Ming O
- O
Wei O
Chang O
, O
Tom O
Kwiatkowski O
, O
Michael O
Collins O
, O
and O
Kristina O
Toutanova O
. O
2019 O
. O

arXiv O
preprint O
arXiv:2202.07646 O
. O

Quantifying O
memorization O
across O
neural O
language O
models O
. O

Nicholas O
Carlini O
, O
Daphne O
Ippolito O
, O
Matthew O
Jagielski O
, O
Katherine O
Lee O
, O
Florian O
Tramer O
, O
and O
Chiyuan O
Zhang O
. O
2022 O
. O

Advances O
in O
neural O
information O
processing O
systems O
, O
33:1877–1901 O
. O

Language O
models O
are O
few O
- O
shot O
learners O
. O

Tom O
Brown O
, O
Benjamin O
Mann O
, O
Nick O
Ryder O
, O
Melanie O
Subbiah O
, O
Jared O
D O
Kaplan O
, O
Prafulla O
Dhariwal O
, O
Arvind O
Neelakantan O
, O
Pranav O
Shyam O
, O
Girish O
Sastry O
, O
Amanda O
Askell O
, O
et O
al. O
2020 O
. O

arXiv O
preprint O
arXiv:2108.07258 O
. O

On O
the O
opportunities O
and O
risks O
of O
foundation O
models O
. O

Rishi O
Bommasani O
, O
Drew O
A O
Hudson O
, O
Ehsan O
Adeli O
, O
Russ O
Altman O
, O
Simran O
Arora O
, O
Sydney O
von O
Arx O
, O
Michael O
S O
Bernstein O
, O
Jeannette O
Bohg O
, O
Antoine O
Bosselut O
, O
Emma O
Brunskill O
, O
et O
al. O
2021 O
. O

Gpt O
- O
neox-20b O
: O
An O
open O
- O
source O
autoregressive O
language O
model O
. O

Sid O
Black O
, O
Stella O
Biderman O
, O
Eric O
Hallahan O
, O
Quentin O
Anthony O
, O
Leo O
Gao O
, O
Laurence O
Golding O
, O
Horace O
He O
, O
Connor O
Leahy O
, O
Kyle O
McDonell O
, O
Jason O
Phang O
, O
et O
al. O
2022 O
. O

In O
Proceedings O
of O
the O
AAAI O
conference O
on O
artificial O
intelligence O
, O
volume O
34 O
, O
pages O
7432–7439 O
. O

Reasoning O
about O
physical O
commonsense O
in O
natural O
language O
. O

Piqa O
: O

Yonatan O
Bisk O
, O
Rowan O
Zellers O
, O
Jianfeng O
Gao O
, O
Yejin O
Choi O
, O
et O
al. O
2020 O
. O

arXiv O
preprint O
arXiv:1903.10676 O
. O

Scibert O
: O
A O
pretrained O
language O
model O
for O
scientific O
text O
. O

Iz O
Beltagy O
, O
Kyle O
Lo O
, O
and O
Arman O
Cohan O
. O
2019 O
. O

GPT O
- O
NeoX O
: O
Large O
scale O
autoregressive O
language O
modeling O
in O
pytorch O
. O

Alex O
Andonian O
, O
Quentin O
Anthony O
, O
Stella O
Biderman O
, O
Sid O
Black O
, O
Preetham O
Gali O
, O
Leo O
Gao O
, O
Eric O
Hallahan O
, O
Josh O
Levy O
- O
Kramer O
, O
Connor O
Leahy O
, O
Lucas O
Nestler O
, O
Kip O
Parker O
, O
Michael O
Pieler O
, O
Shivanshu O
Purohit O
, O
Tri O
Songz O
, O
Phil O
Wang O
, O
and O
Samuel O
Weinbach O
. O
2021 O
. O

arXiv O
preprint O
arXiv:1905.13319 O
. O

Towards O
interpretable O
math O
word O
problem O
solving O
with O
operation O
- O
based O
formalisms O
. O

Mathqa O
: O

Aida O
Amini O
, O
Saadia O
Gabriel O
, O
Peter O
Lin O
, O
Rik O
KoncelKedziorski O
, O
Yejin O
Choi O
, O
and O
Hannaneh O
Hajishirzi O
. O
2019 O
. O

AMiner O
. O
https://www.aminer.org/. O

arXiv O
preprint O
arXiv:1904.03323 O
. O

Publicly O
available O
clinical O
bert O
embeddings O
. O

References O
Emily O
Alsentzer O
, O
John O
R O
Murphy O
, O
Willie O
Boag O
, O
WeiHung O
Weng O
, O
Di O
Jin O
, O
Tristan O
Naumann O
, O
and O
Matthew O
McDermott O
. O
2019 O
. O

167 O

We O
rigorously O
analyzed O
model O
performance O
on O
15 O
+ O
indomain O
and O
out O
- O
of O
- O
domain O
tasks O
. O

We O
pretrained O
and O
released O
25 O
+ O
foundation O
models O
for O
chemistry O
. O

6 O
Conclusions O
In O
this O
paper O
, O
we O
collected O
and O
released O
0.67 O
TB O
of O
research O
publication O
data O
collected O
across O
10 O
+ O
sources O
for O
chemistry O
. O

This O
suggests O
the O
importance O
of O
compute O
budget O
required O
in O
scaling O
foundation O
models O
. O

With O
such O
compute O
budget O
, O
small O
( O
S O
) O
models O
only O
outperforms O
the O
XL O
model O
in O
21 O
% O
in O
- O
domain O
and O
34 O
% O
out O
- O
of O
- O
domain O
evaluation O
tasks O
. O

5.6 O
Training O
Efficiency O
We O
use O
several O
dimensions O
to O
describe O
the O
training B-MetricName
efficiency I-MetricName
, O
i.e. O
, O
# O
FLOPs B-MetricName
, O
throughput B-MetricName
( O
speed O
) O
, O
and O
memory B-MetricName
. O

The O
smallest O
( O
S O
) O
model O
has O
59 B-MetricValue
% I-MetricValue
FLOPs B-MetricName
of O
the O
largest O
( O
XL O
) O
model O
, O
twice O
the O
speed B-MetricName
( O
steps O
/ O
s O
) O
, O
32 B-MetricValue
% I-MetricValue
per O
device O
GPU B-MetricName
memory I-MetricName
savings I-MetricName
, O
and O
76 B-MetricValue
% I-MetricValue
total B-MetricName
parameter I-MetricName
savings I-MetricName
( O
see O
Figure O
4 O
) O
. O

across O
the O
four O
model B-HyperparameterName
sizes I-HyperparameterName
described O
in O
the O
Table O
2 O
. O

We O
compare O
these O
compute O
dimensions O
( O
a O
) O
GPU O
computation O
in O
# O
Floating O
Point O
Operations O
( O
b O
) O
GPU O
Memory O
Allocation O
Figure O
4 O
: O
GPU O
system O
performance O
during O
pretraining O
. O

This O
initialization O
may O
diverge O
the O
model O
in O
the O
optimization O
process O
that O
may O
not O
be O
recovered O
. O

On O
the O
other O
hand O
, O
the O
tuned O
model O
starts O
with O
the O
suboptimal O
initialization O
from O
the O
general O
- O
domain O
language O
model O
( O
Gu O
et O
al. O
, O
2021 O
) O
. O

As O
the O
tuned O
model O
uses O
the O
original O
GPT-2 B-MethodName
vocabulary O
, O
it O
must O
use O
the O
fragmented O
general O
subwords O
to O
tokenize O
the O
chemistry O
terms O
available O
in O
our O
corpora O
. O

There O
are O
several O
factors O
in O
the O
continual O
pretraining O
that O
may O
contribute O
to O
this O
. O

For O
example O
, O
the O
tuned O
model O
records O
6x O
performance O
drop O
in O
the O
Wikitext B-DatasetName
compared O
to O
the O
best O
performing O
model O
. O

Second O
, O
fine O
- O
tuned O
models O
have O
a O
significant O
performance O
drop O
in O
the O
general O
language B-TaskName
modeling I-TaskName
tasks O
( O
Lambada B-TaskName
and O
Wikitext B-TaskName
) O
. O

HT B-DatasetName
- I-DatasetName
CC I-DatasetName
is O
the O
only O
in O
- O
domain O
task O
that O
the O
tuned O
model O
outperforms O
the O
rest O
of O
models O
, O
yet O
fails O
to O
outperform O
the O
best O
performing O
model O
trained O
from O
scratch O
. O

First O
, O
fine O
- O
tuned O
models O
fall O
behind O
other O
baselines O
in O
a O
majority O
of O
in O
- O
domain O
tasks O
. O

We O
have O
two O
main O
observations O
from O
this O
experiment O
. O

We O
report O
the O
zero O
- O
shot O
performance O
of O
the O
tuned O
model O
across O
in O
- O
domain O
( O
Table O
4 O
) O
and O
out O
- O
of O
- O
domain O
( O
Table O
5 O
) O
tasks O
. O

Pretraining O
In O
this O
section O
, O
we O
test O
whether O
the O
continual O
pretraining O
of O
a O
base O
GPT O
model O
with O
additional O
domain O
- O
specific O
data O
is O
helpful O
in O
the O
downstream O
task O
performance O
. O

5.5 O
Continual O
vs. O
From O
Scratch O

vestigate O
other O
confounding O
factors O
that O
may O
contribute O
to O
this O
performance O
patterns O
. O

We O
repeat O
the O
process O
in O
a O
randomly O
- O
ordered O
corpus O
for O
comparison O
, O
recording O
model O
checkpoints O
after O
performing O
continual O
pretraining O
on O
each O
data O
subset O
. O

We O
align O
publications O
in O
the O
MAG O
corpus O
by O
year O
and O
split O
them O
into O
ten O
equal O
subsets O
. O

( O
a O
) O
Random O
Order O
( O
b O
) O
Temporal O
Order O
Figure O
3 O
: O
The O
effect O
of O
temporal O
order O
of O
publications O
during O
pretraining O
. O

Future O
work O
will O
in O
166 O

This O
may O
be O
due O
to O
the O
catastrophic O
forgetting O
prevalent O
in O
continual O
learning O
( O
Ramasesh O
et O
al. O
, O
2021 O
) O
. O

On O
the O
other O
hand O
, O
there O
is O
a O
performance O
drop O
in O
the O
BoolQ B-DatasetName
and O
WSC B-DatasetName
over O
time O
. O

For O
example O
, O
a O
slight O
performance O
increase O
in O
the O
PIQA B-DatasetName
, O
CB B-DatasetName
, O
PubMedQA B-DatasetName
, O
and O
WIC B-DatasetName
over O
time O
with O
the O
models O
trained O
with O
temporally O
- O
ordered O
scientific O
texts O
. O

There O
are O
mixed O
patterns O
in O
performance O
across O
out O
- O
of O
- O
domain O
tasks O
. O

When O
the O
model O
was O
pretrained O
with O
random O
- O
ordered O
data O
subsets O
, O
we O
observe O
only O
a O
slight O
( O
< O
1 B-MetricValue
% I-MetricValue
) O
performance O
increase O
( O
as O
shown O
in O
Figure O
3a O
) O
. O

This O
is O
due O
to O
the O
temporal O
order O
of O
the O
knowledge O
acquired O
by O
the O
model O
. O

Similarly O
, O
ARC B-DatasetName
- I-DatasetName
E I-DatasetName
accuracy B-MetricName
improves O
from O
0.43 B-MetricValue
to O
0.45 B-MetricValue
. O

For O
example O
, O
SciQ B-DatasetName
accuracy B-MetricName
improves O
from O
0.64 B-MetricValue
to O
0.73 B-MetricValue
from O
the O
base O
model O
checkpoint O
to O
the O
final O
model O
checkpoint O
. O

First O
, O
SciQ B-DatasetName
and O
ARCE B-DatasetName
zero O
- O
shot O
task O
performances O
improve O
over O
time O
with O
the O
models O
trained O
with O
temporally O
- O
ordered O
scientific O
texts O
( O
as O
shown O
in O
Figure O
3b O
) O
. O

There O
are O
two O
key O
findings O
. O

Figure O
3 O
shows O
the O
performance O
of O
model O
checkpoints O
across O
in O
- O
domain O
and O
out O
- O
of O
- O
domain O
tasks O
. O

We O
train O
the O
initial O
model O
for O
150 B-HyperparameterValue
K I-HyperparameterValue
steps B-HyperparameterName
and O
each O
subsequent O
model O
for O
10 B-HyperparameterValue
K I-HyperparameterValue
steps B-HyperparameterName
with O
additional O
data O
. O

For O
example O
, O
in O
the O
temporally O
- O
aligned O
experiments O
, O
we O
first O
pretrain O
a O
model O
with O
3.4 O
M O
( O
10 O
% O
) O
articles O
from O
before O
1978 O
, O
and O
then O
use O
it O
as O
the O
base O
model O
to O
continue O
pretraining O
with O
another O
3.4 O
M O
( O
10 O
% O
) O
articles O
from O
between O
1978 O
and O
1989 O
. O

We O
continue O
pretraining O
a O
base O
medium O
( O
M O
) O
sized O
model O
iteratively O
with O
the O
subsets O
in O
the O
order O
they O
appeared O
in O
the O
respective O
data O
variant O
. O

For O
this O
experiment O
, O
we O
maintain O
two O
variants O
of O
the O
MAG O
dataset O
with O
random O
- O
ordered O
and O
temporal O
- O
ordered O
articles O
, O
splitting O
each O
into O
ten O
equal O
subsets O
. O

Third O
, O
we O
compare O
model O
performance O
trained O
with O
abstracts O
vs. O
full O
texts O
in O
the O
HT B-DatasetName
task O
and O
see O
that O
the O
best O
accuracy O
is O
achieved O
using O
the O
MAG O
and O
S2ORC O
datasets O
rather O
than O
the O
combined O
abstracts O
. O

In O
this O
section O
, O
we O
test O
how O
continual O
pretraining O
on O
temporal O
- O
aligned O
scientific O
publications O
impacts O
downstream O
performance O
. O

5.4 O
Temporal O
Effect O
Scientific O
knowledge O
evolves O
over O
time O
reflecting O
new O
research O
ideas O
, O
innovations O
, O
and O
findings O
. O

Thus O
, O
expanding O
full O
text O
coverage O
may O
improve O
out O
- O
of O
- O
domain O
task O
generalization O
. O

This O
performance O
difference O
may O
be O
due O
to O
the O
more O
expressive O
and O
diverse O
language O
presented O
in O
the O
full O
texts O
than O
in O
the O
abstracts O
. O

Finally O
, O
combined O
full O
text O
model O
performs O
better O
than O
the O
model O
trained O
with O
the O
abstracts O
in O
all O
outof O
- O
domain O
tasks O
except O
PIQA B-TaskName
. O

This O
suggests O
the O
importance O
of O
contextual O
knowledge O
provided O
by O
different O
data O
sources O
. O

We O
believe O
the O
diversity O
of O
scientific O
knowledge O
provided O
from O
the O
abstract O
data O
is O
useful O
since O
SciQ B-DatasetName
questions O
span O
biology O
, O
chemistry O
, O
earth O
science O
, O
and O
physics O
. O

Second O
, O
the O
model O
trained O
with O
the O
combined O
abstracts O
achieves O
the O
second O
best O
accuracy B-MetricName
( O
0.83 B-MetricValue
in O
comparison O
to O
0.79 B-MetricValue
for O
the O
full O
text O
model O
) O
in O
SciQ. B-DatasetName
Some O
of O
the O
models O
pretrained O
on O
individual O
abstract O
data O
achieve O
comparable O
performance O
in O
SciQ B-DatasetName
, O
e.g. O
, O
MAG B-MethodName
and O
AMiner B-MethodName
models O
achieve O
0.8 B-MetricValue
and O
0.78 B-MetricValue
accuracy B-MetricName
, O
respectively O
. O

There O
are O
might O
be O
several O
factors O
that O
contribute O
to O
this O
, O
but O
one O
may O
be O
the O
focused O
language O
in O
abstracts O
. O

First O
, O
the O
XL O
models O
trained O
with O
the O
combined O
abstract O
dataset O
achieve O
the O
lowest O
perplexity B-MetricName
score O
( O
22.77 B-MetricValue
) O
on O
the O
Pile B-DatasetName
– O
a O
45 B-MetricValue
% I-MetricValue
performance O
advantage O
over O
the O
full O
text O
version O
. O

In O
this O
section O
, O
we O
analyze O
the O
performance O
of O
models O
trained O
on O
paper O
abstracts O
versus O
full O
texts O
. O

5.3 O
Diversity O
Effect O
While O
abstracts O
often O
provide O
a O
summary O
of O
scientific O
publications O
, O
the O
full O
text O
contains O
more O
details O
. O

However O
, O
model B-HyperparameterName
size I-HyperparameterName
significantly O
contributes O
to O
the O
task O
performance O
. O

We O
suggest O
that O
pretraining O
performance O
may O
not O
be O
the O
ideal O
indicator O
to O
speculate O
the O
overall O
downstream O
task O
performance O
, O
especially O
in O
the O
zero O
- O
shot O
setting O
. O

However O
, O
there O
is O
no O
clear O
relationship O
between O
the O
task O
performance O
and O
the O
model B-HyperparameterName
sizes I-HyperparameterName
in O
the O
rest O
of O
benchmark O
datasets O
. O

Third O
, O
we O
find O
that O
zero O
- O
shot O
task O
performance O
in O
SciQ B-DatasetName
, O
HT B-DatasetName
- I-DatasetName
CC I-DatasetName
and O
ARC B-DatasetName
- I-DatasetName
E I-DatasetName
increases O
as O
we O
increase O
the O
model B-HyperparameterName
size I-HyperparameterName
( O
see O
Table O
5 O
) O
. O

We O
believe O
that O
the O
XL B-MethodName
( I-MethodName
4x I-MethodName
) I-MethodName
model O
can O
reach O
the O
similar O
perplexity B-MetricName
values O
when O
trained O
for O
this O
data O
scale O
. O

We O
note O
that O
the O
baseline O
models O
( O
Radford O
et O
al. O
, O
2019 O
) O
were O
trained O
with O
4x O
larger O
batch B-HyperparameterName
size I-HyperparameterName
( O
total O
batch B-HyperparameterName
size I-HyperparameterName
512 B-HyperparameterValue
) O
than O
what O
used O
in O
XL B-MethodName
( I-MethodName
4x I-MethodName
) I-MethodName
model O
. O

This O
experiment O
highlights O
the O
importance O
of O
training O
models O
with O
larger O
batch B-HyperparameterName
size I-HyperparameterName
. O

The O
same O
model O
also O
achieves O
the O
best O
SciQ B-DatasetName
performance O
with O
0.84 B-MetricValue
accuracy B-MetricName
and O
comparable O
in O
other O
tasks O
performance O
with O
the O
XL O
model O
. O

As O
shown O
in O
Table O
4 O
, O
larger O
models O
perform O
well O
on O
these O
language O
modeling O
tasks O
. O

Second O
, O
we O
noticed O
that O
the O
XL B-MethodName
( I-MethodName
4x I-MethodName
) I-MethodName
model O
trained O
for O
more O
tokens O
performs O
significantly O
better O
than O
the O
similar O
sized O
XL O
model O
. O

Specifically O
, O
XL B-MethodName
( I-MethodName
4x I-MethodName
) I-MethodName
model O
was O
trained O
with O
128 B-HyperparameterValue
total O
batch B-HyperparameterName
165 O

XL B-MethodName
( I-MethodName
4x I-MethodName
) I-MethodName
model O
achieves O
the O
lowest O
Lambada B-DatasetName
and O
WikiText B-DatasetName
perplexity B-MetricName
values O
across O
all O
our O
models O
trained O
from O
scratch O
( O
as O
shown O
in O
Table O
5 O
) O
. O

size O
compared O
to O
the O
32 B-HyperparameterValue
total O
batch B-HyperparameterName
size I-HyperparameterName
used O
in O
XL O
model O
. O

It O
is O
i O
m O
portant O
to O
note O
that O
we O
exclude O
PubMed O
Abstracts O
in O
the O
individual O
data O
collection O
to O
avoid O
potential O
contamination O
between O
the O
training O
and O
Pile O
testing O
data O
. O

This O
may O
be O
due O
to O
the O
models O
capturing O
scientific O
language O
better O
than O
general O
language O
. O

There O
is O
a O
48 B-MetricValue
% I-MetricValue
performance O
advantage O
in O
this O
task O
over O
the O
best O
performing O
baseline O
GPT-2 B-MethodName
model O
. O

First O
, O
we O
find O
that O
the O
models O
perform O
considerably O
well O
on O
Pile B-DatasetName
in O
comparison O
to O
the O
Lambada B-DatasetName
or O
WikiText B-DatasetName
. O

Analyzing O
downstream O
task O
performance O
Can O
we O
speculate O
downstream O
task O
performance O
of O
a O
model O
from O
the O
pretraining O
performance O
? O

Figure O
2 O
: O
Distribution O
of O
validation O
loss O
by O
model O
size O
: O
performance O
improves O
as O
the O
model O
size O
increases O
. O

This O
observation O
illustrates O
the O
relationship O
between O
model O
performance O
( O
as O
measured O
by O
the O
upstream O
cross O
entropy O
loss O
) O
and O
model B-HyperparameterName
size I-HyperparameterName
, O
confirming O
( O
Kaplan O
et O
al. O
, O
2020 O
) O
. O

Larger O
models O
reach O
a O
given O
loss O
value O
in O
a O
higher O
rate O
than O
the O
smaller O
models O
. O

We O
find O
that O
the O
cross O
entropy O
loss O
decreases O
as O
we O
increase O
the O
model B-HyperparameterName
size I-HyperparameterName
( O
as O
shown O
in O
Figure O
2 O
) O
. O

This O
measure O
will O
be O
averaged O
over O
the O
2048 O
- O
token O
context O
. O

Model O
Baseline O
AMiner B-MethodName
CORE B-MethodName
MAG B-MethodName
PubMed B-MethodName
- I-MethodName
F I-MethodName
S2ORC B-MethodName
WoS B-MethodName
Combined B-MethodName
- I-MethodName
A I-MethodName
Combined B-MethodName
- I-MethodName
F I-MethodName
Combined B-MethodName
- I-MethodName
A+F I-MethodName
Combined B-MethodName
- I-MethodName
A+F I-MethodName
Size B-HyperparameterName
S O
M O
L O
XL O
M‡ O
S O
M O
L O
XL O
S O
M O
L O
XL O
S O
M O
L O
XL O
S O
M O
L O
XL O
S O
M O
L O
XL O
S O
M O
L O
XL O
XL O
XL O
XL O
XL O
( O
4x O
) O
BoolQ B-DatasetName
CB B-DatasetName
WIC B-DatasetName
WSC B-DatasetName
MathQA B-DatasetName
PIQA B-DatasetName
PubMedQA B-DatasetName
Lambada B-DatasetName
Wikitext B-DatasetName
0.49 O
0.59 O
0.60 O
0.61 O
0.62 O
0.41 O
0.40 O
0.61 O
0.50 O
0.62 O
0.62 O
0.61 O
0.61 O
0.41 O
0.38 O
0.51 O
0.40 O
0.58 O
0.61 O
0.57 O
0.60 O
0.38 O
0.38 O
0.38 O
0.38 O
0.38 O
0.38 O
0.41 O
0.57 O
0.56 O
0.62 O
0.61 O
0.61 O
0.41 O
0.43 O
0.45 O
0.39 O
0.34 O
0.39 O
0.39 O
0.48 O
0.39 O
0.41 O
0.41 O
0.41 O
0.38 O
0.23 O
0.07 O
0.14 O
0.11 O
0.41 O
0.39 O
0.41 O
0.41 O
0.41 O
0.43 O
0.46 O
0.50 O
0.39 O
0.45 O
0.36 O
0.34 O
0.16 O
0.38 O
0.41 O
0.41 O
0.49 O
0.50 O
0.50 O
0.50 O
0.50 O
0.50 O
0.51 O
0.50 O
0.50 O
0.50 O
0.50 O
0.50 O
0.50 O
0.50 O
0.50 O
0.50 O
0.51 O
0.50 O
0.50 O
0.50 O
0.50 O
0.50 O
0.50 O
0.50 O
0.50 O
0.50 O
0.50 O
0.47 O
0.50 O
0.50 O
0.50 O
0.50 O
0.50 O
0.43 O
0.40 O
0.46 O
0.50 O
0.36 O
0.44 O
0.41 O
0.47 O
0.37 O
0.37 O
0.37 O
0.37 O
0.37 O
0.40 O
0.37 O
0.35 O
0.62 O
0.45 O
0.38 O
0.38 O
0.39 O
0.63 O
0.63 O
0.63 O
0.63 O
0.63 O
0.63 O
0.54 O
0.37 O
0.37 O
0.37 O
0.39 O
0.37 O
0.21 O
0.23 O
0.23 O
0.24 O
0.20 O
0.22 O
0.21 O
0.22 O
0.21 O
0.20 O
0.21 O
0.21 O
0.22 O
0.21 O
0.21 O
0.22 O
0.22 O
0.21 O
0.20 O
0.21 O
0.22 O
0.20 O
0.22 O
0.21 O
0.20 O
0.21 O
0.19 O
0.21 O
0.20 O
0.21 O
0.22 O
0.23 O
0.24 O
0.63 O
0.68 O
0.70 O
0.71 O
0.55 O
0.56 O
0.57 O
0.58 O
0.58 O
0.55 O
0.56 O
0.57 O
0.58 O
0.56 O
0.57 O
0.59 O
0.59 O
0.57 O
0.58 O
0.59 O
0.59 O
0.57 O
0.56 O
0.56 O
0.56 O
0.55 O
0.54 O
0.56 O
0.55 O
0.60 O
0.57 O
0.59 O
0.60 O
0.44 O
0.53 O
0.54 O
0.59 O
0.55 O
0.46 O
0.43 O
0.36 O
0.43 O
0.55 O
0.55 O
0.51 O
0.45 O
0.43 O
0.41 O
0.39 O
0.34 O
0.54 O
0.49 O
0.42 O
0.49 O
0.34 O
0.34 O
0.34 O
0.33 O
0.34 O
0.34 O
0.42 O
0.56 O
0.50 O
0.55 O
0.48 O
0.56 O
40.06 O
18.25 O
12.97 O
10.63 O
2834.51 O
2825.84 O
1802.35 O
661.81 O
786.22 O
671.43 O
273.06 O
173.15 O
79.95 O
1142.83 O
628.72 O
282.39 O
364.54 O
2670.39 O
1742.00 O
843.83 O
679.80 O
122739.30 O
80151.10 O
89136.68 O
107065.48 O
140552.69 O
182967.37 O
148609.73 O
192970.64 O
250.88 O
72.50 O
71.43 O
30.40 O
37.37 O
26.75 O
22.61 O
20.38 O
126.55 O
158.85 O
116.93 O
87.23 O
91.28 O
100.53 O
77.96 O
69.62 O
50.47 O
118.40 O
91.36 O
67.74 O
70.71 O
148.88 O
119.74 O
95.75 O
90.38 O
403.48 O
330.56 O
327.53 O
351.81 O
556.00 O
498.36 O
480.91 O
509.06 O
61.07 O
48.96 O
48.65 O
33.05 O
model O
performance O
on O
validation O
data O
using O
cross O
entropy O
loss O
in O
nats O
. O

XL O
( O
4x O
) O
model O
is O
trained O
with O
4x O
larger O
batch B-HyperparameterName
size I-HyperparameterName
that O
used O
in O
other O
models O
. O

Top-4 O
performance O
highlighted O
in O
bold O
, O
with O
best O
performance O
indicated O
with O
underlines O
. O

Performance O
on O
Lambada B-DatasetName
and O
Wikitext B-DatasetName
is O
reported O
using O
perplexity B-MetricName
, O
all O
other O
tasks O
report O
accuracy B-MetricName
. O

We O
use O
‡ O
to O
indicate O
the O
baseline O
model O
tuned O
from O
the O
base O
GPT-2 B-MethodName
model O
. O

Table O
5 O
: O
Downstream O
Out O
- O
of O
- O
domain O
Task O
Performance O
. O

We O
report O
the O
164 O

Analyzing O
upstream O
cross O
entropy O
loss O
During O
pretraining O
, O
we O
group O
each O
dataset O
into O
training B-HyperparameterName
/ I-HyperparameterName
validation I-HyperparameterName
/ I-HyperparameterName
test I-HyperparameterName
( O
949/50/1 B-HyperparameterValue
) O
splits O
. O

In O
this O
section O
, O
we O
revisit O
these O
claims O
on O
scaling O
Transformer O
architectures O
. O

5.2 O
Scaling O
Effect O
Previous O
work O
( O
Kaplan O
et O
al. O
, O
2020 O
) O
has O
shown O
that O
upstream O
cross O
entropy O
loss O
scales O
as O
a O
power O
- O
law O
with O
model O
size B-HyperparameterName
, O
dataset O
size O
, O
and O
the O
amount O
of O
compute O
. O

As O
shown O
in O
Table O
5 O
, O
our O
models O
outperform O
baseline O
GPT-2 B-MethodName
models O
for O
CB O
, O
WIC B-DatasetName
and O
WSC B-DatasetName
and O
match O
the O
best O
accuracy B-MetricName
for O
BoolQ B-DatasetName
but O
the O
GPT-2 B-MethodName
baselines O
outperform O
on O
the O
remaining O
tasks O
, O
particularly O
Lambada B-TaskName
and O
Wikitext B-TaskName
– O
the O
two O
general O
language B-TaskName
modeling I-TaskName
tasks O
. O

Out O
- O
of O
- O
domain O
Evaluation O
We O
evaluate O
outof O
- O
domain O
performance O
using O
9 O
commonly O
used O
LLM B-TaskName
benchmarks O
: O
BoolQ B-DatasetName
( O
Clark O
et O
al. O
, O
2019 O
) O
, O
CB B-DatasetName
( O
De O
Marneffe O
et O
al. O
, O
2019 O
) O
, O
WIC B-DatasetName
( O
Pilehvar O
and O
Camacho O
- O
Collados O
, O
2018 O
) O
, O
WSC B-DatasetName
( O
Levesque O
et O
al. O
, O
2012 O
) O
, O
MathQA B-DatasetName
( O
Amini O
et O
al. O
, O
2019 O
) O
, O
PIQA B-DatasetName
( O
Bisk O
et O
al. O
, O
2020 O
) O
, O
PubMedQA B-DatasetName
( O
Jin O
et O
al. O
, O
2019 O
) O
, O
Lambada B-DatasetName
( O
Paperno O
et O
al. O
, O
2016 O
) O
and O
WikiText B-DatasetName
( O
Merity O
et O
al. O
, O
2016 O
) O
. O

Of O
the O
remaining O
tasks O
, O
our O
models O
perform O
within O
1 B-MetricValue
- I-MetricValue
4 I-MetricValue
% I-MetricValue
of O
GPT-2 B-MethodName
baselines O
. O

As O
shown O
in O
Table O
4 O
, O
one O
or O
more O
of O
our O
models O
outperform O
baseline O
GPT-2 B-MethodName
models O
for O
the O
two O
chemistry O
tasks O
, O
general O
science B-TaskName
QA I-TaskName
( O
SciQ O
) O
and O
the O
sciencefocused B-TaskName
language I-TaskName
modelling I-TaskName
. O

( O
Mihaylov O
et O
al. O
, O
2018 O
) O
, O
Pile O
- O
PubMed B-DatasetName
- I-DatasetName
Abstracts I-DatasetName
( O
Gao O
et O
al. O
, O
2020 O
) O
) O
. O

In O
- O
domain O
Evaluation O
We O
consider O
five O
existing O
chemistry O
benchmarks O
, O
specifically O
HendrycksTest O
( O
Hendrycks O
et O
al. O
, O
2020 O
) O
for O
high O
school O
( O
HT B-DatasetName
- I-DatasetName
HC I-DatasetName
) O
and O
college O
( O
HT B-DatasetName
- I-DatasetName
CC I-DatasetName
) O
levels O
, O
and O
sciencefocused O
– O
ARC B-DatasetName
( O
Clark O
et O
al. O
, O
2018 O
) O
, O
SciQ B-DatasetName
( O
Welbl O
et O
al. O
, O
2017 O
) O
, O
OpenBookQA B-DatasetName

Pile O
0.22 O
0.18 O
0.18 O
0.18 O
0.19 O
0.18 O
0.18 O
0.23 O
0.23 O
0.19 O
0.22 O
0.17 O
0.20 O
0.24 O
0.18 O
0.19 O
0.20 O
0.26 O
0.19 O
0.18 O
0.18 O
0.26 O
0.27 O
0.28 O
0.24 O
0.22 O
0.25 O
0.27 O
0.23 O
0.17 O
0.20 O
0.18 O
0.18 O
0.25 O
0.27 O
0.28 O
0.26 O
0.31 O
0.27 O
0.34 O
0.34 O
0.34 O
0.28 O
0.34 O
0.30 O
0.28 O
0.28 O
0.27 O
0.36 O
0.36 O
0.30 O
0.27 O
0.28 O
0.27 O
0.33 O
0.22 O
0.23 O
0.31 O
0.31 O
0.32 O
0.30 O
0.34 O
0.28 O
0.30 O
0.30 O
0.25 O
0.44 O
0.49 O
0.53 O
0.58 O
0.35 O
0.43 O
0.45 O
0.49 O
0.50 O
0.36 O
0.40 O
0.41 O
0.47 O
0.41 O
0.45 O
0.51 O
0.50 O
0.41 O
0.43 O
0.43 O
0.48 O
0.31 O
0.33 O
0.32 O
0.33 O
0.33 O
0.32 O
0.32 O
0.34 O
0.54 O
0.48 O
0.48 O
0.55 O
0.19 O
0.22 O
0.22 O
0.25 O
0.19 O
0.21 O
0.20 O
0.23 O
0.23 O
0.19 O
0.20 O
0.19 O
0.21 O
0.20 O
0.21 O
0.24 O
0.22 O
0.20 O
0.21 O
0.22 O
0.21 O
0.21 O
0.18 O
0.21 O
0.19 O
0.22 O
0.20 O
0.21 O
0.21 O
0.23 O
0.21 O
0.22 O
0.24 O
0.75 O
0.77 O
0.80 O
0.83 O
0.61 O
0.70 O
0.74 O
0.78 O
0.77 O
0.69 O
0.71 O
0.75 O
0.78 O
0.66 O
0.68 O
0.80 O
0.80 O
0.60 O
0.68 O
0.74 O
0.77 O
0.31 O
0.31 O
0.31 O
0.30 O
0.37 O
0.34 O
0.37 O
0.39 O
0.83 O
0.79 O
0.79 O
0.84 O
0.16 O
0.19 O
0.19 O
0.22 O
0.13 O
0.17 O
0.16 O
0.18 O
0.17 O
0.15 O
0.15 O
0.14 O
0.15 O
0.17 O
0.17 O
0.18 O
0.20 O
0.16 O
0.18 O
0.17 O
0.16 O
0.17 O
0.16 O
0.17 O
0.18 O
0.17 O
0.16 O
0.17 O
0.16 O
0.18 O
0.15 O
0.17 O
0.17 O
96.50 O
61.26 O
48.86 O
42.29 O
87.57 O
38.40 O
30.55 O
24.18 O
25.52 O
78.24 O
59.19 O
52.95 O
39.46 O
38.03 O
30.88 O
24.78 O
26.09 O
56.03 O
45.69 O
37.22 O
35.14 O
59.20 O
45.60 O
42.14 O
42.35 O
54.41 O
48.31 O
46.44 O
45.86 O
22.77 O
40.18 O
31.03 O
23.01 O
2021 O
) O
for O
the O
benchmark O
implementation O
. O

Model O
Baseline O
AMiner B-MethodName
CORE B-MethodName
MAG B-MethodName
PubMed B-MethodName
- I-MethodName
F I-MethodName
S2ORC B-MethodName
WoS B-MethodName
Combined B-MethodName
- I-MethodName
A I-MethodName
Combined B-MethodName
- I-MethodName
F I-MethodName
Combined B-MethodName
- I-MethodName
A+F I-MethodName
Combined B-MethodName
- I-MethodName
A+F I-MethodName
Size B-HyperparameterName
S O
M O
L O
XL O
M‡ O
S O
M O
L O
XL O
S O
M O
L O
XL O
S O
M O
L O
XL O
S O
M O
L O
XL O
S O
M O
L O
XL O
S O
M O
L O
XL O
XL O
XL O
XL O
XL O
( O
4x O
) O
HT B-DatasetName
- I-DatasetName
HC I-DatasetName
HT B-DatasetName
- I-DatasetName
CC I-DatasetName
ARC B-DatasetName
- I-DatasetName
E I-DatasetName
ARC B-DatasetName
- I-DatasetName
C I-DatasetName
SciQ B-DatasetName
OpenBookQA B-DatasetName

XL O
( O
4x O
) O
model O
is O
trained O
with O
4x O
larger O
batch B-HyperparameterName
size I-HyperparameterName
that O
used O
in O
other O
models O
. O

We O
highlight O
the O
top-4 O
performance O
per O
task O
in O
bold O
, O
with O
top O
performance O
indicated O
with O
an O
underline O
. O

Pile O
performance O
is O
reported O
using O
perplexity B-MetricName
, O
with O
all O
other O
tasks O
reported O
using O
accuracy B-MetricName
. O

We O
use O
‡ O
to O
indicate O
the O
baseline O
model O
tuned O
from O
the O
base O
GPT-2 B-MethodName
model O
. O

Table O
4 O
: O
Downstream O
Zero O
- O
shot O
In O
- O
Domain O
Task O
Performance O
. O

The O
benchmarks O
we O
include O
are O
described O
in O
Appendix O
B. O
We O
use O
the O
lmevaluation O
- O
harness O
Python O
repository O
( O
Gao O
et O
al. O
, O
163 O

5.1 O
Zero O
- O
shot O
Performance O
We O
evaluate O
our O
models O
using O
several O
benchmarks O
to O
assess O
the O
effectiveness O
in O
both O
in O
- O
domain O
and O
out O
- O
of O
- O
domain O
tasks O
. O

We O
also O
trained O
one O
XL O
( O
4x O
) O
model O
with O
4x O
larger O
batch B-HyperparameterName
size I-HyperparameterName
than O
what O
used O
in O
XL O
model O
to O
evaluate O
the O
impact O
of O
the O
number O
of O
training O
tokens O
. O

This O
is O
to O
control O
the O
number O
of O
tokens O
seen O
during O
model O
pretraining O
( O
320,000 B-HyperparameterValue
steps B-HyperparameterName
* O
4 O
GPUs O
* O
4 B-HyperparameterValue
micro B-HyperparameterName
batch I-HyperparameterName
size I-HyperparameterName
* O
2,048 B-HyperparameterValue
context B-HyperparameterName
size I-HyperparameterName
= O
10B O
tokens O
) O
relative O
to O
the O
maximum O
number O
of O
tokens O
available O
in O
the O
respective O
datasets O
( O
as O
reported O
in O
Table O
3 O
) O
. O

We O
only O
use O
4 O
GPUs O
for O
the O
models O
pretrained O
with O
individual O
datasets O
and O
8 O
GPUs O
for O
the O
rest O
. O

For O
example O
, O
PubMed O
publications O
cover O
mostly O
bio O
- O
medicinal O
terms O
( O
Gu O
et O
al. O
, O
2021 O
) O
, O
while O
the O
majority O
of O
S2ORC O
publications O
are O
from O
medicine O
, O
biology O
, O
physics O
, O
and O
mathematics O
( O
Lo O
et O
al. O
, O
2020 O
) O
. O

Our O
goal O
is O
to O
systematically O
study O
data O
biases O
in O
the O
model O
performance O
when O
pretraining O
models O
with O
individual O
datasets O
. O

Our O
Models O
We O
pretrained O
models O
with O
individual O
datasets O
( O
AMiner O
, O
CORE O
, O
MAG O
, O
PubMed O
, O
S2ORC O
, O
WOS O
) O
and O
combined O
abstracts O
and O
fulltexts O
. O

S2ORC O
4.0 O
% O
AMiner O
49.6 O
% O
MAG O
29.9 O
% O
arXiv O
OSTI O
Analysis O
and O
Results O
CORD19 O
PubMed O
DBLP O
Distribution O
within O
Sources O
with O
< O
1 O
% O
( O
upper O
right O
, O
black O
box O
) O
Figure O
1 O
: O
Summary O
of O
data O
source O
representation O
within O
the O
Combined O
A+F O
data O
sample O
. O

We O
split O
datasets O
to O
those O
that O
include O
abstracts O
〈A〉 O
vs. O
full O
texts O
〈FT〉. O
0.80 O
Data O
Collection O
and O
Processing O
PubMed O
# O
Params O
( O
B O
) O
0.11 O
0.11 O
0.11 O
0.11 O
0.11 O
0.22 O
0.77 O
0.02 O
0.30 O
0.34 O
0.80 O
1.20 O
0.11 O
0.11 O
0.11 O
0.11 O
0.34 O
2016 O
) O
, O
Microsoft O
Academic O
Graph O
( O
MAG O
) O
( O
Wang O
et O
al. O
, O
2020a O
) O
, O
OSTI O
, O
PubMed O
( O
Gao O
et O
al. O
, O
2020 O
) O
( O
abstracts O
and O
fulltexts O
) O
, O
and O
the O
Web O
of O
Science O
( O
WoS O
) O
. O

Size B-HyperparameterName
S O
M O
L O
XL O
Model O
GPT O
- O
NeoX O
GPT-2 B-MethodName
GPT O
- O
NeoX O
GPT-2 B-MethodName
GPT O
- O
NeoX O
GPT-2 B-MethodName
GPT O
- O
NeoX O
GPT-2 B-MethodName
dL B-HyperparameterName
12 B-HyperparameterValue
12 B-HyperparameterValue
24 B-HyperparameterValue
24 B-HyperparameterValue
24 B-HyperparameterValue
36 B-HyperparameterValue
24 B-HyperparameterValue
48 B-HyperparameterValue
ddim B-HyperparameterName
768 B-HyperparameterValue
768 B-HyperparameterValue
1024 B-HyperparameterValue
1024 B-HyperparameterValue
1536 B-HyperparameterValue
1280 B-HyperparameterValue
2048 B-HyperparameterValue
1600 B-HyperparameterValue
dheads B-HyperparameterName
12 B-HyperparameterValue
12 B-HyperparameterValue
16 B-HyperparameterValue
16 B-HyperparameterValue
16 B-HyperparameterValue
20 B-HyperparameterValue
16 B-HyperparameterValue
25 B-HyperparameterValue
# B-HyperparameterName
Params I-HyperparameterName
( O
B O
) O
0.18 B-HyperparameterValue
4 B-HyperparameterValue
1.47 B-HyperparameterValue
Source O
# O
Articles O
( O
M O
) O
# O
Tokens O
( O
B O
) O
Size O
( O
Gb O
) O
MAG O
〈A〉 O
34.26 O
7.43 O
46 O
Aminer O
〈A〉 O
18.50 O
5.80 O
35 O
S2ORC O
〈A〉 O
10.44 O
2.05 O
32 O
WoS O
〈A〉 O
7.90 O
3.31 O
18 O
CORD-19 O
〈A〉 O
< O
0.01 O
< O
0.01 O
0.2 O
OSTI O
〈A〉 O
0.05 O
< O
0.01 O
0.1 O
Arxiv O
〈A〉 O
0.38 O
0.04 O
0.4 O
PubMed O
〈A〉 O
0.28 O
0.08 O
0.5 O
PubMed O
〈FT〉 O
0.70 O
7.34 O
32 O
CORE O
〈FT〉 O
7.27 O
215.50 O
743 O

GPT O
- O
NeoX O
architecture O
is O
originally O
from O
GPT-3 O
( O
Brown O
et O
al. O
, O
2020 O
) O

We O
compare O
model O
configurations O
between O
GPT O
- O
NeoX O
and O
OpenAI O
’s O
GPT-2 B-MethodName
. O

Thus O
, O
we O
also O
include O
a O
base O
GPT-2 B-MethodName
model O
( O
medium O
) O
that O
has O
been O
updated O
with O
continual O
pretraining O
using O
our O
Combined O
〈A+FT〉 O
dataset O
. O

We O
note O
that O
GPT-2 B-MethodName
models O
were O
pretrained O
on O
WebText O
– O
8 O
million O
web O
documents O
( O
40Gb O
) O
. O

We O
compare O
our O
performance O
with O
four O
variants O
of O
the O
original O
GPT-2 B-MethodName
models O
, O
corresponding O
to O
small O
( O
S O
) O
, O
medium O
( O
M O
) O
, O
large O
( O
L O
) O
, O
and O
extra O
- O
large O
( O
XL O
) O
sized O
transformer O
architectures O
shown O
in O
Table O
2 O
. O

Baseline O
Models O
As O
we O
use O
a O
similar O
model O
architecture O
, O
we O
identify O
Open O
AI O
’s O
GPT-2 B-MethodName
( O
Radford O
et O
al. O
, O
2019 O
) O
as O
a O
baseline O
comparison O
model O
. O

For O
example O
, O
dimethylnitroxide O
was O
tokenized O
into O
# O
dimethyl O
, O
# O
nitr O
, O
# O
oxide O
using O
the O
in O
- O
domain O
vocabulary O
and O
# O
dim O
, O
# O
ethyl O
, O
# O
nit O
, O
# O
rox O
, O
# O
ide O
using O
the O
GPT-2 B-MethodName
vocabulary O
. O

We O
compare O
the O
GPT2 B-MethodName
vocabulary O
generated O
from O
the O
WebText O
and O
the O
in O
- O
domain O
vocabularies O
generated O
from O
our O
corpora O
and O
find O
that O
the O
in O
- O
domain O
vocabulary O
breaks O
chemical O
entities O
into O
fewer O
tokens O
. O

We O
train O
BPE O
tokenizers O
for O
each O
data O
sample O
with O
a O
vocabulary O
size O
of O
64 O
K O
as O
preliminary O
experiments O
varying O
vocabulary O
sizes O
from O
64 O
K O
to O
256 O
K O
for O
smaller O
scale O
model O
pretraining O
did O
not O
show O
significant O
differences O
in O
performance O
. O

Tokenization O
As O
used O
in O
GPT-2 B-MethodName
model O
, O
we O
use O
a O
Byte O
Pair O
Encoding O
( O
BPE O
) O
tokenizer O
. O

The O
deduplication O
process O
reduced O
our O
corpus O
from O
875 O
GB O
to O
670 O
GB O
( O
67.8 O
M O
to O
53.5 O
M O
publications O
) O
, O
removing O
14.3 O
M O
duplicates O
. O

With O
this O
technique O
, O
we O
were O
able O
to O
remove O
significant O
amounts O
of O
duplicate O
scientific O
articles O
both O
within O
and O
across O
sources O
. O

We O
processed O
titles O
to O
strip O
punctuation O
and O
casefold O
and O
considered O
two O
articles O
A1 O
and O
A2 O
to O
be O
duplicates O
if O
they O
had O
the O
same O
processed O
title O
. O

To O
this O
end O
, O
we O
performed O
deduplication O
of O
our O
corpus O
based O
on O
overlap O
of O
titles O
within O
and O
across O
data O
sources O
. O

Data O
Cleaning O
Recent O
research O
has O
shown O
that O
duplicates O
in O
training O
data O
can O
significantly O
impact O
the O
downstream O
task O
performance O
of O
LLMs O
( O
Lee O
et O
al. O
, O
2021 O
; O
Carlini O
et O
al. O
, O
2022 O
) O
. O

Coloring O
illustrates O
whether O
a O
data O
source O
contains O
peer O
reviewed O
( O
Blue O
) O
, O
mixed O
( O
Purple O
) O
, O
or O
not O
peer O
reviewed O
( O
Red O
) O
articles O
. O

We O
also O
compare O
the O
results O
from O
continual O
vs. O
from O
scratch O
pretraining O
( O
Section O
5.5 O
) O
and O
present O
the O
analysis O
of O
large O
- O
scale O
training O
efficiency O
( O
Section O
5.6 O
) O
. O

We O
investigate O
the O
effects O
of O
model O
and O
data O
scaling O
( O
RQ2 O
, O
Section O
5.2 O
) O
, O
knowledge O
diversity O
( O
RQ3 O
, O
Section O
5.3 O
) O
, O
and O
temporal O
order O
( O
RQ4 O
, O
Section O
5.4 O
) O
on O
the O
downstream O
performance O
. O

5 O
0.8 O
% O
WoS B-DatasetName
3.8 O
% O
CORE B-DatasetName
11.9 O
% O
This O
section O
presents O
the O
analysis O
of O
28 O
pretrained O
models O
evaluated O
on O
15 O
+ O
in O
- O
domain O
and O
out O
- O
ofdomain O
downstream O
tasks O
( O
RQ1 O
, O
Section O
5.1 O
) O
. O

This O
resulted O
in O
a O
list O
of O
more O
than O
1 O
K O
chemistry O
- O
related O
entities O
, O
ranging O
from O
compound O
names O
like O
ethyl O
acetate O
, O
methyl O
methacrylate O
, O
sulfoxide O
, O
etc O
. O
to O
experiment O
and O
procedures O
like O
tunneling O
microscopy O
, O
neutralization O
, O
enzymatic O
hydrolysis O
, O
etc O
. O
162 O

These O
keywords O
were O
extracted O
by O
using O
a O
Correlation O
Explanation O
( O
Gallagher O
et O
al. O
, O
2017 O
) O
topic O
model O
followed O
by O
manual O
filtering O
by O
subject O
matter O
experts O
. O

Because O
the O
data O
sources O
we O
relied O
on O
comprise O
research O
publications O
from O
many O
science O
domains O
, O
we O
sampled O
articles O
using O
a O
list O
of O
domain O
- O
specific O
keywords O
for O
chemistry O
to O
create O
the O
dataset O
summarized O
in O
Table O
3 O
. O

Corpus O
PubMed O
MIMIC1 O
PubMed O
+ O
MIMIC O
Arxiv O
Chemistry O
Journals O

We O
perform O
experiments O
in O
a O
single O
DGX O
- O
A100 O
machine O
with O
8 O
80Gb O
GPUs O
. O

The O
original O
GPT-2 B-MethodName
models O
are O
fine O
- O
tuned O
for O
150 B-HyperparameterValue
K I-HyperparameterValue
steps O
. O

Models O
are O
pretrained O
from O
scratch O
for O
a O
total O
of O
320 B-HyperparameterValue
K I-HyperparameterValue
steps O
. O

As O
the O
largest O
model O
in O
our O
experiments O
fit O
on O
a O
single O
GPU O
, O
we O
did O
n’t O
use O
the O
model O
( O
tensor O
) O
or O
pipeline O
parallelism O
. O

Our O
models O
are O
pretrained O
across O
multiple O
workers O
with O
data O
parallelism O
. O

0.40 O
advantages O
in O
tasks O
with O
longer O
texts O
by O
capturing O
relative O
position O
dependency O
in O
self O
- O
attention O
. O

See O
Appendix O
A O
for O
full O
data O
descriptions O
. O

As O
shown O
in O
Table O
3 O
, O
our O
corpus O
was O
collected O
from O
10 O
different O
data O
sources O
: O
Arxiv B-DatasetName
, O
Aminer B-DatasetName
( O
AMiner O
) O
, O
CORD19 B-DatasetName
( O
Wang O
et O
al. O
, O
2020b O
) O
, O
CORE B-DatasetName
( O
Pontika O
et O
al. O
, O
PubMed B-DatasetName
PubMed O
PMC O
+ O
CS O
OAG O
PubMed O
10 O
+ O
sources O
( O
Chemistry O
) O
Table O
3 O
: O
Dataset O
statistics O
: O
combined O
datasets O
are O
after O
the O
de O
- O
duplication O
process O
. O

Combined O
〈A〉 O
46.94 O
16.18 O
67 O
Combined O
〈FT〉 O
6.52 O
184.42 O
603 O
Combined O
〈A+FT〉 O
53.45 O
200.61 O
670 O
1.47 O
We O
collected O
a O
large O
corpus O
of O
53.45 O
million O
chemistry O
- O
focused O
scientific O
articles O
and O
abstracts O
, O
resulting O
in O
670 O
GB O
of O
text O
data O
. O

Table O
2 O
: O
Our O
model O
configurations O
: O
dL B-HyperparameterName
is O
the O
number B-HyperparameterName
of I-HyperparameterName
decoder I-HyperparameterName
layers I-HyperparameterName
, O
ddim B-HyperparameterName
is O
the O
hidden B-HyperparameterName
size I-HyperparameterName
of O
the O
model O
, O
dheads B-HyperparameterName
is O
the O
number B-HyperparameterName
of I-HyperparameterName
attention I-HyperparameterName
heads I-HyperparameterName
. O

Gu O
et O
al. O
2021 O
BioELECTRA O
ELECTRAMed O
SciBERT O
OAG O
- O
BERT O
PubMedBERT O
PubMed O
PubMed O
PMC O
+ O
CS O
OAG O
PubMed O
10 O
+ O
sources O
( O
Chemistry O
) O
continual O
pretraining O
continual O
pretraining O
continual O
pretraining O
from O
scratch O
continual O
pretraining O
from O
scratch O
from O
scratch O
from O
scratch O
from O
scratch O
from O
scratch O
from O
scratch O
continual O
pretraining O
Our O
Work O
( O
autoregressive O
) O
† O

Liu O
et O
al. O
2021 O

Beltagy O
et O
al. O
2019 O

Miolo O
et O
al. O
2021 O

BioMegatron O
PubMed O
Kanakarajan O
et O
al. O
2021 O

Yuan O
et O
al. O
2021 O
BioALBERT O
BioRoBERTa O
KeBioLM O
Wiki O
+ O
Books O
Wiki O
+ O
Books O
PubMed O
PMC O
+ O
MIMIC O
- O
II O
PMC O
+ O
MIMIC O
- O
III O
PubMed O
+ O
UMLS2 O
Shin O
et O
al. O
2020 O

Pretraining O
continual O
pretraining O
continual O
pretraining O
continual O
pretraining O
continual O
pretraining O
continual O
pretraining O
Phan O
et O
al. O
2021 O
SciFive O
C4 O
continual O
pretraining O
PubMed O
Naseem O
et O
al. O
2021 O
Lewis O
et O
al. O
2020 O

† O
Data O
Source O
Wiki O
+ O
Books O
Wiki O
+ O
Books O
Wiki O
+ O
Books O
Arxiv O
Wiki O
+ O
Books O

MATH O
- O
BERT O
Guo O
et O
al. O
2021 O
Chem(Rxn)BERT O

BioBERT O
Alsentzer O
et O
al. O
2019 O
ClinicalBERT O
Peng O
et O
al. O
2019 O
BlueBERT O
Liu O
et O
al. O
2021 O

Model O
Lee O
et O
al. O
2020 O

We O
use O
† O
to O
indicate O
models O
trained O
for O
chemistry O
. O

Table O
1 O
: O
Foundation O
models O
for O
science O
focus O
on O
the O
biomedical O
, O
math O
, O
computer O
science O
and O
chemistry O
domains O
. O

We O
also O
use O
the O
Rotary O
positional O
embeddings O
( O
Su O
et O
al. O
, O
2021 O
) O
instead O
of O
the O
learned O
positional O
embeddings O
used O
in O
the O
GPT-2 B-MethodName
model O
( O
Radford O
et O
al. O
, O
2019 O
) O
because O
they O
offer O
performance O
161 O

To O
reduce O
memory O
and O
increase O
training O
throughput O
, O
we O
use O
mixed B-MethodName
- I-MethodName
precision I-MethodName
training I-MethodName
( O
Rasley O
et O
al. O
, O
2020 O
) O
and O
the O
parallel O
attention O
and O
feed O
- O
forward O
implementations O
available O
in O
GPT O
- O
NeoX O
( O
Black O
et O
al. O
, O
2022 O
) O
. O

In O
addition O
, O
ZeRO B-HyperparameterValue
optimizer B-HyperparameterName
( O
Rajbhandari O
et O
al. O
, O
2019 O
) O
was O
used O
to O
reduce O
memory O
footprint O
by O
distributing O
optimizer O
states O
across O
several O
processes O
. O

We O
use O
an O
Adam B-HyperparameterValue
optimizer B-HyperparameterName
with O
β1 B-HyperparameterName
= O
0.9 B-HyperparameterValue
, O
β2 B-HyperparameterName
= O
0.99 B-HyperparameterValue
, O
and O
σ B-HyperparameterName
= O
10−8 B-HyperparameterValue
and O
clip O
the O
gradient O
norm O
at O
1.0 B-HyperparameterValue
. O

We O
set O
the O
micro B-HyperparameterName
batch I-HyperparameterName
size I-HyperparameterName
per O
GPU O
as O
4 B-HyperparameterValue
, O
and O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
2 B-HyperparameterValue
× I-HyperparameterValue
10−4 I-HyperparameterValue
, O
and O
rely O
on O
the O
cosine O
decay O
. O

We O
optimize O
the O
autoregressive O
log O
- O
likelihood O
( O
i.e. O
, O
cross O
- O
entropy O
loss O
) O
averaged O
over O
a O
2048 B-HyperparameterValue
- O
token O
context O
. O

Our O
experiments O
leverage O
the O
GPT O
- O
NeoX O
Python O
library O
( O
Andonian O
et O
al. O
, O
2021 O
) O
developed O
with O
Megatron O
( O
Shoeybi O
et O
al. O
, O
2019 O
) O
and O
DeepSpeed O
( O
Rasley O
et O
al. O
, O
2020 O
) O
. O

These O
models O
differ O
in O
the O
number B-HyperparameterName
of I-HyperparameterName
decoder I-HyperparameterName
layers I-HyperparameterName
, O
hidden B-HyperparameterName
size I-HyperparameterName
of O
the O
model O
, O
and O
the O
number B-HyperparameterName
of I-HyperparameterName
attention I-HyperparameterName
heads I-HyperparameterName
in O
transformer O
blocks O
as O
shown O
in O
Table O
2 O
. O

To O
understand O
the O
impact O
of O
model B-HyperparameterName
size I-HyperparameterName
( O
RQ2 O
) O
, O
we O
experiment O
with O
four O
different O
Transformer O
sizes O
: O
small O
( O
S O
) O
, O
medium O
( O
M O
) O
, O
large O
( O
L O
) O
, O
and O
extra O
- O
large O
( O
XL O
) O
. O

3 O
Model O
Pretraining O
Unlike O
the O
majority O
of O
related O
models O
that O
rely O
on O
a O
base O
BERT B-MethodName
( O
or O
variant O
) O
model O
, O
we O
adapt O
the O
OpenAI O
’s O
GPT-2 B-MethodName
transformer O
decoder O
architecture O
( O
Radford O
et O
al. O
, O
2019 O
) O
to O
train O
autoregressive O
language O
models O
for O
Chemistry O
. O

Unlike O
any O
previous O
work O
, O
we O
use O
both O
continual O
and O
from O
scratch O
pretraining O
to O
build O
the O
largest O
foundation O
model O
for O
Chemistry O
( O
1.47B B-HyperparameterValue
) O
on O
the O
largest O
( O
0.67 O
TB O
) O
and O
the O
most O
diverse O
corpus O
( O
10 O
+ O
sources O
) O
collected O
to O
date O
. O

PubMedBERT O
( O
Gu O
et O
al. O
, O
2021 O
) O
is O
another O
example O
of O
pretraining O
the O
base O
BERT O
model O
from O
scratch O
using O
PubMed O
. O

SciBERT B-MethodName
( O
Beltagy O
et O
al. O
, O
2019 O
) O
is O
pretrained O
according O
to O
this O
procedure O
using O
the O
vocabulary O
generated O
from O
computer O
science O
and O
biomedical O
domains O
. O

This O
is O
mainly O
due O
to O
the O
availability O
of O
in O
- O
domain O
data O
for O
both O
generating O
the O
vocabulary O
and O
pretraining O
. O

In B-MethodName
- I-MethodName
Domain I-MethodName
Pretraining I-MethodName
from I-MethodName
Scratch I-MethodName
Previous O
work O
has O
shown O
that O
pretraining O
models O
from O
scratch O
on O
domain O
- O
specific O
data O
has O
a O
significant O
benefit O
over O
continual O
pretraining O
of O
generaldomain O
language O
models O
( O
Gu O
et O
al. O
, O
2021 O
) O
. O

In O
the O
Chemistry O
domain O
, O
Guo O
et O
al. O
( O
2021 O
) O
performed O
continual O
pretraining O
of O
a O
base O
BERT O
model O
on O
200 O
K O
chemistry O
journal O
articles O
for O
product O
extraction O
( O
ChemBERT O
) O
and O
reaction O
role O
labeling O
( O
ChemRxnBERT O
) O
. O

Several O
models O
have O
been O
developed O
for O
the O
biomedical O
domain O
and O
the O
most O
frequently O
used O
corpora O
for O
domain O
- O
specific O
continual O
preraining O
are O
PubMed B-DatasetName
abstracts O
and O
PubMed B-DatasetName
Central I-DatasetName
full O
- O
text O
articles O
( O
PMC O
) O
( O
Lee O
et O
al. O
, O
2020 O
; O
Peng O
et O
al. O
, O
2019 O
; O
Phan O
et O
al. O
, O
2021 O
) O
. O

Mixed B-MethodName
- I-MethodName
Domain I-MethodName
Continual I-MethodName
Pretraining I-MethodName
Many O
efforts O
have O
focused O
on O
continual O
pretraining O
of O
a O
BERT O
( O
Devlin O
et O
al. O
, O
2018 O
) O
base O
model O
. O

We O
present O
a O
model O
summary O
in O
Table O
1 O
. O

2 O
Related O
Work O
In O
this O
section O
we O
summarize O
previous O
efforts O
in O
two O
categories O
: O
mixed O
- O
domain O
continual O
pretraining O
that O
continues O
pretraining O
of O
a O
base O
model O
on O
domain O
data O
and O
in O
- O
domain O
pretraining O
from O
scratch O
that O
pretrains O
a O
from O
scratch O
on O
domain O
data O
. O

Temporal O
Effect O
How O
does O
the O
recency O
of O
scientific O
knowledge O
, O
e.g. O
, O
when O
manipulating O
the O
temporal O
order O
of O
the O
documents O
processed O
by O
the O
model O
, O
affect O
downstream O
performance O
? O

( O
RQ4 O
) O

( O
RQ3 O
) O
Diversity O
Effect O
How O
does O
the O
depth O
of O
scientific O
knowledge O
, O
e.g. O
, O
from O
paper O
abstracts O
vs. O
full O
text O
, O
affect O
downstream O
performance O
? O

Do O
neural O
scaling O
laws O
presented O
in O
( O
Kaplan O
et O
al. O
, O
2020 O
) O
hold O
for O
the O
foundation O
models O
for O
science O
? O

How O
does O
model O
scale O
affect O
the O
downstream O
performance O
? O

Scaling O
Effect O

( O
RQ2 O
) O

( O
RQ1 O
) O
Science O
- O
Focused O
Benchmarks O
What O
are O
the O
strengths O
and O
weaknesses O
of O
foundation O
models O
pretrained O
on O
scientific O
literature O
when O
evaluated O
on O
out O
- O
of O
- O
domain O
vs. O
in O
- O
domain O
tasks O
? O

There O
are O
three O
major O
contributions O
of O
this O
work O
: O
( O
1 O
) O
we O
collect O
and O
release O
a O
0.67 O
TB O
dataset O
covering O
research B-DatasetName
publication I-DatasetName
data O
across O
10 O
+ O
sources O
for O
chemistry O
; O
( O
2 O
) O
we O
release O
28 O
auto B-MethodName
- I-MethodName
regressive I-MethodName
foundation I-MethodName
models O
for O
chemistry O
that O
have O
been O
pretrained O
from O
scratch O
; O
and O
( O
3 O
) O
we O
present O
a O
rigorous O
evaluation O
of O
model O
performance O
on O
15 O
+ O
indomain O
and O
out O
- O
of O
- O
domain O
tasks O
that O
investigates O
the O
effects O
of O
model O
and O
data O
scaling O
, O
knowledge O
depth O
( O
aka O
diversity O
) O
, O
and O
temporal O
order O
on O
performance O
as O
described O
in O
research O
questions O
below O
. O

evaluation O
, O
model O
prompting O
and O
interactions O
. O

Challenges O
include O
limited O
benchmarks O
that O
can O
be O
used O
to O
perform O
model O
160 O
Proceedings O
of O
BigScience O
Episode O
# O
5 O
– O
Workshop O
on O
Challenges O
& O
Perspectives O
in O
Creating O
Large O
Language O
Models O
, O
pages O
160 O
- O
172 O
May O
27 O
, O
2022 O
c O
2022 O
Association O
for O
Computational O
Linguistics O

Opportunities O
include O
the O
scale O
and O
diversity O
of O
scientific O
literature O
, O
the O
explicit O
structure O
, O
and O
explicit O
alignment O
across O
different O
modalities O
in O
the O
papers O
, O
e.g. O
, O
table O
and O
figure O
references O
. O

Using O
scientific O
literature O
presents O
unique O
opportunities O
and O
challenges O
. O

Thus O
, O
in O
- O
context O
learning O
allows O
foundation O
models O
to O
be O
effectively O
used O
across O
new O
downstream O
tasks O
with O
only O
simple O
instructions O
and O
a O
few O
optional O
examples O
. O

Another O
key O
advantage O
of O
scaling O
language O
models O
is O
that O
they O
perform O
competitively O
on O
language O
tasks O
using O
in O
- O
context O
learning O
without O
fine O
- O
tuning O
or O
gradient O
updates O
. O

Homogenization O
is O
the O
consolidation O
of O
methods O
for O
building O
machine O
learning O
systems O
across O
a O
wide O
range O
of O
tasks O
. O

Emergence O
, O
or O
emergent O
behavior O
, O
reflect O
new O
behaviors O
that O
a O
model O
introduces O
or O
is O
capable O
of O
that O
it O
was O
not O
explicitly O
trained O
to O
perform O
. O

The O
wide O
community O
adoption O
of O
foundation O
models O
can O
be O
explained O
by O
their O
key O
properties O
, O
two O
of O
which O
are O
emergent O
behavior O
and O
homogenization O
– O
which O
also O
make O
foundation O
models O
appealing O
for O
adaption O
across O
science O
and O
security O
domains O
. O

Our O
novel O
findings O
demonstrate O
that O
( O
1 O
) O
model B-HyperparameterName
size I-HyperparameterName
significantly O
contributes O
to O
the O
task O
performance O
when O
evaluated O
in O
a O
zero O
- O
shot O
setting O
; O
( O
2 O
) O
data O
quality O
( O
aka O
diversity O
) O
affects O
model O
performance O
more O
than O
data O
quantity O
; O
( O
3 O
) O
similarly O
, O
unlike O
previous O
work O
( O
Luu O
et O
al. O
, O
2021 O
) O
temporal O
order O
of O
the O
documents O
in O
the O
corpus O
boosts O
model O
performance O
only O
for O
specific O
tasks O
, O
e.g. O
, O
SciQ B-TaskName
; O
and O
( O
4 O
) O
models O
pre O
- O
trained O
from O
scratch O
perform O
better O
on O
in O
- O
domain O
tasks O
than O
those O
tuned O
from O
general O
- O
purpose O
models O
like O
Open O
AI O
’s O
GPT-2 B-MethodName
. O

Evaluating O
these O
models O
in O
a O
zero O
- O
shot O
setting O
, O
we O
analyze O
the O
effect O
of O
model O
and O
data O
scaling O
, O
knowledge O
depth O
, O
and O
temporality O
on O
model O
performance O
in O
context O
of O
model O
training O
efficiency O
. O

Specifically O
, O
we O
build O
large O
- O
scale O
( O
1.47B B-HyperparameterValue
parameter O
) O
general O
- O
purpose O
models O
for O
chemistry O
that O
can O
be O
effectively O
used O
to O
perform O
a O
wide O
range O
of O
in O
- O
domain O
and O
out O
- O
of O
- O
domain O
tasks O
. O

In O
this O
work O
, O
we O
develop O
foundation O
models O
of O
scientific O
knowledge O
for O
chemistry O
to O
augment O
scientists O
with O
the O
advanced O
ability O
to O
perceive O
and O
reason O
at O
scale O
previously O
unimagined O
. O

However O
, O
only O
limited O
efforts O
have O
investigated O
the O
opportunities O
and O
limitations O
of O
applying O
these O
powerful O
models O
to O
science O
and O
security O
applications O
. O

Sameera O
Horawalavithana O
, O
Ellyn O
Ayton O
, O
Shivam O
Sharma O
, O
Scott O
Howland O
, O
Megha O
Subramanian O
, O
Scott O
Vasquez O
, O
Robin O
Cosbey O
, O
Maria O
Glenski O
, O
Svitlana O
Volkova O
Pacific O
Northwest O
National O
Laboratory O
, O
Richland O
, O
WA O
Abstract O
Foundation O
models O
pre O
- O
trained O
on O
large O
corpora O
demonstrate O
significant O
gains O
across O
many O
natural O
language O
processing O
tasks O
and O
domains O
e.g. O
, O
law O
, O
healthcare O
, O
education O
, O
etc O
. O

Foundation O
Models O
of O
Scientific O
Knowledge O
for O
Chemistry O
: O
Opportunities O
, O
Challenges O
and O
Lessons O
Learned O

-DOCSTART- O
Proceedings O
of O
the O
Fourth O
International O
Workshop O
on O
Semantic O
Evaluations O
( O
SemEval2007 O
) O
. O

Future O
work O
should O
explore O
new O
methods O
for O
corrupting O
documents O
for O
pre O
- O
training O
, O
perhaps O
tailoring O
them O
to O
specific O
end O
tasks O
. O

BART B-MethodName
achieves O
similar O
performance O
to O
RoBERTa B-MethodName
on O
discriminative O
tasks O
, O
while O
achieving O
new O
state O
- O
of O
- O
theart O
results O
on O
a O
number O
of O
text O
generation O
tasks O
. O

8 O
Conclusions O
We O
introduced O
BART B-MethodName
, O
a O
pre O
- O
training O
approach O
that O
learns O
to O
map O
corrupted O
documents O
to O
the O
original O
. O

We O
show O
how O
BART B-MethodName
can O
be O
used O
to O
improve O
machine O
translation O
decoders O
. O

Other O
work O
has O
shown O
that O
encoders O
can O
be O
improved O
using O
pre O
- O
trained O
representations O
( O
Edunov O
et O
al. O
, O
2019 O
) O
, O
but O
gains O
in O
decoders O
are O
more O
limited O
. O

The O
largest O
improvements O
have O
come O
from O
pre O
- O
training O
on O
both O
source O
and O
target O
languages O
( O
Song O
et O
al. O
, O
2019 O
; O
Lample O
& O
Conneau O
, O
2019 O
) O
, O
but O
this O
requires O
pretraining O
on O
all O
languages O
of O
interest O
. O

Several O
papers O
have O
explored O
using O
pre O
- O
trained O
representations O
to O
improve O
machine B-TaskName
translation I-TaskName
. O

In O
contrast O
, O
the O
BART B-MethodName
decoder O
works O
left O
- O
to O
- O
right O
during O
pre O
- O
training O
, O
matching O
the O
setting O
during O
generation O
. O

This O
objective O
allows O
predictions O
to O
condition O
on O
both O
left O
and O
right O
context O
. O

dicting O
masked O
tokens O
auto O
- O
regressively O
in O
a O
permuted O
order O
. O

Summaries O
combine O
information O
from O
across O
the O
article O
and O
prior O
knowledge O
. O

For O
clarity O
, O
only O
relevant O
excerpts O
of O
the O
source O
are O
shown O
. O

Table O
7 O
: O
Example O
summaries O
from O
the O
XSum B-DatasetName
- O
tuned O
BART B-MethodName
model O
on O
WikiNews O
articles O
. O

Source O
Document O
( O
abbreviated O
) O
BART B-MethodName
Summary O

XL B-MethodName
- I-MethodName
Net I-MethodName
( O
Yang O
et O
al. O
, O
2019 O
) O
extends O
BERT O
by O
pre O

MASS B-MethodName
is O
less O
effective O
for O
discriminative O
tasks O
, O
because O
disjoint O
sets O
of O
tokens O
are O
fed O
into O
the O
encoder O
and O
decoder O
. O

An O
input O
sequence O
where O
a O
contiguous O
span O
of O
tokens O
is O
masked O
is O
mapped O
to O
a O
sequence O
consisting O
of O
the O
missing O
tokens O
. O

MASS B-MethodName
( O
Song O
et O
al. O
, O
2019 O
) O
is O
perhaps O
the O
most O
similar O
model O
to O
BART B-MethodName
. O

BART B-MethodName
reduces O
the O
mismatch O
between O
pre O
- O
training O
and O
generation O
tasks O
, O
because O
the O
decoder O
is O
always O
trained O
on O
uncorrupted O
context O
. O

A O
difference O
is O
that O
UniLM B-MethodName
predictions O
are O
conditionally O
independent O
, O
whereas O
BART B-MethodName
’s I-MethodName
are O
autoregressive O
. O

Like O
BART B-MethodName
, O
this O
allows O
UniLM B-MethodName
to O
be O
used O
for O
both O
generative O
and O
discriminative O
tasks O
. O

UniLM B-MethodName
( O
Dong O
et O
al. O
, O
2019 O
) O
fine O
- O
tunes O
BERT O
with O
an O
ensemble O
of O
masks O
, O
some O
of O
which O
allow O
only O
leftward O
context O
. O

Predictions O
are O
not O
made O
auto O
- O
regressively O
, O
reducing O
the O
effectiveness O
of O
BERT B-MethodName
for O
generation O
tasks O
. O

Recent O
work O
has O
shown O
that O
very O
strong O
performance O
can O
be O
achieved O
by O
training O
for O
longer O
( O
Liu O
et O
al. O
, O
2019 O
) O
, O
by O
tying O
parameters O
across O
layers O
( O
Lan O
et O
al. O
, O
2019 O
) O
, O
and O
by O
masking O
spans O
instead O
of O
words O
( O
Joshi O
et O
al. O
, O
2019 O
) O
. O

BERT B-MethodName
( O
Devlin O
et O
al. O
, O
2019 O
) O
introduced O
masked O
language O
modelling O
, O
which O
allows O
pre O
- O
training O
to O
learn O
interactions O
between O
left O
and O
right O
context O
words O
. O

Radford O
et O
al. O
( O
2019 O
) O
demonstrated O
that O
very O
large O
language O
models O
can O
act O
as O
unsupervised O
multitask O
models O
. O

ELMo B-MethodName
( O
Peters O
et O
al. O
, O
2018 O
) O
concatenates O
left O
- O
only O
and O
right O
- O
only O
representations O
, O
but O
does O
not O
pre O
- O
train O
interactions O
between O
these O
features O
. O

GPT B-MethodName
( O
Radford O
et O
al. O
, O
2018 O
) O
only O
models O
leftward O
context O
, O
which O
is O
problematic O
for O
some O
tasks O
. O

Related O
Work O
Early O
methods O
for O
pretraining O
were O
based O
on O
language O
models O
. O

To O
understand O
BART B-MethodName
’s I-MethodName
performance O
beyond O
automated O
metrics O
, O
we O
analyse O
its O
generations O
qualitatively O
. O

6 O
Qualitative O
Analysis O
BART B-MethodName
shows O
large O
improvements O
on O
summarization O
metrics O
, O
of O
up O
to O
6 B-MetricValue
points I-MetricValue
over O
the O
prior O
state O
- O
of O
- O
the O
- O
art O
. O

Preliminary O
results O
suggested O
that O
our O
approach O
was O
less O
effective O
without O
back O
- O
translation O
data O
, O
and O
prone O
to O
overfitting O
— O
future O
work O
should O
explore O
additional O
regularization O
techniques O
. O

We O
use O
a O
beam B-HyperparameterName
width I-HyperparameterName
of O
5 B-HyperparameterValue
and O
a O
length B-HyperparameterName
penalty I-HyperparameterName
of I-HyperparameterName
α I-HyperparameterName
= O
1 B-HyperparameterValue
. O

For O
each O
row O
we O
experiment O
on O
the O
original O
WMT16 B-DatasetName
Romanian I-DatasetName
- I-DatasetName
English I-DatasetName
augmented O
with O
back O
- O
translation O
data O
. O

We O
show O
the O
performance O
of O
both O
steps O
of O
our O
model O
in O
the O
fixed O
BART B-MethodName
and O
tuned O
BART B-MethodName
rows O
. O

We O
compare O
our O
results O
against O
a O
baseline O
Transformer O
architecture O
( O
Vaswani O
et O
al. O
, O
2017 O
) O
with O
Transformerlarge O
settings O
( O
the O
baseline O
row O
) O
. O

Experiment O
results O
are O
presented O
in O
Table O
6 O
. O

We O
use O
a O
6 B-HyperparameterValue
- O
layer B-HyperparameterName
transformer O
source O
encoder O
to O
map O
Romanian O
into O
a O
representation O
that O
BART B-MethodName
is O
able O
to O
de O
- O
noise O
into O
English O
, O
following O
the O
approach O
introduced O
in O
§ O
3.4 O
. O

5.4 O
Translation B-TaskName
We O
also O
evaluated O
performance O
on O
WMT16 B-DatasetName
RomanianEnglish I-DatasetName
, O
augmented O
with O
back O
- O
translation O
data O
from O
Sennrich O
et O
al. O
( O
2016 O
) O
. O

We O
find O
BART B-MethodName
outperforms O
the O
best O
previous O
work O
by O
1.2 B-MetricValue
ROUGE B-MetricName
- I-MetricName
L I-MetricName
, O
but O
the O
dataset O
remains O
a O
challenging O
, O
because O
answers O
are O
only O
weakly O
specified O
by O
the O
question O
. O

7 O
Abstractive B-TaskName
QA I-TaskName
We O
use O
the O
recently O
proposed O
ELI5 B-DatasetName
dataset O
to O
test O
the O
model O
’s O
ability O
to O
generate O
long O
freeform O
answers O
. O

These O
samples O
demonstrate O
that O
the O
BART B-MethodName
pretraining O
has O
learned O
a O
strong O
combination O
of O
natural O
language O
understanding O
and O
generation O
. O

However O
, O
the O
claim O
that O
the O
work O
was O
published O
in O
Science O
is O
not O
supported O
by O
the O
source O
. O

Table O
7 O
shows O
example O
summaries O
generated O
by O
BART B-MethodName
. O

BART B-MethodName
improves O
over O
a O
strong O
backtranslation O
( O
BT O
) O
baseline O
by O
using O
monolingual O
English O
pre O
- O
training O
. O

RO B-DatasetName
- I-DatasetName
EN I-DatasetName
Baseline B-MethodName
Fixed B-MethodName
BART I-MethodName
Tuned B-MethodName
BART I-MethodName
36.80 B-MetricValue
36.29 B-MetricValue
37.96 B-MetricValue
Table O
6 O
: O
The O
performance O
( O
BLEU B-MetricName
) O
of O
baseline O
and O
BART B-MethodName
on O
WMT’16 B-DatasetName
RO I-DatasetName
- I-DatasetName
EN I-DatasetName
augmented O
with O
backtranslation O
data O
. O

Best B-MethodName
Extractive I-MethodName
Language I-MethodName
Model I-MethodName
Seq2Seq B-MethodName
Seq2Seq B-MethodName
Multitask I-MethodName
BART B-MethodName
R1 B-MetricName
ELI5 B-DatasetName
R2 B-MetricName
RL B-MetricName
23.5 B-MetricValue
27.8 B-MetricValue
28.3 B-MetricValue
28.9 B-MetricValue
30.6 B-MetricValue
3.1 B-MetricValue
4.7 B-MetricValue
5.1 B-MetricValue
5.4 B-MetricValue
6.2 B-MetricValue
17.5 B-MetricValue
23.1 B-MetricValue
22.8 B-MetricValue
23.1 B-MetricValue
24.3 B-MetricValue
Table O
5 O
: O
BART B-MethodName
achieves O
state O
- O
of O
- O
the O
- O
art O
results O
on O
the O
challenging O
ELI5 B-DatasetName
abstractive O
question O
answering O
dataset O
. O

BART B-MethodName
outperforms O
previous O
work O
on O
two O
automated O
metrics O
. O

Dialogue B-TaskName
We O
evaluate O
dialogue B-TaskName
response I-TaskName
generation I-TaskName
on O
C B-DatasetName
ONVAI2 I-DatasetName
( O
Dinan O
et O
al. O
, O
2019 O
) O
, O
in O
which O
agents O
must O
generate O
responses O
conditioned O
on O
both O
the O
previous O
context O
and O
a O
textually O
- O
specified O
persona O
. O

BART B-MethodName
outperforms O
the O
best O
previous O
work O
, O
which O
leverages O
BERT B-MethodName
, O
by O
roughly O
6.0 B-MetricValue
points I-MetricValue
on O
all O
ROUGE B-MetricName
metrics O
— O
representing O
a O
significant O
advance O
in O
performance O
on O
this O
problem O
. O

In O
contrast O
, O
XSum B-DatasetName
is O
highly O
abstractive O
, O
and O
extractive O
models O
perform O
poorly O
. O

Nevertheless O
, O
BART B-MethodName
outperforms O
all O
existing O
work O
. O

Summaries O
in O
the O
CNN B-DatasetName
/ I-DatasetName
DailyMail I-DatasetName
tend O
to O
resemble O
source O
sentences O
. O

Summarization B-TaskName
To O
provide O
a O
comparison O
with O
the O
state O
- O
of O
- O
the O
- O
art O
in O
summarization O
, O
we O
present O
results O
on O
two O
summarization O
datasets O
, O
CNN B-DatasetName
/ I-DatasetName
DailyMail I-DatasetName
and O
XSum B-DatasetName
, O
which O
have O
distinct O
properties O
. O

Perplexities B-MetricName
are O
renormalized O
based O
on O
official O
tokenizer O
for O
ConvAI2 B-DatasetName
. O

Attention B-MethodName
Best B-MethodName
System I-MethodName
BART B-MethodName
16.02 B-MetricValue
19.09 B-MetricValue
20.72 B-MetricValue
35.07 B-MetricValue
17.51 B-MetricValue
11.85 B-MetricValue
Table O
4 O
: O
BART B-MethodName
outperforms O
previous O
work O
on O
conversational B-TaskName
response I-TaskName
generation I-TaskName
. O

ConvAI2 B-DatasetName
Valid B-MetricName
F1 I-MetricName
Valid B-MetricName
PPL I-MetricName
Seq2Seq B-MethodName
+ I-MethodName

During O
generation O
, O
we O
set O
beam B-HyperparameterName
size I-HyperparameterName
as O
5 B-HyperparameterValue
, O
remove O
duplicated O
trigrams O
in O
beam O
search O
, O
and O
tuned O
the O
model O
with O
min O
- O
len O
, O
max O
- O
len O
, O
length O
penalty O
on O
the O
validation O
set O
( O
Fan O
et O
al. O
, O
2017 O
) O
. O

During O
finetuning O
we O
use O
a O
label O
smoothed O
cross O
entropy O
loss O
( O
Pereyra O
et O
al. O
, O
2017 O
) O
, O
with O
the O
smoothing B-HyperparameterName
parameter I-HyperparameterName
set O
to O
0.1 B-HyperparameterValue
. O

BART B-MethodName
is O
fine O
- O
tuned O
as O
a O
standard O
sequence O
- O
to O
- O
sequence O
model O
from O
the O
input O
to O
the O
output O
text O
. O

5.3 O
Generation B-TaskName
Tasks I-TaskName
We O
also O
experiment O
with O
several O
text B-TaskName
generation I-TaskName
tasks O
. O

suggesting O
that O
BART B-MethodName
’s I-MethodName
improvements O
on O
generation B-TaskName
tasks I-TaskName
do O
not O
come O
at O
the O
expense O
of O
classification O
performance O
. O

Overall O
, O
BART B-MethodName
performs O
similarly O
, O
with O
only O
small O
differences O
between O
the O
models O
on O
most O
tasks O
. O

The O
most O
directly O
comparable O
baseline O
is O
RoBERTa B-MethodName
, O
which O
was O
pre O
- O
trained O
with O
the O
same O
resources O
, O
but O
a O
different O
objective O
. O

5.2 O
Discriminative O
Tasks O
Table O
2 O
compares O
the O
performance O
of O
BART B-MethodName
with O
several O
recent O
approaches O
on O
the O
well O
- O
studied O
SQuAD B-DatasetName
and O
GLUE B-DatasetName
tasks O
( O
Warstadt O
et O
al. O
, O
2018 O
; O
Socher O
et O
al. O
, O
2013 O
; O
Dolan O
& O
Brockett O
, O
2005 O
; O
Agirre O
et O
al. O
, O
2007 O
; O
Williams O
et O
al. O
, O
2018 O
; O
Dagan O
et O
al. O
, O
2006 O
; O
Levesque O
et O
al. O
, O
2011 O
) O
. O

To O
help O
the O
model O
better O
fit O
the O
data O
, O
we O
disabled O
dropout B-HyperparameterName
for O
the O
final O
10 B-HyperparameterValue
% I-HyperparameterValue
of O
training B-HyperparameterName
steps I-HyperparameterName
. O

on O
the O
CNN B-DatasetName
/ I-DatasetName
DM I-DatasetName
summarization I-DatasetName
dataset I-DatasetName
, O
we O
hypothesised O
that O
larger O
pre O
- O
trained O
models O
may O
be O
better O
able O
to O
learn O
from O
this O
task O
. O

BART B-MethodName
outperforms O
previous O
work O
on O
summarization O
on O
two O
tasks O
and O
all O
metrics O
, O
with O
gains O
of O
roughly O
6 B-MetricValue
points I-MetricValue
on O
the O
more O
abstractive O
dataset O
. O

PTGEN+COV B-MethodName
( O
See O
et O
al. O
, O
2017 O
) O
UniLM B-MethodName
BERTSUMABS B-MethodName
( O
Liu O
& O
Lapata O
, O
2019 O
) O
BERTSUMEXTABS B-MethodName
( O
Liu O
& O
Lapata O
, O
2019 O
) O
40.42 B-MetricValue
36.44 B-MetricValue
39.53 B-MetricValue
43.33 B-MetricValue
41.72 B-MetricValue
42.13 B-MetricValue
17.62 B-MetricValue
15.66 B-MetricValue
17.28 B-MetricValue
20.21 B-MetricValue
19.39 B-MetricValue
19.60 B-MetricValue
36.67 B-MetricValue
33.42 B-MetricValue
36.38 B-MetricValue
40.51 B-MetricValue
38.76 B-MetricValue
39.18 B-MetricValue
16.30 B-MetricValue
29.70 B-MetricValue
28.10 B-MetricValue
38.76 B-MetricValue
38.81 B-MetricValue
1.60 B-MetricValue
9.21 B-MetricValue
8.02 B-MetricValue
16.33 B-MetricValue
16.50 B-MetricValue
11.95 B-MetricValue
23.24 B-MetricValue
21.72 B-MetricValue
31.15 B-MetricValue
31.27 B-MetricValue
BART B-MethodName
44.16 B-MetricValue
21.28 B-MetricValue
40.90 B-MetricValue
45.14 B-MetricValue
22.27 B-MetricValue
37.25 B-MetricValue
Table O
3 O
: O
Results O
on O
two O
standard O
summarization O
datasets O
. O

CNN B-DatasetName
/ I-DatasetName
DailyMail I-DatasetName
R1 B-MetricName
R2 B-MetricName
RL B-MetricName
R1 B-MetricName
XSum B-DatasetName
R2 B-MetricName
RL B-MetricName
Lead-3 B-MethodName
PTGEN B-MethodName
( O
See O
et O
al. O
, O
2017 O
) O

BART B-MethodName
performs O
comparably O
to O
RoBERTa B-MethodName
and O
XLNet B-MethodName
, O
suggesting O
that O
BART B-MethodName
’s O
uni O
- O
directional O
decoder O
layers O
do O
not O
reduce O
performance O
on O
discriminative O
tasks O
. O

88.9/94.6 B-MetricValue
88.8/94.6 B-MetricValue
79.0/81.8 B-MetricValue
80.5/83.4 B-MetricValue
86.1/88.8 B-MetricValue
86.5/89.4 B-MetricValue
86.1/89.2 B-MetricValue
86.6/87.0/85.9 B-MetricValue
89.8/90.2/90.2 B-MetricValue
89.9/90.1 B-MetricValue
93.2 B-MetricValue
94.5 B-MetricValue
95.6 B-MetricValue
96.4 B-MetricValue
96.6 B-MetricValue
91.3 B-MetricValue
91.8 B-MetricValue
92.2 B-MetricValue
92.5 B-MetricValue
92.3 B-MetricValue
92.7 B-MetricValue
93.9 B-MetricValue
94.7 B-MetricValue
94.9 B-MetricValue
90.0 B-MetricValue
91.8 B-MetricValue
92.4 B-MetricValue
91.2 B-MetricValue
70.4 B-MetricValue
70.9 B-MetricValue
83.8 B-MetricValue
86.6 B-MetricValue
87.0 B-MetricValue
88.0 B-MetricValue
89.2 B-MetricValue
90.9 B-MetricValue
90.4 B-MetricValue
60.6 B-MetricValue
61.1 B-MetricValue
63.6 B-MetricValue
68.0 B-MetricValue
62.8 B-MetricValue
BERT B-MethodName
UniLM B-MethodName
XLNet B-MethodName
RoBERTa B-MethodName
BART B-MethodName
Table O
2 O
: O
Results O
for O
large O
models O
on O
SQuAD B-DatasetName
and O
GLUE B-DatasetName
tasks O
. O

Acc B-MetricName
STS B-DatasetName
- I-DatasetName
B I-DatasetName
Acc B-MetricName
RTE B-DatasetName
Acc B-MetricName
MRPC B-DatasetName
Acc B-MetricName
CoLA B-DatasetName
Mcc B-MetricName
84.1/90.9 B-MetricValue
-/89.0/94.5 B-MetricValue

m B-MetricName
/ O
mm B-MetricName
SST B-DatasetName
Acc B-MetricName
QQP B-DatasetName
Acc B-MetricName
QNLI B-DatasetName

SQuAD B-DatasetName
1.1 I-DatasetName
EM B-MetricName
/ O
F1 B-MetricName
SQuAD B-DatasetName
2.0 I-DatasetName
EM B-MetricName
/ O
F1 B-MetricName
MNLI B-DatasetName

Documents O
are O
tokenized O
with O
the O
same O
byte O
- O
pair O
encoding O
as O
GPT-2 B-MethodName
( O
Radford O
et O
al. O
, O
2019 O
) O
. O

Following O
RoBERTa B-MethodName
( O
Liu O
et O
al. O
, O
2019 O
) O
, O
we O
use O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
8000 B-HyperparameterValue
, O
and O
train O
the O
model O
for O
500000 B-HyperparameterValue
steps B-HyperparameterName
. O

5.1 O
Experimental O
Setup O
We O
pre O
- O
train O
a O
large O
model O
with O
12 B-HyperparameterValue
layers B-HyperparameterName
in O
each O
of O
the O
encoder O
and O
decoder O
, O
and O
a O
hidden B-HyperparameterName
size I-HyperparameterName
of O
1024 B-HyperparameterValue
. O

To O
test O
how O
well O
BART B-MethodName
performs O
in O
this O
regime O
, O
and O
to O
create O
a O
useful O
model O
for O
downstream O
tasks O
, O
we O
trained O
BART B-MethodName
using O
the O
same O
scale O
as O
the O
RoBERTa B-MethodName
model O
. O

Large O
- O
scale O
Pre O
- O
training O
Experiments O
Recent O
work O
has O
shown O
that O
downstream O
performance O
can O
dramatically O
improve O
when O
pre O
- O
training O
is O
scaled O
to O
large O
batch B-HyperparameterName
sizes I-HyperparameterName
( O
Yang O
et O
al. O
, O
2019 O
; O
Liu O
et O
al. O
, O
2019 O
) O
and O
corpora O
. O

The O
pre O
- O
training O
objective O
is O
not O
the O
only O
important O
factor O
Our O
Permuted O
Language O
Model O
performs O
less O
well O
than O
XLNet B-MethodName
( O
Yang O
et O
al. O
, O
2019 O
) O
. O

However O
, O
BART B-MethodName
achieves O
similar O
performance O
with O
only O
half B-HyperparameterValue
the O
number B-HyperparameterName
of I-HyperparameterName
bidirectional I-HyperparameterName
layers I-HyperparameterName
. O

Bidirectional O
encoders O
are O
crucial O
for O
SQuAD B-DatasetName
As O
noted O
in O
previous O
work O
( O
Devlin O
et O
al. O
, O
2019 O
) O
, O
just O
left O
- O
to O
- O
right O
decoder O
performs O
poorly O
on O
SQuAD B-DatasetName
, O
because O
future O
context O
is O
crucial O
in O
classification O
decisions O
. O

With O
the O
exception O
of O
ELI5 B-DatasetName
, O
BART B-MethodName
models O
using O
text O
- O
infilling O
perform O
well O
on O
all O
tasks O
. O

BART B-MethodName
achieves O
the O
most O
consistently O
strong O
performance O
. O

A O
pure O
language O
model O
performs O
best O
, O
suggesting O
that O
BART B-MethodName
is O
less O
effective O
when O
the O
output O
is O
only O
loosely O
constrained O
by O
the O
input O
. O

Pure O
language O
models O
perform O
best O
on O
ELI5 B-DatasetName
The O
ELI5 B-DatasetName
dataset O
is O
an O
outlier O
, O
with O
much O
higher O
perplexities B-MetricName
than O
other O
tasks O
, O
and O
is O
the O
only O
generation B-TaskName
task I-TaskName
where O
other O
models O
outperform O
BART B-MethodName
. O

Deletion O
appears O
to O
outperform O
masking O
on O
generation B-TaskName
tasks O
. O

For O
example O
, O
a O
simple O
language O
model O
achieves O
the O
best O
ELI5 B-DatasetName
performance O
, O
but O
the O
worst O
SQUAD B-DatasetName
results O
. O

Performance O
varies O
considerably O
across O
tasks O
, O
but O
the O
BART B-MethodName
models O
with O
text O
infilling O
demonstrate O
the O
most O
consistently O
strong O
performance O
. O

All O
models O
are O
of O
comparable O
size O
and O
are O
trained O
for O
1 B-HyperparameterValue
M I-HyperparameterValue
steps B-HyperparameterName
on O
a O
combination O
of O
books O
and O
Wikipedia O
data O
. O

Shuffling O
w/ O
Text O
Infilling O
+ O
Sentence O
Shuffling O
90.4 B-MetricValue
90.4 B-MetricValue
90.8 B-MetricValue
77.2 B-MetricValue
85.4 B-MetricValue
90.8 B-MetricValue
84.1 B-MetricValue
84.1 B-MetricValue
84.0 B-MetricValue
75.3 B-MetricValue
81.5 B-MetricValue
83.8 B-MetricValue
25.05 B-MetricValue
24.61 B-MetricValue
24.26 B-MetricValue
53.69 B-MetricValue
41.87 B-MetricValue
24.17 B-MetricValue
7.08 B-MetricValue
6.90 B-MetricValue
6.61 B-MetricValue
17.14 B-MetricValue
10.93 B-MetricValue
6.62 B-MetricValue
11.73 B-MetricValue
11.46 B-MetricValue
11.05 B-MetricValue
19.87 B-MetricValue
16.67 B-MetricValue
11.12 B-MetricValue
6.10 B-MetricValue
5.87 B-MetricValue
5.83 B-MetricValue
10.59 B-MetricValue
7.89 B-MetricValue
5.41 B-MetricValue
Table O
1 O
: O
Comparison O
of O
pre O
- O
training O
objectives O
. O

Acc B-MetricName
ELI5 B-DatasetName
PPL B-MetricName
XSum B-DatasetName
PPL B-MetricName
ConvAI2 B-DatasetName
PPL B-MetricName
CNN B-DatasetName
/ I-DatasetName
DM I-DatasetName
PPL B-MetricName
BERT B-MethodName
Base I-MethodName
( O
Devlin O
et O
al. O
, O
2019 O
) O
88.5 B-MetricValue
84.3 B-MetricValue
Masked O
Language O
Model O
Masked O
Seq2seq O
Language O
Model O
Permuted O
Language O
Model O
Multitask O
Masked O
Language O
Model O
90.0 B-MetricValue
87.0 B-MetricValue
76.7 B-MetricValue
89.1 B-MetricValue
89.2 B-MetricValue
83.5 B-MetricValue
82.1 B-MetricValue
80.1 B-MetricValue
83.7 B-MetricValue
82.4 B-MetricValue
24.77 B-MetricValue
23.40 B-MetricValue
21.40 B-MetricValue
24.03 B-MetricValue
23.73 B-MetricValue
7.87 B-MetricValue
6.80 B-MetricValue
7.00 B-MetricValue
7.69 B-MetricValue
7.50 B-MetricValue
12.59 B-MetricValue
11.43 B-MetricValue
11.51 B-MetricValue
12.23 B-MetricValue
12.39 B-MetricValue
7.06 B-MetricValue
6.19 B-MetricValue
6.56 B-MetricValue
6.96 B-MetricValue
6.74 B-MetricValue
BART B-MethodName
Base I-MethodName
w/ O

Model O
SQuAD B-DatasetName
1.1 I-DatasetName
F1 B-MetricName
MNLI B-DatasetName

CNN B-DatasetName
/ I-DatasetName
DM I-DatasetName
( O
Hermann O
et O
al. O
, O
2015 O
) O
, O
a O
news O
summarization B-TaskName
dataset O
. O

ConvAI2 B-DatasetName
( O
Dinan O
et O
al. O
, O
2019 O
) O
, O
a O
dialogue B-TaskName
response I-TaskName
generation I-TaskName
task I-TaskName
, O
conditioned O
on O
context O
and O
a O
persona O
. O

XSum B-DatasetName
( O
Narayan O
et O
al. O
, O
2018 O
) O
, O
a O
news O
summarization B-TaskName
dataset O
with O
highly O
abstractive O
summaries O
. O

ELI5 B-DatasetName
( O
Fan O
et O
al. O
, O
2019 O
) O
, O
a O
long B-TaskName
- I-TaskName
form I-TaskName
abstractive I-TaskName
question I-TaskName
answering I-TaskName
dataset O
. O

In O
contrast O
to O
BERT B-MethodName
, O
the O
representation O
of O
the O
EOS O
token O
is O
used O
to O
classify O
the O
sentences O
relations O
. O

The O
fine O
- O
tuned O
model O
concatenates O
the O
two O
sentences O
with O
appended O
an O
EOS O
token O
, O
and O
passes O
them O
to O
both O
the O
BART B-MethodName
encoder O
and O
decoder O
. O

MNLI B-DatasetName
( O
Williams O
et O
al. O
, O
2017 O
) O
, O
a O
bitext B-TaskName
classification I-TaskName
task I-TaskName
to O
predict O
whether O
one O
sentence O
entails O
another O
. O

Similar O
to O
BERT B-MethodName
( O
Devlin O
et O
al. O
, O
2019 O
) O
, O
we O
use O
concatenated O
question O
and O
context O
as O
input O
to O
the O
encoder O
of O
BART B-MethodName
, O
and O
additionally O
pass O
them O
to O
the O
decoder O
. O

Tasks O
SQuAD B-DatasetName
( O
Rajpurkar O
et O
al. O
, O
2016)a O
an O
extractive B-TaskName
question I-TaskName
answering I-TaskName
task I-TaskName
on O
Wikipedia O
paragraphs O
. O

Masked O
Seq O
- O
to O
- O
Seq O
Inspired O
by O
MASS B-MethodName
( O
Song O
et O
al. O
, O
2019 O
) O

As O
in O
UniLM B-MethodName
( O
Dong O
et O
al. O
, O
2019 O
) O
, O
we O
train O
a O
Masked O
Language O
Model O
with O
additional O
self O
- O
attention O
masks O
. O

Masked O
Language O
Model O
Following O
BERT B-MethodName
( O
Devlin O
et O
al. O
, O
2019 O
) O
, O
we O
replace O
15 O
% O
of O
tokens O
with O
[ O
MASK O
] O
symbols O
, O
and O
train O
the O
model O
to O
independently O
predict O
the O
original O
tokens O
. O

For O
consistency O
with O
other O
models O
, O
we O
do O
not O
implement O
the O
relative O
positional O
embeddings O
or O
attention O
across O
segments O
from O
XLNet B-MethodName
. O

Permuted O
Language O
Model O
Based O
on O
XLNet B-MethodName
( O
Yang O
et O
al. O
, O
2019 O
) O
, O
we O
sample O
1/6 O
of O
the O
tokens O
, O
and O
generate O
them O
in O
a O
random O
order O
autoregressively O
. O

This O
model O
is O
equivalent O
to O
the O
BART B-MethodName
decoder O
, O
without O
cross O
- O
attention O
. O

To O
most O
directly O
compare O
our O
models O
on O
their O
ability O
to O
model O
their O
fine O
- O
tuning O
objective O
( O
the O
log O
likelihood O
of O
the O
human O
text O
) O
, O
we O
report O
perplexity O
in O
Table O
1 O
. O
4.2 O
Language O
Model O
Similarly O
to O
GPT B-MethodName
( O
Radford O
et O
al. O
, O
2018 O
) O
, O
we O
train O
a O
left O
- O
to O
- O
right O
Transformer O
language O
model O
. O

We O
find O
the O
former O
works O
better O
for O
BART B-MethodName
models O
, O
and O
the O
latter O
for O
other O
models O
. O

For O
reference O
, O
we O
compare O
our O
implementations O
with O
published O
numbers O
from O
BERT B-MethodName
, O
which O
was O
also O
trained O
for O
1 B-HyperparameterValue
M I-HyperparameterValue
steps B-HyperparameterName
on O
a O
combination O
of O
books O
and O
Wikipedia O
data O
. O

However O
, O
we O
do O
make O
minor O
changes O
to O
the O
learning B-HyperparameterName
rate I-HyperparameterName
and O
usage O
of O
layer B-HyperparameterName
normalisation I-HyperparameterName
in O
order O
to O
improve O
performance O
( O
tuning O
these O
separately O
for O
each O
objective O
) O
. O

Figure O
3 O
: O
Fine O
tuning O
BART B-MethodName
for O
classification B-TaskName
and O
translation B-TaskName
. O
re O
- O
implement O
strong O
pre O
- O
training O
approaches O
recently O
proposed O
for O
discriminative O
and O
generation O
tasks O
. O

, O
we O
learn O
a O
small O
additional O
encoder O
that O
replaces O
the O
word O
embeddings O
in O
BART B-MethodName
. O

β O
γ O
δ O
ε O
( O
b O
) O
For O
machine B-TaskName
translation I-TaskName

A O
B O
C O
D O
E O
label O
Pre O
- O
trained O
Encoder O
Pre O
- O
trained O
Decoder O
A O
B O
C O
D O
E O
< O
s O
> O
A O
B O
C O
D O
E O
( O
a O
) O
To O
use O
BART B-MethodName
for O
classification O
problems O
, O
the O
same O
input O
is O
fed O
into O
the O
encoder O
and O
decoder O
, O
and O
the O
representation O
from O
the O
final O
output O
is O
used O
. O

We O
compare O
a O
range O
of O
options O
using O
base O
- O
size O
models O
( O
6 B-HyperparameterValue
encoder O
and O
6 B-HyperparameterValue
decoder O
layers B-HyperparameterName
, O
with O
a O
hidden B-HyperparameterName
size I-HyperparameterName
of O
768 B-HyperparameterValue
) O
, O
evaluated O
on O
a O
representative O
subset O
of O
the O
tasks O
we O
will O
consider O
for O
the O
full O
large O
scale O
experiments O
in O
§ O
5 O
. O
4.1 O
Comparison O
Objectives O
While O
many O
pre O
- O
training O
objectives O
have O
been O
proposed O
, O
fair O
comparisons O
between O
these O
have O
been O
difficult O
to O
perform O
, O
at O
least O
in O
part O
due O
to O
differences O
in O
training O
data O
, O
training O
resources O
, O
architectural O
differences O
between O
models O
, O
and O
fine O
- O
tuning O
procedures O
. O

4 O
Comparing O
Pre O
- O
training O
Objectives O
BART B-MethodName
supports O
a O
much O
wider O
range O
of O
noising O
schemes O
during O
pre O
- O
training O
than O
previous O
work O
. O

In O
the O
second O
step O
, O
we O
train O
all O
model O
parameters O
for O
a O
small O
number B-HyperparameterName
of I-HyperparameterName
iterations I-HyperparameterName
. O

In O
the O
first O
step O
, O
we O
freeze O
most O
of O
BART B-MethodName
parameters O
and O
only O
update O
the O
randomly O
initialized O
source O
encoder O
, O
the O
BART B-MethodName
positional O
embeddings O
, O
and O
the O
self O
- O
attention O
input O
projection O
matrix O
of O
BART B-MethodName
’s I-MethodName
encoder O
first O
layer O
. O

We O
train O
the O
source O
encoder O
in O
two O
steps O
, O
in O
both O
cases O
backpropagating O
the O
cross O
- O
entropy O
loss O
from O
the O
output O
of O
the O
BART B-MethodName
model O
. O

The O
new O
encoder O
can O
use O
a O
separate O
vocabulary O
from O
the O
original O
BART B-MethodName
model O
. O

The O
model O
is O
trained O
end O
- O
to O
- O
end O
, O
which O
trains O
the O
new O
encoder O
to O
map O
foreign O
words O
into O
an O
input O
that O
BART B-MethodName
can O
de O
- O
noise O
to O
English O
. O

More O
precisely O
, O
we O
replace O
BART B-MethodName
’s O
encoder O
embedding O
layer O
with O
a O
new O
randomly O
initialized O
encoder O
. O

We O
show O
that O
it O
is O
possible O
to O
use O
the O
entire O
BART B-MethodName
model O
( O
both O
encoder O
and O
decoder O
) O
as O
a O
single O
pretrained O
decoder O
for O
machine B-TaskName
translation I-TaskName
, O
by O
adding O
a O
new O
set O
of O
encoder O
parameters O
that O
are O
learned O
from O
bitext O
( O
see O
Figure O
3b O
) O
. O

3.4 O
Machine B-TaskName
Translation I-TaskName
We O
also O
explore O
using O
BART B-MethodName
to O
improve O
machine O
translation O
decoders O
for O
translating O
into O
English O
. O

3.3 O
Sequence B-TaskName
Generation I-TaskName
Tasks O
Because O
BART B-MethodName
has O
an O
autoregressive O
decoder O
, O
it O
can O
be O
directly O
fine O
tuned O
for O
sequence O
generation O
tasks O
such O
as O
abstractive B-TaskName
question I-TaskName
answering I-TaskName
and O
summarization B-TaskName
. O

This O
representation O
is O
used O
to O
classify O
the O
token O
. O

3.2 O
Token B-TaskName
Classification I-TaskName
Tasks O
For O
token O
classification O
tasks O
, O
such O
as O
answer B-TaskName
endpoint I-TaskName
classification I-TaskName
for O
SQuAD B-DatasetName
, O
we O
feed O
the O
complete O
document O
into O
the O
encoder O
and O
decoder O
, O
and O
use O
the O
top O
hidden O
state O
of O
the O
decoder O
as O
a O
representation O
for O
each O
word O
. O

This O
approach O
is O
related O
to O
the O
CLS O
token O
in O
BERT B-MethodName
; O
however O
we O
add O
the O
additional O
token O
to O
the O
end O
so O
that O
representation O
for O
the O
token O
in O
the O
decoder O
can O
attend O
to O
decoder O
states O
from O
the O
complete O
input O
( O
Figure O
3a O
) O
. O

3.1 O
Sequence B-TaskName
Classification I-TaskName
Tasks O
For O
sequence B-TaskName
classification I-TaskName
tasks O
, O
the O
same O
input O
is O
fed O
into O
the O
encoder O
and O
decoder O
, O
and O
the O
final O
hidden O
state O
of O
the O
final O
decoder O
token O
is O
fed O
into O
new O
multi O
- O
class O
linear O
classifier O
. O

The O
representations O
produced O
by O
BART B-MethodName
can O
be O
used O
in O
several O
ways O
for O
downstream O
applications O
. O

3 O
Fine O
- O
tuning O
BART B-MethodName

Text O
infilling O
is O
inspired O
by O
SpanBERT B-MethodName
( O
Joshi O
et O
al. O
, O
2019 O
) O
, O
but O
SpanBERT B-MethodName
samples O
span O
lengths O
from O
a O
different O
( O
clamped O
geometric O
) O
distribution O
, O
and O
replaces O
each O
span O
with O
a O
sequence O
of O
[ O
MASK O
] O
tokens O
of O
exactly O
the O
same O
length O
. O

In O
contrast O
to O
token O
masking O
, O
the O
model O
must O
decide O
which O
positions O
are O
missing O
inputs O
. O

Token O
Deletion O
Random O
tokens O
are O
deleted O
from O
the O
input O
. O

Token O
Masking O
Following O
BERT B-MethodName
( O
Devlin O
et O
al. O
, O
2019 O
) O
, O
random O
tokens O
are O
sampled O
and O
replaced O
with O
[ O
MASK O
] O
elements O
. O

The O
transformations O
we O
used O
are O
summarized O
below O
, O
and O
examples O
are O
shown O
in O
Figure O
2 O
. O

We O
experiment O
with O
several O
previously O
proposed O
and O
novel O
transformations O
, O
but O
we O
believe O
there O
is O
a O
significant O
potential O
for O
development O
of O
other O
new O
alternatives O
. O

In O
the O
extreme O
case O
, O
where O
all O
information O
about O
the O
source O
is O
lost O
, O
BART B-MethodName
is O
equivalent O
to O
a O
language O
model O
. O

Unlike O
existing O
denoising O
autoencoders O
, O
which O
are O
tailored O
to O
specific O
noising O
schemes O
, O
BART B-MethodName
allows O
us O
to O
apply O
any O
type O
of O
document O
corruption O
. O

2.2 O
Pre O
- O
training O
BART B-MethodName
BART B-MethodName
is O
trained O
by O
corrupting O
documents O
and O
then O
optimizing O
a O
reconstruction O
loss O
— O
the O
cross O
- O
entropy O
between O
the O
decoder O
’s O
output O
and O
the O
original O
document O
. O

In O
total O
, O
BART B-MethodName
contains O
roughly O
10 O
% O
more O
parameters O
than O
the O
equivalently O
sized O
BERT B-MethodName
model O
. O

The O
architecture O
is O
closely O
related O
to O
that O
used O
in O
BERT B-MethodName
, O
with O
the O
following O
differences O
: O
( O
1 O
) O
each O
layer O
of O
the O
decoder O
additionally O
performs O
cross O
- O
attention O
over O
the O
final O
hidden O
layer O
of O
the O
encoder O
( O
as O
in O
the O
transformer O
sequence O
- O
to O
- O
sequence O
model O
) O
; O
and O
( O
2 O
) O
BERT B-MethodName
uses O
an O
additional O
feed O
- O
forward O
network O
before O
wordprediction O
, O
which O
BART B-MethodName
does O
not O
. O

For O
our O
base O
model O
, O
we O
use O
6 B-HyperparameterValue
layers B-HyperparameterName
in O
the O
encoder O
and O
de O
coder O
, O
and O
for O
our O
large O
model O
we O
use O
12 B-HyperparameterValue
layers B-HyperparameterName
in O
each O
. O

2.1 O
Architecture O
BART B-MethodName
uses O
the O
standard O
sequence O
- O
to O
- O
sequence O
Transformer O
architecture O
from O
( O
Vaswani O
et O
al. O
, O
2017 O
) O
, O
except O
, O
following O
GPT B-MethodName
, O
that O
we O
modify O
ReLU O
activation O
functions O
to O
GeLUs O
( O
Hendrycks O
& O
Gimpel O
, O
2016 O
) O
and O
initialise O
parameters O
from O
N O
( O
0 O
, O
0.02 O
) O
. O

For O
pre O
- O
training O
, O
we O
optimize O
the O
negative O
log O
likelihood O
of O
the O
original O
document O
. O

It O
is O
implemented O
as O
a O
sequence O
- O
to O
- O
sequence O
model O
with O
a O
bidirectional O
encoder O
over O
corrupted O
text O
and O
a O
left O
- O
to O
- O
right O
autoregressive O
decoder O
. O

2 O
Model O
BART B-MethodName
is O
a O
denoising O
autoencoder O
that O
maps O
a O
corrupted O
document O
to O
the O
original O
document O
it O
was O
derived O
from O
. O

We O
find O
that O
BART B-MethodName
exhibits O
the O
most O
consistently O
strong O
performance O
across O
the O
full O
range O
of O
tasks O
we O
consider O
. O

This O
study O
allows O
us O
to O
carefully O
control O
for O
a O
number O
of O
factors O
, O
including O
data O
and O
optimization O
parameters O
, O
which O
have O
been O
shown O
to O
be O
as O
important O
for O
overall O
performance O
as O
the O
selection O
of O
training O
objectives O
( O
Liu O
et O
al. O
, O
2019 O
) O
. O

To O
better O
understand O
these O
effects O
, O
we O
also O
report O
an O
ablation O
analysis O
that O
replicates O
other O
recently O
proposed O
training O
objectives O
. O

This O
approach O
improves O
performance O
over O
a O
strong O
back O
- O
translation O
MT B-TaskName
baseline O
by O
1.1 B-MetricValue
BLEU B-MetricName
on O
the O
WMT B-DatasetName
Romanian I-DatasetName
- I-DatasetName
English I-DatasetName
benchmark I-DatasetName
. O

English O
, O
by O
propagation O
through O
BART B-MethodName
, O
thereby O
using O
BART B-MethodName
as O
a O
pre O
- O
trained O
target O
- O
side O
language O
model O
. O

Figure O
1 O
: O
A O
schematic O
comparison O
of O
BART B-MethodName
with O
BERT B-MethodName
( O
Devlin O
et O
al. O
, O
2019 O
) O
and O
GPT B-MethodName
( O
Radford O
et O
al. O
, O
2018 O
) O
. O

For O
fine O
- O
tuning O
, O
an O
uncorrupted O
document O
is O
input O
to O
both O
the O
encoder O
and O
decoder O
, O
and O
we O
use O
representations O
from O
the O
final O
hidden O
state O
of O
the O
decoder O
. O

The O
corrupted O
document O
( O
left O
) O
is O
encoded O
with O
a O
bidirectional O
model O
, O
and O
then O
the O
likelihood O
of O
the O
original O
document O
( O
right O
) O
is O
calculated O
with O
an O
autoregressive O
decoder O
. O

Here O
, O
a O
document O
has O
been O
corrupted O
by O
replacing O
spans O
of O
text O
with O
mask O
symbols O
. O

Inputs O
to O
the O
encoder O
need O
not O
be O
aligned O
with O
decoder O
outputs O
, O
allowing O
arbitary O
noise O
transformations O
. O

E O
< O
s O
> O
A O
B O
C O
D O
( O
c O
) O
BART B-MethodName
: O

A O
B O
C O
D O
E O
Bidirectional O
Encoder O
Autoregressive O
Decoder O
A O
_ O
B O
_ O

However O
words O
can O
only O
condition O
on O
leftward O
context O
, O
so O
it O
can O
not O
learn O
bidirectional O
interactions O
. O

( O
b O
) O
GPT B-MethodName
: O
Tokens O
are O
predicted O
auto O
- O
regressively O
, O
meaning O
GPT B-MethodName
can O
be O
used O
for O
generation O
. O

Missing O
tokens O
are O
predicted O
independently O
, O
so O
BERT B-MethodName
can O
not O
easily O
be O
used O
for O
generation O
. O

Random O
tokens O
are O
replaced O
with O
masks O
, O
and O
the O
document O
is O
encoded O
bidirectionally O
. O

E O
< O
s O
> O
A O
B O
C O
D O
( O
a O
) O
BERT B-MethodName
: O

C O
_ O

B O
A O
B O
C O
D O
E O
D O
Bidirectional O
Encoder O
Autoregressive O
Decoder O
A O
_ O

These O
layers O
are O
trained O
to O
essentially O
translate O
the O
foreign O
language O
to O
noised O

We O
present O
a O
new O
scheme O
for O
machine O
translation O
where O
a O
BART B-MethodName
model O
is O
stacked O
above O
a O
few O
additional O
transformer O
layers O
. O

BART B-MethodName
also O
opens O
up O
new O
ways O
of O
thinking O
about O
fine O
tuning O
. O

For O
example O
, O
it O
improves O
performance O
by O
6 B-MetricValue
ROUGE B-MetricName
over O
previous O
work O
on O
XSum B-DatasetName
( O
Narayan O
et O
al. O
, O
2018 O
) O
. O

It O
matches O
the O
performance O
of O
RoBERTa B-MethodName
( O
Liu O
et O
al. O
, O
2019 O
) O
with O
comparable O
training O
resources O
on O
GLUE B-DatasetName
( O
Wang O
et O
al. O
, O
2018 O
) O
and O
SQuAD B-DatasetName
( O
Rajpurkar O
et O
al. O
, O
2016 O
) O
, O
and O
achieves O
new O
state O
- O
of O
- O
the O
- O
art O
results O
on O
a O
range O
of O
abstractive B-TaskName
dialogue I-TaskName
, O
question B-TaskName
answering I-TaskName
, O
and O
summarization B-TaskName
tasks O
. O

BART B-MethodName
is O
particularly O
effective O
when O
fine O
tuned O
for O
text B-TaskName
generation I-TaskName
but O
also O
works O
well O
for O
comprehension B-TaskName
tasks O
. O

This O
approach O
generalizes O
the O
original O
word O
masking O
and O
next O
sentence O
prediction O
objectives O
in O
BERT B-MethodName
by O
forcing O
the O
model O
to O
reason O
more O
about O
overall O
sentence O
length O
and O
make O
longer O
range O
transformations O
to O
the O
input O
. O

We O
evaluate O
a O
number O
of O
noising O
approaches O
, O
finding O
the O
best O
performance O
by O
both O
randomly O
shuffling O
the O
order O
of O
the O
original O
sentences O
and O
using O
a O
novel O
in O
- O
filling O
scheme O
, O
where O
arbitrary O
length O
spans O
of O
text O
( O
including O
zero O
length O
) O
are O
replaced O
with O
a O
single O
mask O
token O
. O

A O
key O
advantage O
of O
this O
setup O
is O
the O
noising O
flexibility O
; O
arbitrary O
transformations O
can O
be O
applied O
to O
the O
original O
text O
, O
including O
changing O
its O
length O
. O

BART B-MethodName
uses O
a O
standard O
Tranformer O
- O
based O
neural O
machine O
translation O
architecture O
which O
, O
despite O
its O
simplicity O
, O
can O
be O
seen O
as O
generalizing O
BERT B-MethodName
( O
due O
to O
the O
bidirectional O
encoder O
) O
, O
GPT B-MethodName
( O
with O
the O
left O
- O
to O
- O
right O
decoder O
) O
, O
and O
many O
other O
more O
recent O
pretraining O
schemes O
( O
see O
Figure O
1 O
) O
. O

Pretraining O
has O
two O
stages O
( O
1 O
) O
text O
is O
corrupted O
with O
an O
arbitrary O
noising O
function O
, O
and O
( O
2 O
) O
a O
sequence O
- O
to O
- O
sequence O
model O
is O
learned O
to O
reconstruct O
the O
original O
text O
. O

BART B-MethodName
is O
a O
denoising O
autoencoder O
built O
with O
a O
sequence O
- O
to O
- O
sequence O
model O
that O
is O
applicable O
to O
a O
very O
wide O
range O
of O
end O
tasks O
. O

In O
this O
paper O
, O
we O
present O
BART B-MethodName
, O
which O
pre O
- O
trains O
a O
model O
combining O
Bidirectional O
and O
Auto O
- O
Regressive O
Transformers O
. O

However O
, O
these O
methods O
typically O
focus O
on O
particular O
types O
of O
end O
tasks O
( O
e.g. O
span B-TaskName
prediction I-TaskName
, O
generation B-TaskName
, O
etc O
. O
) O
, O
limiting O
their O
applicability O
. O

Recent O
work O
has O
shown O
gains O
by O
improving O
the O
distribution O
of O
masked O
tokens O
( O
Joshi O
et O
al. O
, O
2019 O
) O
, O
the O
order O
in O
which O
masked O
tokens O
are O
predicted O
( O
Yang O
et O
al. O
, O
2019 O
) O
, O
and O
the O
available O
context O
for O
replacing O
masked O
tokens O
( O
Dong O
et O
al. O
, O
2019 O
) O
. O

The O
most O
successful O
approaches O
have O
been O
variants O
of O
masked O
language O
models O
, O
which O
are O
denoising O
autoencoders O
that O
are O
trained O
to O
reconstruct O
text O
where O
a O
random O
subset O
of O
the O
words O
has O
been O
masked O
out O
. O

We O
present O
BART B-MethodName
, O
a O
denoising O
autoencoder O
for O
pretraining O
sequence O
- O
to O
- O
sequence O
models O
. O

1 O
Introduction O
Self O
- O
supervised O
methods O
have O
achieved O
remarkable O
success O
in O
a O
wide O
range O
of O
NLP O
tasks O
( O
Mikolov O
et O
al. O
, O
2013 O
; O
Peters O
et O
al. O
, O
2018 O
; O
Devlin O
et O
al. O
, O
2019 O
; O
Joshi O
et O
al. O
, O
2019 O
; O
Yang O
et O
al. O
, O
2019 O
; O
Liu O
et O
al. O
, O
2019 O
) O
. O

We O
also O
report O
ablation O
experiments O
that O
replicate O
other O
pretraining O
schemes O
within O
the O
BART B-MethodName
framework O
, O
to O
better O
measure O
which O
factors O
most O
influence O
end O
- O
task O
performance O
. O

BART B-MethodName
also O
provides O
a O
1.1 B-MetricValue
BLEU B-MetricName
increase O
over O
a O
back O
- O
translation O
system O
for O
machine B-TaskName
translation I-TaskName
, O
with O
only O
target O
language O
pretraining O
. O

It O
matches O
the O
performance O
of O
RoBERTa B-MethodName
with O
comparable O
training O
resources O
on O
GLUE B-DatasetName
and O
SQuAD B-DatasetName
, O
achieves O
new O
stateof O
- O
the O
- O
art O
results O
on O
a O
range O
of O
abstractive B-TaskName
dialogue I-TaskName
, O
question B-TaskName
answering I-TaskName
, O
and O
summarization B-TaskName
tasks O
, O
with O
gains O
of O
up O
to O
6 B-MetricValue
ROUGE B-MetricName
. O

BART B-MethodName
is O
particularly O
effective O
when O
fine O
tuned O
for O
text B-TaskName
generation I-TaskName
but O
also O
works O
well O
for O
comprehension B-TaskName
tasks I-TaskName
. O

We O
evaluate O
a O
number O
of O
noising O
approaches O
, O
finding O
the O
best O
performance O
by O
both O
randomly O
shuffling O
the O
order O
of O
the O
original O
sentences O
and O
using O
a O
novel O
in O
- O
filling O
scheme O
, O
where O
spans O
of O
text O
are O
replaced O
with O
a O
single O
mask O
token O
. O

It O
uses O
a O
standard O
Tranformer O
- O
based O
neural O
machine O
translation O
architecture O
which O
, O
despite O
its O
simplicity O
, O
can O
be O
seen O
as O
generalizing O
BERT B-MethodName
( O
due O
to O
the O
bidirectional O
encoder O
) O
, O
GPT B-MethodName
( O
with O
the O
left O
- O
to O
- O
right O
decoder O
) O
, O
and O
many O
other O
more O
recent O
pretraining O
schemes O
. O

BART B-MethodName
is O
trained O
by O
( O
1 O
) O
corrupting O
text O
with O
an O
arbitrary O
noising O
function O
, O
and O
( O
2 O
) O
learning O
a O
model O
to O
reconstruct O
the O
original O
text O
. O

[ O
cs O
. O
CL O
] O
29 O
Oct O
2019 O

Denoising O
Sequence O
- O
to O
- O
Sequence O
Pre O
- O
training O
for O
Natural B-TaskName
Language I-TaskName
Generation I-TaskName
, O
Translation B-TaskName
, O
and O
Comprehension B-TaskName
Mike O
Lewis O
* O
, O
Yinhan O
Liu O
* O
, O
Naman O
Goyal O
* O
, O
Marjan O
Ghazvininejad O
, O
Abdelrahman O
Mohamed O
, O
Omer O
Levy O
, O
Ves O
Stoyanov O
, O
Luke O
Zettlemoyer O
Facebook O
AI O
{ O
mikelewis,yinhanliu,naman}@fb.com O
Abstract O
arXiv:1910.13461v1 O

BART B-MethodName
: O

-DOCSTART- O
Linjing O
Li O
is O
the O
corresponding O
author O
. O

This O
work O
was O
supported O
in O
part O
by O
the O
National O
Key O
Research O
and O
Development O
Program O
of O
China O
under O
Grant O
2020AAA0103405 O
, O
the O
National O
Natural O
Science O
Foundation O
of O
China O
under O
Grants O
71902179 O
and O
71621002 O
. O

Acknowledgment O

As O
to O
the O
future O
work O
, O
we O
aim O
to O
find O
out O
how O
to O
decide O
the O
keywords O
in O
sentence O
pairs O
that O
determine O
their O
relationship O
. O

Experimental O
results O
validated O
the O
effectiveness O
of O
the O
proposed O
model O
. O

These O
two O
representations O
are O
then O
merged O
by O
a O
feed O
- O
forward O
neural O
network O
to O
predict O
the O
relationship O
label O
. O

For O
a O
sentence O
pair O
, O
the O
proposed O
model O
learns O
a O
knowledgerelation O
representation O
based O
on O
paths O
of O
knowledge O
graph O
and O
a O
semantic O
- O
relation O
representation O
through O
BiLSTM B-MethodName
. O

The O
results O
partially O
validate O
that O
parts O
of O
sentences O
are O
crucial O
in O
NLI B-TaskName
related O
task O
. O

In O
the O
experiment O
, O
parts O
of O
sentences O
are O
removed O
. O

original O
p O
: O
s O
: O
man O
, O
p O
: O
ride O
, O
o O
: O
bicycle O
h O
: O
s O
: O
man O
, O
p O
: O
ride O
, O
o O
: O
bicycle O
unique O
p O
: O
brick O
h O
: O
cement O
Ablation O
Study O
Table O
6 O
shows O
the O
results O
of O
ablation O
study O
on O
SciTail B-DatasetName
dataset O
. O

h O
: O
The O
man O
rides O
bicycle O
up O
the O
cement O
wall O
. O

4.4 O
sentences O
p O
: O
The O
man O
rides O
bicycle O
up O
the O
brick O
wall O
. O

Table O
5 O
: O
Examples O
from O
BNLI B-DatasetName
dataset O
. O

This O
experiment O
also O
shows O
that O
the O
proposed O
model O
can O
capture O
the O
background O
knowledge O
and O
utilize O
it O
to O
improve O
model O
performance O
. O

As O
indicated O
by O
Table O
4 O
, O
our O
model O
achieves O
the O
best O
performance O
among O
all O
the O
models O
. O

For O
other O
datasets O
, O
we O
choose O
keywords O
among O
subjects O
, O
predicates O
, O
and O
objects O
. O

We O
treat O
these O
words O
as O
key O
words O
, O
remove O
the O
knowledge O
composition O
layer O
, O
and O
directly O
set O
the O
composed O
vector O
as O
the O
relation O
vector O
of O
these O
key O
words O
. O

An O
example O
is O
given O
in O
Table O
5 O
. O

extracted O
. O

For O
dataset O
BNLI B-DatasetName
, O
the O
two O
different O
words O
of O
sentence O
pair O
( O
p O
, O
h O
) O
are O
6505 O

In O
order O
to O
test O
the O
performance O
of O
our O
model O
, O
we O
conduct O
a O
new O
experiment O
under O
another O
setting O
, O
named O
“ O
unique O
- O
word O
setting O
” O
. O

Table O
4 O
: O
Performance O
of O
models O
on O
BNLI B-DatasetName
. O

This O
is O
because O
the O
subjects O
, O
predicates O
, O
and O
objects O
are O
almost O
the O
same O
for O
a O
sentence O
pair O
in O
BNLI B-DatasetName
. O

The O
performance O
difference O
between O
BNLI B-DatasetName
and O
SNLI B-DatasetName
as O
test O
set O
is O
denoted O
as O
∆. O
As O
shown O
in O
Table O
4 O
, O
under O
this O
setting O
, O
the O
result O
of O
our O
model O
, O
denoted O
as O
“ O
original O
setting O
” O
, O
is O
similar O
to O
that O
of O
ESIM B-MethodName
. O

In O
practice O
, O
BNLI B-DatasetName
is O
used O
as O
test O
set O
, O
while O
training O
on O
SNLI B-DatasetName
, O
MultiNLI B-DatasetName
, O
and O
SciTail B-DatasetName
. O

For O
a O
sentence O
pair O
in O
BNLI B-DatasetName
, O
only O
one O
word O
of O
hypothesis O
h O
is O
different O
with O
premise O
p. O

Table O
3 O
: O
Performance O
of O
models O
on O
SciTail B-DatasetName
. O

The O
result O
shows O
that O
the O
proposed O
model O
KGNLI B-MethodName
can O
capture O
external O
knowledge O
and O
use O
them O
effectively O
. O

As O
SciTail B-DatasetName
consists O
of O
more O
factual O
sentences O
than O
SNLI B-DatasetName
and O
MultiNLI B-DatasetName
datasets O
( O
Tay O
et O
al. O
, O
2018 O
) O
, O
background O
knowledge O
plays O
a O
more O
important O
role O
on O
the O
inference B-TaskName
. O

Our O
model O
achieves O
the O
state O
- O
of O
- O
the O
- O
art O
result O
with O
large O
margin O
of O
improvement O
against O
ESIM B-MethodName
which O
only O
considers O
semantic O
knowledge O
. O

Table O
2 O
: O
Performance O
of O
models O
on O
MultiNLI B-DatasetName
. O

Model O
LSTM B-TaskName
Att I-TaskName
. O

Table O
1 O
: O
Performance O
of O
models O
on O
SNLI B-DatasetName
. O

This O
explains O
why O
our O
model O
generates O
similar O
results O
with O
baselines O
. O

According O
to O
( O
Glockner O
et O
al. O
, O
2018 O
) O
, O
inference B-TaskName
on O
SNLI B-DatasetName
dataset O
may O
not O
require O
much O
knowledge O
, O
thus O
results O
are O
not O
affected O
significantly O
by O
external O
knowledge O
. O

Our O
model O
gets O
the O
best O
result O
. O

We O
do O
not O
consider O
ensemble O
models O
in O
this O
paper O
. O

4.3 O
Results O
Table O
1 O
shows O
the O
results O
on O
the O
benchmark O
SNLI B-DatasetName
. O

Finally O
, O
we O
stem O
the O
subjects O
, O
predicates O
, O
and O
objects O
to O
match O
entities O
in O
the O
knowledge O
graph O
. O

In O
both O
NP O
and O
PP O
we O
search O
for O
the O
first O
noun O
, O
while O
in O
ADJP O
we O
just O
treat O
the O
first O
adjective O
to O
be O
an O
object O
. O

Objects O
are O
found O
in O
three O
different O
subtrees O
, O
PP O
, O
NP O
, O
and O
ADJP O
, O
which O
are O
siblings O
of O
the O
VP O
subtree O
containing O
the O
predicate O
. O

Predicates O
are O
chosen O
from O
verbs O
labeled O
as O
VB O
, O
VBD O
, O
VBG O
, O
VBN O
, O
VBP O
, O
or O
VBZ O
. O

The O
deepest O
verb O
descendent O
of O
the O
VP O
subtree O
is O
considered O
as O
predicates O
. O

Subjects O
are O
selected O
from O
entities O
labeled O
as O
NN O
, O
NNP O
, O
NNPS O
, O
or O
NNS O
. O

For O
subject O
, O
we O
employ O
breadth O
first O
search O
to O
select O
the O
first O
descendent O
of O
NP O
that O
is O
a O
noun O
. O

Some O
datasets O
provide O
hand O
- O
annotated O
syntax O
trees O
, O
while O
for O
others O
, O
we O
use O
StandfordNLP O
( O
Qi O
et O
al. O
, O
2018 O
) O
to O
generate O
their O
syntax O
trees O
. O

To O
extract O
subjects O
, O
predicates O
, O
and O
objects O
of O
sentences O
, O
we O
employ O
their O
syntax O
trees O
( O
Rusu O
et O
al. O
, O
2007 O
) O
. O

We O
use O
early O
stopping O
according O
to O
the O
per O
- O
epoch O
accuracy O
on O
the O
validation O
set O
. O

The O
optimizer B-HyperparameterName
is O
Adam B-HyperparameterValue
with O
batch B-HyperparameterName
size I-HyperparameterName
32 B-HyperparameterValue
and O
learning B-HyperparameterName
rate I-HyperparameterName
0.0004 B-HyperparameterValue
. O

Dropout B-HyperparameterName
is O
set O
between O
layers O
to O
avoid O
overfitting O
with O
rate O
0.5 B-HyperparameterValue
. O

Dimensions B-HyperparameterName
of I-HyperparameterName
embeddings I-HyperparameterName
are O
all O
set O
to O
be O
300 B-HyperparameterValue
. O

We O
intialize O
word O
embeddings O
by O
GloVe O
( O
Pennington O
et O
al. O
, O
2014 O
) O
and O
entity O
embeddings O
by O
TransE(Bordes O
et O
al. O
, O
2013 O
) O
. O

We O
limit O
the O
path B-HyperparameterName
length I-HyperparameterName
and O
the O
number B-HyperparameterName
of I-HyperparameterName
paths I-HyperparameterName
both O
to O
10 B-HyperparameterValue
. O

4.2 O
Implementation O
Details O
We O
use O
Concept O
Graph O
( O
Cheng O
et O
al. O
, O
2015 O
; O
Wu O
et O
al. O
, O
2012 O
) O
as O
the O
external O
knowledge O
graph O
, O
as O
it O
has O
the O
largest O
coverage O
on O
datasets O
we O
used O
in O
this O
paper O
. O

Though O
much O
simpler O
and O
smaller O
than O
the O
SNLI B-DatasetName
dataset O
, O
the O
performance O
on O
BNLI B-DatasetName
is O
substantially O
worse O
across O
models O
trained O
on O
SNLI B-DatasetName
( O
Glockner O
et O
al. O
, O
2018 O
) O
. O

In O
BNLI B-DatasetName
, O
the O
premises O
are O
taken O
from O
the O
SNLI B-DatasetName
training O
set O
, O
and O
hypotheses O
are O
generated O
by O
replacing O
a O
single O
word O
within O
the O
premise O
by O
a O
different O
word O
. O

• O
BNLI B-DatasetName
is O
a O
dataset O
constructed O
based O
on O
SNLI B-DatasetName
( O
Glockner O
et O
al. O
, O
2018 O
) O
. O

SciTail B-DatasetName
is O
a O
difficult O
benchmark O
for O
NLI B-TaskName
( O
Tay O
et O
al. O
, O
2018 O
) O
. O

It O
contains O
24k O
sentence O
pairs O
and O
only O
classifies O
sentences O
into O
two O
relationships O
: O
entailment O
and O
neutral O
. O

• O
SciTail B-DatasetName
is O
a O
small O
- O
scale O
dataset O
constructed O
from O
multiple O
- O
choice O
science O
exams O
and O
web O
sentences O
( O
Khot O
et O
al. O
, O
2018 O
) O
. O

In O
MultiNLI B-DatasetName
, O
the O
development O
/ O
test O
sets O
whose O
genres O
appear O
in O
training O
set O
are O
referred O
to O
as O
“ O
matched O
” O
dataset O
, O
and O
“ O
mismatched O
” O
otherwise O
. O

• O
MultiNLI B-DatasetName
Multi B-DatasetName
- I-DatasetName
Genre I-DatasetName
Natural I-DatasetName
Language I-DatasetName
Inference I-DatasetName
( O
Williams O
et O
al. O
, O
2018 O
) O
is O
also O
a O
large O
- O
scale O
corpus O
containing O
433k O
sentence O
pairs O
. O

It O
is O
the O
largest O
corpus O
for O
NLI B-TaskName
tasks O
, O
with O
more O
than O
570k O
human O
annotated O
sentence O
pairs O
. O

SNLI B-DatasetName
Stanford B-DatasetName
Natural I-DatasetName
Language I-DatasetName
Inference I-DatasetName
( O
Bowman O
et O
al. O
, O
2015 O
) O
is O
extracted O
from O
Flickr30k O
corpus O
. O

4 O
Experiments O
4.1 O
Datasets O
We O
use O
four O
widely O
adopted O
standard O
benchmarks O
: O
SNLI B-DatasetName
, O
MultiNLI B-DatasetName
, O
SciTail B-DatasetName
, O
and O
BNLI B-DatasetName
. O

y O
= O

The O
perceptron O
classifier O
predict O
a O
label O
y O
to O
be O
entailment O
, O
contradiction O
, O
or O
neutral O
, O
i O
h O

The O
label O
prediction O
layer O
is O
designed O
to O
determine O
the O
overall O
logical O
relationship O
between O
two O
sentences O
. O

3.3 O
Label O
Prediction O

The O
semantic O
relationship O
v O
s O
between O
p O
and O
h O
is O
, O
where O
Gs O
is O
a O
feed O
- O
forward O
neural O
network O
with O
ReLU O
as O
the O
activation O
function O
. O

The O
resulting O
vectors O
av O
and O
bv O
are O
fed O
into O
the O
pooling O
layer O
which O
computes O
both O
average O
and O
max O
pooling O
for O
premise O
and O
hypothesis O
, O
m O
X O
av O
, O
amax O
= O
max O
avi O
, O
( O
21 O
) O
, O
bmax O
= O
max O
bvj O
. O

The O
composed O
vectors O
avi O
, O
bvj O
for O
premise O
and O
hypothesis O
are O
computed O
by O
BiLSTM B-MethodName
, O
avi O
= O
BiLSTM(am O
i O
) O
, O
i O
= O
1 O
, O
. O
. O
. O

3.2.3 O
Semantic O
Composition O
A O
composition O
layer O
is O
employed O
to O
learn O
the O
types O
of O
local O
inference O
relationship O
at O
sentence O
- O
level O
. O

We O
set O
it O
as O
a O
one O
- O
layer O
feed O
- O
forward O
neural O
network O
with O
ReLU O
as O
the O
activation O
function O
. O

Local O
inference O
information O
is O
then O
enhanced O
by O
computing O
difference O
and O
element O
- O
wise O
product O
for O
( O
as O
, O
ac O
) O
and O
( O
bs O
, O
bc O
) O
, O
am O
= O
G([as O
; O

ik O
k=1 O

Pn O
bsj O
, O
exp O
( O
E O
) O

The O
context O
vector O
bc O
that O
encoded O
relevant O
semantics O
in O
premise O
can O
be O
calculated O
in O
the O
same O
way O
, O
aci O
= O
bcj O
= O
n O
X O
j=1 O
m O
X O
i=1 O
exp O
( O
Eij O
) O

For O
premise O
, O
the O
relevant O
semantics O
in O
hypothesis O
is O
encoded O
into O
a O
context O
vector O
ac O
based O
on O
co O
- O
attention O
matrix O
E O
and O
hypothesis O
semantic O
embedding O
b. O

Next O
, O
we O
compute O
the O
local O
relevance O
information O
according O
to O
the O
co O
- O
attention O
E. O

For O
premise O
embedding O
asi O
and O
hypothesis O
embedding O
bsj O
, O
their O
similarity O
is O
Eij O
= O
( O
asi O
) O
T O
bsj O
, O
( O
14 O
) O
all O
those O
Eij O
form O
the O
co O
- O
attention O
matrix O
E O
∈ O
Rm×n O
. O

First O
, O
a O
soft O
alignment O
layer O
is O
employed O
to O
compute O
the O
similarity O
between O
words O
. O

No O
external O
knowledge O
is O
concerned O
in O
this O
part O
. O

3.2.2 O
Local O
Inference O
The O
computing O
of O
local O
inference O
information O
between O
two O
sentences O
is O
based O
on O
their O
semantic O
embeddings O
as O
and O
bs O
. O


k B-HyperparameterName
is O
the O
embedding B-HyperparameterName
dimension I-HyperparameterName
, O
i O
and O
j O
index O
the O
position O
of O
words O
in O
sentences O
, O
s O
indicates O
that O
the O
embeddings O
are O
learned O
based O
on O
semantic O
. O

Denote O
as O
and O
bs O
as O
embedding O
vectors O
of O
both O
premise O
and O
hypothesis O
, O
as O
= O
[ O
as1 O
, O
as2 O
, O
. O
. O
. O

Semantic O
Embedding O
We O
first O
initialize O
words O
into O
embeddings O
based O
on O
pre O
- O
trained O
word O
vectors O
GloVe O
( O
Pennington O
et O
al. O
, O
2014 O
) O
, O
then O
encode O
premise O
and O
hypothesis O
using O
bidirectional B-MethodName
LSTM I-MethodName
( O
BiLSTM B-MethodName
) O
. O

, O
hn O
] O
, O
where O
pi O
and O
hj O
are O
words O
, O
1 O
≤ O
i O
≤ O
m O
, O
1 O
≤ O
j O
≤ O
n O
, O
m O
and O
n O
are O
the O
lengths O
of O
premise O
p O
and O
hypothesis O
h. O
3.2.1 O

In O
the O
following O
, O
we O
denote O
sentences O
p O
and O
h O
as O
p O
= O

3.2 O
Semantic O
- O
Relation O
Representation O
To O
capture O
the O
semantic O
relationship O
between O
premise O
p O
and O
hypothesis O
h O
, O
we O
follow O
the O
widely O
adopted O
framework O
to O
get O
the O
relationship O
representations O
( O
Chen O
et O
al. O
, O
2017 O
) O
. O

Apart O
from O
relation O
representations O
ωSp O
, O
ωPp O
and O
ωO O
, O
we O
also O
consider O
the O
correlation O
among O
them O
by O
using O
element O
- O
wise O
product O
, O
p O
; O
ωSp O
v O
k O
= O
Gk O
( O
[ O
ωSp O
; O
ωPp O
; O
ωO O
p O
ωPp O
; O
ωO O
ωPp O
] O
) O
( O
11 O
) O
where O
v O
k O
is O
the O
composed O
representation O
, O
and O
Gk O
is O
a O
feed O
- O
forward O
neural O
network O
with O
ReLU O
as O
the O
activation O
function O
. O

We O
where O
ωSp O
, O
ωPp O
and O
ωO O
set O
entity O
embeddings O
as O
the O
updated O
embeddings O
and O
relation O
embeddings O
according O
to O
TransE. O
3.1.4 O
Knowledge O
Composition O
We O
use O
a O
composition O
layer O
to O
merge O
the O
relationship O
of O
subject O
pair O
, O
predicate O
pair O
, O
and O
object O
pair O
. O

Relations O
are O
represented O
by O
the O
average O
of O
the O
representations O
of O
all O
paths O
N O
ωSp O
= O

We O
encode O
the O
path O
sequence O
with O
BiLSTM B-MethodName
. O

The O
paths O
liP O
and O
liO O
of O
predicate O
and O
object O
pair O
are O
defined O
in O
the O
same O
way O
. O

[ O
pS O
, O
r1 O
, O
e1 O
, O
r2 O
, O
e2 O
, O
. O
. O
. O

Denote O
the O
i O
- O
th O
path O
between O
subject O
pair O
( O
pS O
, O
hS O
) O
as O
liS O
= O

We O
get O
the O
relationship O
representation O
based O
on O
the O
representation O
of O
paths O
in O
the O
corresponding O
subgraph O
. O

where O
ake O
and O
akr O
are O
the O
embeddings O
of O
entity O
e O
and O
relation O
r O
initialized O
by O
TransE. O
For O
hypothesis O
, O
the O
attention O
score O
is O
calculated O
in O
the O
same O
way O
, O
exp O
( O
W O
[ O
bke O
, O
bkr O
] O
) O
. O

ϕp O
( O
e O
, O
r O
) O
= O

For O
premise O
, O
ϕp O
( O
e O
, O
r O
) O
is O
computed O
as O
exp O
( O
W O
[ O
ake O
, O
akr O
] O
) O
, O
k O
k O
( O
e O
, O
r)∈Sp O
exp O
( O
W O
[ O
ae O
, O
ar O
] O
) O

[ O
bke O
; O
bkr O
] O
) O
, O
( O
4 O
) O
( O
e O
, O
r)∈Shj O
where O
aki O
and O
bkj O
are O
the O
propagated O
entity O
embedding O
, O
γ O
is O
a O
tradeoff O
parameter O
, O
and O
Spi O
represents O
the O
set O
of O
all O
( O
e O
, O
r O
) O
pairs O
of O
pi O
, O
where O
e O
is O
a O
neighobor O
of O
pi O
, O
and O
r O
is O
the O
relation O
between O
pi O
and O
e. O
σ O
( O
· O
) O
is O
the O
activation O
function O
, O
W O
is O
a O
transformation O
matrix O
, O
[ O
· O
; O
· O
] O
denotes O
the O
concatenation O
operator O
, O
and O
ϕp O
( O
e O
, O
r O
) O
is O
an O
attention O
score O
over O
( O
e O
, O
r O
) O
, O
which O
is O
calculated O
based O
on O
the O
embedding O
of O
the O
entity O
and O
its O
neighbors O
. O

[ O
ake O
; O
akr O
] O
) O
, O
( O
3 O
) O
( O
e O
, O
r)∈Spi O
bkj O
← O
γbkj O
+ O
( O
1 O
− O
γ O
) O
X O
ϕh O
( O
e O
, O
r)σ(W O

For O
an O
entity O
, O
we O
retrieve O
all O
the O
neighboring O
relations O
of O
it O
in O
the O
sub O
- O
graph O
, O
and O
encode O
the O
neighboring O
knowledge O
into O
its O
embedding O
through O
the O
following O
propagation O
rule O
, O
X O
aki O
← O
γaki O
+ O
( O
1 O
− O
γ O
) O
ϕp O
( O
e O
, O
r)σ(W O

( O
2 O
) O
We O
then O
update O
these O
embeddings O
based O
on O
the O
sub O
- O
graphs O
using O
graph O
neural O
network O
. O

We O
initialize O
entity O
embeddings O
based O
on O
pre O
- O
trained O
vectors O
that O
are O
generated O
by O
TransE O
( O
Bordes O
et O
al. O
, O
2013 O
) O
, O
aki O
= O
TransE(pi O
) O
, O
i O
∈ O
{ O
S O
, O
P O
, O
O O
} O
, O
( O
1 O
) O
bkj O
= O
TransE(hj O
) O
, O
j O
∈ O
{ O
S O
, O
P O
, O
O O
} O
. O

, O
k O
indicates O
that O
the O
embeddings O
are O
learned O
based O
on O
knowledge O
graphs O
. O

 O
k O
ink O
sub O
- O
graphs O
 O
k O
k O
kfor O
entities O
k O
embedded O
vectors O
of O
p O
and O
h O
as O
aS O
, O
aP O
, O
aO O
and O
bS O
, O
bP O
, O
bO O
, O
where O
S O
, O
P O
and O
O O
are O
indices O
of O
subject O
, O
predicate O
, O
and O
object O
as O
above O

Denote O
the O
knowledge O

First O
, O
the O
knowledge O
- O
based O
embeddings O
are O
learned O
. O

3.1.2 O
Knowledge O
Embedding O
With O
the O
help O
of O
sub O
- O
graphs O
, O
we O
can O
learn O
the O
knowledge O
- O
based O
relationship O
of O
sentence O
pairs O
. O

In O
this O
paper O
, O
we O
extract O
subjects O
, O
predicates O
, O
and O
objects O
of O
sentences O
based O
on O
their O
syntax O
tree O
, O
consider O
paths O
with O
length O
up O
to O
L O
and O
limit O
the O
total O
number O
of O
paths O
of O
each O
sub O
- O
graph O
as O
N O
. O

The O
lengths O
of O
the O
first O
two O
paths O
are O
5 O
, O
while O
it O
is O
7 O
of O
the O
last O
one O
. O

sub O
- O
graph O
of O
the O
object O
pair O
( O
piano O
, O
music O
) O
, O
“ O
piano O
- O
has O
subevent O
- O
playing O
piano O
- O
causes O
- O
music O
” O
, O
“ O
pianois O
a O
- O
instrument O
- O
used O
for O
- O
music O
” O
, O
and O
“ O
piano O
- O
related O
to O
- O
orchestra O
- O
related O
to O
- O
classical O
- O
is O
a O
- O
music O
” O
. O

In O
this O
way O
, O
we O
encode O
the O
knowledge O
into O
pooled O
embeddings O
. O

For O
entities O
“ O
piano O
” O
and O
“ O
music O
” O
, O
we O
first O
find O
the O
paths O
between O
them O
in O
the O
knowledge O
graph O
, O
and O
update O
their O
embeddings O
using O
graph O
neural O
networks O
. O

Fig O
. O
3 O
shows O
three O
paths O
in O
the O
6500 O

The O
sub O
- O
graphs O
for O
predicate O
pair O
( O
pP O
, O
hP O
) O
and O
object O
pair O
( O
pO O
, O
hO O
) O
are O
constructed O
in O
the O
same O
way O
. O

For O
each O
sentence O
pair O
, O
the O
sub O
- O
graph O
of O
background O
relationship O
for O
subject O
pair O
( O
pS O
, O
hS O
) O
is O
extracted O
by O
finding O
paths O
between O
entities O
that O
denote O
ps O
and O
hs O
in O
the O
predetermined O
knowledge O
graph O
KG O
with O
the O
help O
of O
random O
walking O
. O

The O
architecture O
of O
this O
module O
is O
shown O
in O
Fig O
. O
3 O
. O
3.1.1 O
Sub O
- O
graph O
of O
background O
relationship O
In O
the O
following O
section O
, O
we O
denote O
the O
subject O
pair O
, O
predicate O
pair O
, O
and O
object O
pair O
of O
p O
and O
h O
as O
( O
pS O
, O
hS O
) O
, O
( O
pP O
, O
hP O
) O
, O
and O
( O
pO O
, O
hO O
) O
, O
respectively O
. O

In O
this O
paper O
, O
we O
assume O
that O
the O
relationship O
between O
sentences O
is O
determined O
by O
the O
relationship O
of O
their O
subjects O
, O
predicates O
, O
and O
objects O
. O

3.1 O
Knowledge O
- O
Relation O
Representation O
To O
build O
the O
relationship O
between O
p O
and O
h O
based O
on O
background O
knowledge O
, O
we O
propose O
a O
novel O
knowledge O
- O
relation O
representation O
module O
. O

It O
contains O
three O
major O
components O
: O
knowledge O
- O
relation O
representation O
module O
, O
semantic O
- O
relation O
representation O
module O
, O
and O
label O
prediction O
module O
. O

Figure O
2 O
: O
The O
overall O
architecture O
of O
the O
model O
. O

Finally O
, O
a O
multilayer O
perceptron O
merges O
both O
knowledge O
and O
semantic O
relationships O
and O
predicts O
the O
label O
. O

The O
novel O
knowledgerelation O
representation O
module O
builds O
the O
relationship O
between O
p O
and O
h O
based O
on O
background O
knowledge O
, O
while O
the O
semantic O
- O
relation O
representation O
module O
captures O
sentence O
semantic O
relationship O
. O

The O
proposed O
model O
consists O
of O
three O
major O
components O
, O
as O
showed O
in O
Fig O
. O
2 O
. O

The O
set O
of O
labels O
includes O
entailment O
( O
h O
can O
be O
logically O
deduced O
from O
p O
) O
, O
neutral O
( O
p O
and O
h O
do O
not O
have O
any O
logical O
relationship O
) O
, O
and O
contradiction O
( O
p O
and O
h O
can O
not O
be O
true O
simultaneously O
) O
. O

Given O
a O
pair O
of O
sentences O
, O
premise O
p O
and O
hypothesis O
h O
, O
the O
goal O
of O
NLI B-TaskName
is O
to O
predict O
a O
label O
y O
that O
indicates O
the O
logical O
relationship O
between O
sentences O
p O
and O
h. O

3 O
Methodology O

In O
this O
paper O
, O
knowledge O
graph O
is O
employed O
to O
provide O
external O
background O
knowledge O
. O

There O
are O
many O
open O
source O
knowledge O
graphs O
which O
can O
be O
easily O
employed O
in O
a O
variety O
of O
applications O
, O
such O
as O
WordNet O
( O
Miller O
, O
1995 O
) O
, O
Freebase O
( O
Bollacker O
et O
al. O
, O
2008 O
) O
, O
and O
Concept O
Graph O
( O
Cheng O
et O
al. O
, O
2015 O
; O
Wu O
et O
al. O
, O
2012 O
) O
. O

Knowledge O
is O
formatted O
as O
triples O
in O
knowledge O
graph O
, O
a O
triple O
( O
h O
, O
r O
, O
t O
) O
indicates O
that O
head O
entity O
h O
and O
tail O
entity O
t O
have O
relation O
r. O

2.3 O
Knowledge O
Graph O
A O
knowledge O
graph O
is O
a O
large O
knowledge O
base O
storing O
relational O
knowledge O
in O
a O
graph O
structure O
. O

However O
, O
it O
can O
only O
deal O
with O
a O
fixed O
number O
of O
types O
of O
knowledge O
, O
and O
pre O
- O
assigned O
scores O
for O
relationships O
are O
needed O
before O
training O
, O
which O
limits O
its O
applications O
in O
practice O
. O

This O
model O
incooperates O
basic O
knowledge O
about O
synonymy O
, O
antonymy O
, O
hypernymy O
, O
hyponymy O
, O
and O
co O
- O
hyponyms O
to O
help O
model O
soft O
- O
alignments O
between O
sentence O
pairs O
. O

To O
the O
best O
of O
our O
knowledge O
, O
the O
only O
neural O
model O
adopting O
external O
knowledge O
is O
KIM B-MethodName
( O
Chen O
et O
al. O
, O
2018 O
) O
. O

Although O
these O
models O
have O
achieved O
state O
- O
of O
- O
the O
- O
art O
results O
in O
NLI O
related O
tasks O
, O
they O
only O
rely O
on O
semantic O
relationship O
learned O
from O
training O
corpus O
, O
in O
other O
words O
, O
no O
external O
knowledge O
is O
utilized O
explicitly O
to O
facilitate O
inference O
. O

For O
instances O
, O
ESIM B-MethodName
( O
Chen O
et O
al. O
, O
2017 O
) O
considers O
recursive O
architectures O
in O
both O
local O
inference O
modeling O
and O
inference O
composition O
, O
CAFE B-MethodName
( O
Tay O
et O
al. O
, O
2018 O
) O
architecture O
propagates O
compressed O
alignment O
features O
to O
upper O
layers O
to O
enhance O
representation O
learning O
, O
DMAN B-MethodName
( O
Pan O
et O
al. O
, O
2018 O
) O
adopts O
reinforcement O
learning O
with O
discourse O
markers O
to O
help O
improve O
the O
performance O
. O

Various O
architectures O
have O
been O
proposed O
to O
capture O
the O
interaction O
and O
soft O
alignment O
between O
sentences O
. O

( O
Williams O
et O
al. O
, O
2018 O
) O
, O
has O
stimulated O
research O
and O
development O
of O
new O
models O
based O
on O
deep O
neural O
network O
. O

The O
emergence O
of O
large O
- O
scale O
datasets O
, O
such O
as O
SNLI B-DatasetName
( O
Bowman O
et O
al. O
, O
2015 O
) O
and O
MultiNLI B-DatasetName

2 O
Related O
Work O
2.1 O
Natural B-TaskName
Language I-TaskName
Inference I-TaskName
Traditional O
NLI B-TaskName
models O
are O
trained O
on O
small O
- O
scale O
datasets O
, O
like O
natural O
logic O
- O
based O
and O
co O
- O
occurrence O
statistics O
- O
based O
models O
, O
the O
former O
identifies O
inferences O
by O
lexical O
and O
syntactic O
features O
( O
MacCartney O
and O
Manning O
, O
2008 O
) O
, O
while O
the O
latter O
considers O
the O
statistical O
features O
( O
Glickman O
and O
Dagan O
, O
2005 O
) O
. O

We O
further O
conduct O
ablation O
tests O
to O
validate O
the O
effectiveness O
and O
necessity O
of O
each O
component O
in O
the O
proposed O
model O
. O

On O
dataset O
SciTail B-DatasetName
and O
BNLI B-DatasetName
, O
where O
knowledge O
is O
crucial O
for O
inference O
( O
Glockner O
et O
al. O
, O
2018 O
) O
, O
our O
model O
achieves O
large O
improvements O
against O
baselines O
. O

Our O
model O
gets O
competitive O
results O
on O
all O
the O
four O
datasets O
. O

In O
order O
to O
evaluate O
our O
model O
, O
we O
conduct O
experiments O
on O
four O
datasets O
: O
SNLI B-DatasetName
, O
MultiNLI B-DatasetName
, O
SciTail B-DatasetName
, O
and O
BNLI B-DatasetName
. O

Finally O
, O
KGNLI B-MethodName
combines O
these O
two O
representations O
and O
feed O
it O
into O
a O
multilayer O
perceptron O
to O
determine O
the O
label O
of O
the O
relationship O
. O

Besides O
it O
, O
KGNLI B-MethodName
also O
learns O
a O
semantic O
- O
relation O
representation O
between O
the O
given O
sentences O
by O
Bi B-MethodName
- I-MethodName
directional I-MethodName
Long I-MethodName
Short I-MethodName
- I-MethodName
Term I-MethodName
Memory I-MethodName
( O
BiLSTM B-MethodName
) O
network O
. O

the O
given O
sentence O
pair O
, O
then O
learns O
a O
knowledge O
- O
relation O
representation O
based O
on O
a O
predetermined O
knowledge O
graph O
which O
contains O
these O
entities O
as O
nodes O
. O

For O
the O
sentence O
pair O
in O
the O
figure O
, O
their O
relationship O
largely O
depends O
on O
the O
relationship O
of O
entities O
“ O
piano O
” O
and O
“ O
music O
” O
, O
which O
can O
be O
learned O
from O
the O
paths O
in O
the O
knowledge O
graph O
. O

Knowledge O
graphs O
can O
provide O
background O
knowledge O
for O
the O
NLI B-TaskName
problem O
. O

To O
be O
more O
specific O
, O
KGNLI B-MethodName
first O
extracts O
entities O
such O
as O
subjects O
, O
predicates O
, O
and O
objects O
from O
This O
work O
is O
licensed O
under O
a O
Creative O
Commons O
Attribution O
4.0 O
International O
License O
. O

KGNLI B-MethodName
enhances O
the O
performance O
of O
NLI B-TaskName
by O
introducing O
background O
knowledge O
stored O
in O
knowledge O
graphs O
. O

In O
this O
paper O
, O
we O
propose O
a O
Knowledge B-MethodName
Graph I-MethodName
enhanced I-MethodName
Natural I-MethodName
Language I-MethodName
Inference I-MethodName
( O
KGNLI B-MethodName
) O
model O
. O

How O
to O
flexibly O
incorporate O
a O
variety O
of O
different O
background O
knowledge O
in O
NLI B-TaskName
is O
still O
a O
challenging O
task O
. O

Previous O
work O
( O
Chen O
et O
al. O
, O
2018 O
) O
shows O
that O
NLI B-TaskName
models O
can O
benefit O
from O
leveraging O
external O
knowledge O
. O

The O
right O
part O
of O
Fig O
. O
1 O
is O
a O
part O
of O
a O
large O
knowledge O
graph O
, O
in O
which O
paths O
between O
“ O
piano O
” O
and O
“ O
music O
” O
represent O
background O
knowledge O
that O
can O
be O
utilized O
to O
determine O
the O
relationship O
of O
the O
sentence O
pair O
. O

However O
, O
the O
latter O
is O
not O
explicitly O
expressed O
in O
the O
sentences O
per O
se O
. O

For O
the O
sentences O
located O
in O
the O
left O
part O
of O
Fig O
. O
1 O
, O
the O
logical O
relationship O
between O
the O
Premise O
and O
the O
Hypothsis O
largely O
depends O
on O
the O
relationship O
between O
“ O
piano O
” O
and O
“ O
music O
” O
. O

Background O
knowledge O
can O
be O
utilized O
to O
facilitate O
inference O
as O
shown O
in O
Fig O
. O
1 O
. O

However O
, O
only O
semantic O
knowledge O
between O
premises O
and O
hypotheses O
are O
utilized O
in O
most O
of O
these O
models O
. O

The O
resulting O
alignments O
and O
semantic O
representations O
of O
sentences O
are O
aggregated O
and O
fed O
into O
a O
multilayer O
feed O
- O
forward O
neural O
network O
to O
judge O
logical O
relationships O
. O

They O
learn O
alignments O
according O
to O
the O
attention O
between O
premises O
and O
hypotheses O
. O

Most O
of O
the O
existing O
NLI B-TaskName
models O
are O
based O
on O
cross O
sentence O
attention O
( O
Chen O
et O
al. O
, O
2017 O
) O
. O

These O
datasets O
enable O
various O
deep O
learning O
models O
to O
achieve O
state O
- O
of O
- O
the O
- O
art O
performances O
( O
Chen O
et O
al. O
, O
2017 O
; O
Chen O
et O
al. O
, O
2018 O
; O
Pan O
et O
al. O
, O
2018 O
; O
Ghaeini O
et O
al. O
, O
2018 O
) O
. O

Recent O
years O
have O
witnessed O
a O
large O
improvement O
in O
NLI B-TaskName
models O
because O
of O
the O
release O
of O
several O
large O
- O
scale O
corpora O
, O
such O
as O
SNLI B-DatasetName
( O
Bowman O
et O
al. O
, O
2015 O
) O
and O
MultiNLI B-DatasetName

NLI B-TaskName
requires O
reasoning O
and O
inference O
abilities O
which O
are O
crucial O
for O
artificial O
intelligence O
system O
. O

NLI B-TaskName
aims O
to O
determine O
whether O
the O
logical O
relationship O
between O
a O
premise O
p O
and O
a O
hypothesis O
h O
is O
entailment O
, O
neutral O
, O
or O
contradiction O
. O

1 O
Introduction O
Natural B-TaskName
Language I-TaskName
Inference I-TaskName
( O
NLI B-TaskName
) O
is O
a O
fundamental O
yet O
challenging O
task O
for O
natural B-TaskName
language I-TaskName
understanding I-TaskName
. O

Experiments O
on O
four O
benchmarks O
, O
SNLI B-DatasetName
, O
MultiNLI B-DatasetName
, O
SciTail B-DatasetName
, O
and O
BNLI B-DatasetName
, O
validate O
the O
effectiveness O
of O
our O
model O
. O

Different O
from O
previous O
methods O
, O
various O
kinds O
of O
background O
knowledge O
can O
be O
flexibly O
combined O
in O
the O
proposed O
KGNLI B-MethodName
model O
. O

KGNLI B-MethodName
model O
consists O
of O
three O
components O
: O
a O
semantic O
- O
relation O
representation O
module O
, O
a O
knowledge O
- O
relation O
representation O
module O
, O
and O
a O
label O
prediction O
module O
. O

In O
this O
paper O
, O
we O
propose O
a O
novel O
Knowledge B-MethodName
Graph I-MethodName
- I-MethodName
enhanced I-MethodName
NLI I-MethodName
( O
KGNLI B-MethodName
) O
model O
to O
leverage O
the O
usage O
of O
background O
knowledge O
stored O
in O
knowledge O
graphs O
in O
the O
field O
of O
NLI B-TaskName
. O

The O
adoption O
of O
background O
knowledge O
is O
rarely O
seen O
or O
limited O
to O
a O
few O
specific O
types O
. O

Most O
of O
the O
existing O
approaches O
make O
such O
inference O
based O
on O
semantic O
knowledge O
obtained O
through O
training O
corpus O
. O

It O
aims O
to O
identify O
the O
logical O
relationship O
between O
two O
sentences O
. O

Knowledge O
Graphs O

Knowledge O
- O
Enhanced O
Natural B-TaskName
Language I-TaskName
Inference I-TaskName
Based O
on O

-DOCSTART- O
γ B-HyperparameterName
indicates O
consistency O
level O
between O
the O
outputs O
from O
two O
teacher O
models O
, O
e.g. O
for O
two O
input O
tokens O
, O
if O
the O
output O
from O
entity O
similarity O
teacher O
is O
high O
, O
and O
the O
similarity O
level O
computed O
from O
the O
outputs O
of O
the O
entity O
recognizer O
teacher O
is O
low O
, O
then O
their O
consistency O
level O
is O
low O
. O

And O
β B-HyperparameterName
is O
set O
such O
that O
it O
is O
high O
when O
the O
output O
of O
the O
entity O
similarity O
teacher O
is O
close O
to O
0 O
or O
1 O
, O
and O
it O
is O
low O
when O
the O
output O
is O
close O
to O
0.5 O
. O

The O
weights O
are O
set O
as O
follows O
: O
α1 B-HyperparameterName
( O
α2 B-HyperparameterName
) O
is O
an O
increasing O
function O
concerning O
the O
output O
of O
the O
entity O
recognizer O
teacher O
as O
shown O
in O
Figure O
4 O
. O

T O
−sim O
( O
xT O
, O
x0 O
T O
, O
i O
, O
j)∈Dtrain O
4.1 O
+ O
α2 B-HyperparameterName
LER O
( O
x0 O
T O
, O
yS0 O
, O
j O
) O
+ O
βLBCE O
( O
t̂T O
( O
xT O
, O
x0 O
T O
, O
i O
, O
j O
) O
, O
t̂S O
) O
) O
where O
α1 B-HyperparameterName
, O
α2 B-HyperparameterName
, O
β B-HyperparameterName
, O
and O
γ B-HyperparameterName
are O
weights O
in O
loss O
function O
which O
are O
set O
to O
make O
the O
student O
model O
learns O
less O
noisy O
knowledge O
from O
teachers O
. O

− O
t̂T O
( O
xT O
, O
x0 O
T O
, O
i O
, O
j)| O
4 O
Experiment O
In O
this O
section O
, O
we O
evaluate O
our O
multiple B-MethodName
- I-MethodName
task I-MethodName
and I-MethodName
multiple I-MethodName
- I-MethodName
teacher I-MethodName
model O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
and O
compare O
our O
model O
with O
a O
series O
of O
state O
- O
of O
- O
the O
- O
art O
models O
. O

, O
yS O
, O
i O
) O
γ O
= O
1 O
− O
|σ(cos(ŷTi O
, O
ŷT0 O
j O
) O
) O

Summering O
over O
all O
the O
samples O
T O
−sim O
in O
Dtrain O
= O
{ O
( O
xT O
, O
x0 O
T O
, O
i O
, O
j O
) O
} O
, O
the O
total O
student O
model O
training O
loss O
takes O
form O
, O
X O
L O
= O
γ B-HyperparameterName
( O
α1 B-HyperparameterName
LER O
( O
xT O

α B-HyperparameterName
( I-HyperparameterName
· I-HyperparameterName
) I-HyperparameterName
= O
( O
max(ŷTi O
) O
) O
2 O
β B-HyperparameterName
= O
( O
2t̂T O
( O
xT O
, O
x0 O
T O
, O
i O
, O
j O
) O
− O

Dtrain O
, O
the O
student O
model O
transform O
them O
as O
follows O
, O
1.0 O
1.0 O
0.8 O
0.8 O
0.6 O
0.6 O
0.4 O
0.4 O
0.2 O
0.2 O
0.0 O
0.0 O
0.0 O
0.2 O
0.4 O
0.6 O
0.8 O
1.0 O
( O
a O
) O
0.0 O
0.2 O
0.4 O
0.6 O
0.8 O
1.0 O
( O
b O
) O
Figure O
4 O
: O
Weights O
of O
loss O
. O

( O
a O
) O
indicates O
the O
weight O
α B-HyperparameterName
( I-HyperparameterName
· I-HyperparameterName
) I-HyperparameterName
of O
LER O
. O

This O
work O
is O
supported O
partly O
by O
the O
Fundamental O
Research O
Funds O
for O
the O
Central O
Universities O
and O
by O
the O
State O
Key O
Laboratory O
of O
Software O
Development O
Environment O
. O

Siamese O
neural O
networks O
for O
one O
- O
shot O
image O
recognition O
. O

177 O

Our O
experimental O
results O
show O
that O
the O
proposed O
model O
yields O
significant O
improvements O
on O
six O
target O
language O
datasets O
and O
outperforms O
the O
existing O
state O
- O
of O
- O
the O
- O
art O
approaches O
. O

Moreover O
, O
to O
guarantee O
the O
student O
learning O
performance O
, O
we O
also O
propose O
a O
weighting O
strategy O
to O
take O
into O
consideration O
the O
reliability O
of O
the O
teachers O
. O

The O
student O
model O
learns O
two O
source O
language O
patterns O
of O
entity O
recognition O
and O
entity O
similarity O
evaluation O
. O

5 O
Conclusion O
In O
this O
paper O
, O
we O
propose O
an O
unsupervised O
multipletask B-MethodName
and I-MethodName
multiple I-MethodName
- I-MethodName
teacher I-MethodName
model O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
. O

The O
student O
model O
learns O
less O
from O
unreasonable O
results O
, O
and O
it O
can O
make O
more O
accurate O
entity O
recognition O
for O
the O
target O
language O
. O

The O
F1 B-MetricName
- I-MetricName
score I-MetricName
and O
similarity O
score O
of O
teachers O
are O
all O
higher O
in O
the O
higher O
γ B-HyperparameterName
intervals O
, O
as O
shown O
in O
Figure O
6c O
. O

For O
γ B-HyperparameterName
analysis O
, O
we O
consider O
the O
consistency O
of O
recognition O
results O
and O
similarity O
score O
by O
teachers O
. O

The O
encoder O
of O
the O
student O
model O
obtains O
the O
clustering O
information O
of O
the O
target O
language O
with O
the O
help O
of O
β B-HyperparameterName
. O

For O
β B-HyperparameterName
analysis O
, O
we O
observe O
that O
F1 B-MetricName
- I-MetricName
score I-MetricName
are O
increasing O
with O
the O
entity O
similarity O
score O
from O
0.5 O
to O
both O
sides O
0 O
and O
1 O
in O
Figure O
6b O
. O

Therefore O
, O
the O
student O
model O
is O
better O
suited O
to O
the O
target O
language O
with O
learning O
fewer O
low O
- O
confidence O
misrecognitions O
for O
the O
target O
language O
. O

For O
α B-HyperparameterName
analysis O
, O
we O
calculate O
the O
F1 B-MetricName
- I-MetricName
score I-MetricName
in O
different O
probability O
intervals O
of O
entity O
recognizer O
teacher O
, O
we O
find O
that O
the O
recognizer O
teacher O
tends O
to O
predict O
more O
correct O
in O
higher O
probability O
interval O
, O
as O
illustrated O
in O
Figure O
6a O
. O

All O
of O
the O
following O
experiments O
are O
conducted O
on O
Spanish(es O
) O
data O
. O

4.7 O
Effect O
of O
Weights O
In O
this O
section O
, O
we O
evaluate O
the O
effectiveness O
of O
weight O
loss O
in O
student O
learning O
from O
a O
quantitative O
perspective O
. O

This O
validates O
the O
proposed O
MTMT B-MethodName
model O
not O
only O
transfers O
cross O
- O
lingual O
NER O
knowledge O
from O
source O
language O
, O
but O
also O
learns O
the O
similarity O
knowledge O
of O
target O
language O
data O
. O

We O
conjecture O
that O
the O
student O
model O
captures O
similarity O
knowledge O
from O
the O
similarity O
evaluator O
teacher O
, O
i.e. O
the O
same O
class O
of O
examples O
tend O
to O
cluster O
and O
the O
different O
class O
of O
examples O
tend O
to O
segregate O
in O
the O
embedding O
distribution O
. O

It O
can O
be O
seen O
that O
the O
embedding O
distribution O
of O
the O
student O
model O
is O
close O
to O
similarity O
evaluator O
teacher O
, O
as O
illustrated O
in O
Figure O
5 O
. O

This O
section O
investigates O
the O
effect O
of O
embeddings O
of O
the O
two O
different O
teacher O
models O
. O

4.6 O
Embedding O
Distribution O

Examples O
# O
2 O
and O
# O
3 O
present O
the O
same O
results O
with O
different O
sentences O
. O

The O
student O
learns O
from O
both O
teachers O
and O
predict O
the O
correct O
label O
for O
“ O
Arévalo O
” O
. O

The O
reason O
lies O
in O
that O
the O
entity O
recognizer O
teacher O
predicts O
“ O
Viena”(‘Madrid O
” O
) O
as O
B O
- O
LOC O
type O
correctly O
, O
and O
the O
similarity O
evaluator O
teacher O
predicts O
“ O
Viena”(“Madrid O
” O
) O
to O
have O
a O
high O
similarity O
score(0.7157 O
, O
0.7156 O
) O
with O
“ O
Arévalo O
” O
. O

As O
shown O
in O
Table O
6 O
, O
in O
example O
# O
1 O
, O
the O
entity O
recognizer O
teacher O
fails O
to O
identify O
“ O
Arévalo O
” O
as O
B O
- O
ORG O
type O
, O
while O
the O
student O
model O
can O
correctly O
predict O
it O
. O

Specifically O
, O
if O
there O
is O
a O
set O
of O
tokens O
in O
which O
every O
two O
of O
them O
have O
a O
high O
Entity O
Similarity O
score O
, O
and O
one O
of O
the O
tokens O
is O
predicted O
to O
have O
a O
distinct O
label O
while O
other O
tokens O
have O
identical O
labels O
, O
then O
the O
one O
with O
the O
distinct O
label O
is O
predicted O
wrongly O
and O
is O
corrected O
by O
the O
student O
model O
to O
have O
the O
label O
of O
all O
other O
tokens O
. O

The O
proposed O
MTMT B-MethodName
model O
can O
help O
to O
correct O
labels O
using O
the O
Entity O
Similarity O
defined O
in O
section O
3.2 O
. O

We O
try O
to O
bring O
up O
insights O
on O
why O
the O
proposed O
multiple B-MethodName
- I-MethodName
task I-MethodName
and I-MethodName
multiple I-MethodName
- I-MethodName
teacher I-MethodName
model O
works O
. O

4.5 O
Case O
Study O
We O
give O
a O
case O
study O
to O
show O
that O
the O
failed O
cases O
of O
baseline O
models O
can O
be O
corrected O
by O
our O
model O
. O

Without O
the O
similarity O
knowledge O
fed O
into O
the O
student O
model O
, O
the O
performance O
drops O
significantly O
. O

In O
this O
case O
, O
our O
approach O
degrades O
into O
the O
single O
teacherstudent O
learning O
model O
as O
in O
TSL B-MethodName
( O
Wu O
et O
al. O
, O
2020a O
) O
. O

1.0 O
w O
F1 O
0.8 O
0.6 O
0.6 O
0.4 O
0.4 O
0.4 O
0.2 O
0.2 O
0.0 O
0.2 O
0.4 O
0.6 O
0.8 O
1.0 O
0.0 O
( O
a O
) O
y O
y O
' O
0.8 O
0.6 O
0.0 O
( O
3 O
) O
MTMT B-MethodName
w/o I-MethodName
similarity I-MethodName
, O
which O
removes O
the O
similarity O
teacher O
model O
. O

1.0 B-MetricValue
for O
Dutch(nl O
) O
to O
0.98 B-MetricValue
for O
Spanish(es O
) O
, O
which O
validates O
that O
weighting O
loss O
can O
bring O
more O
confident O
knowledge O
to O
the O
student O
model O
. O

It O
can O
be O
seen O
that O
the O
performance O
decrease O
in O
terms O
of O
F1 B-MetricName
- I-MetricName
score I-MetricName
ranges O
from O
0.45 B-MetricValue

( O
2 O
) O
MTMT B-MethodName
w/o I-MethodName
weighting I-MethodName
, O
which O
set O
the O
α B-HyperparameterName
( I-HyperparameterName
· I-HyperparameterName
) I-HyperparameterName
, O
β B-HyperparameterName
and O
γ B-HyperparameterName
all O
to O
be O
1 B-HyperparameterValue
in O
the O
loss O
of O
student O
learning O
. O

This O
causes O
a O
performance O
drop O
across O
all O
languages O
due O
to O
two O
single O
teachers O
can O
not O
make O
a O
difference O
with O
the O
combination O
. O

That O
is O
, O
the O
teacher O
model O
has O
the O
same O
as O
the O
neural O
network O
structure O
of O
the O
student O
model O
. O

176 O
( O
1 O
) O
MTST B-MethodName
, O
which O
combines O
the O
multiple O
- O
teacher O
to O
single O
- O
teacher O
. O

Table O
5 O
presents O
the O
results O
. O

4.4 O
Ablation O
Study O
To O
demonstrate O
the O
effectiveness O
of O
our O
approach O
, O
we O
designed O
the O
following O
ablation O
studies O
. O

( O
c O
) O
Student O
. O
2021 O
) O
, O
RIKD B-MethodName
w/o I-MethodName
IKD I-MethodName
( O
Liang O
et O
al. O
, O
2021 O
) O
and O
Unitrans B-MethodName
w/o I-MethodName
translation I-MethodName
( O
Wu O
et O
al. O
, O
2020b O
) O
as O
reported O
in O
their O
paper O
. O

Moreover O
, O
compared O
with O
the O
latest O
model O
TOF B-MethodName
, O
RIKD B-MethodName
, O
Unitrans B-MethodName
, O
our O
model O
requires O
much O
lower O
computational O
costs O
for O
both O
translation O
and O
iterative O
knowledge O
distillation O
, O
meanwhile O
reaching O
superior O
performance O
. O

Note O
that O
BERT B-MethodName
- I-MethodName
f I-MethodName
performs O
better O
than O
our O
model O
on O
the O
Chinese O
dataset O
due O
to O
their O
re O
- O
tokenization O
of O
the O
dataset O
. O

That O
demonstrates O
the O
benefits O
of O
our O
proposed O
MTMT B-MethodName
model O
, O
compared O
to O
direct O
model O
transfer O
( O
Wu O
and O
Dredze O
, O
2019 O
) O
. O

Specifically O
, O
compared O
with O
the O
remarkable O
RIKD B-MethodName
, O
AdvPicker B-MethodName
, O
and O
Unitrans B-MethodName
, O
which O
also O
use O
knowledge O
distillation O
but O
ignore O
the O
entity O
similarity O
knowledge O
, O
our O
model O
obtains O
significant O
and O
consistent O
improvements O
in O
F1 B-MetricName
- I-MetricName
score I-MetricName
ranging O
from O
0.23 B-MetricValue
for O
German[de O
] O
to O
6.81 B-MetricValue
for O
Arabic[ar O
] O
. O

It O
can O
be O
seen O
that O
our O
model O
outperforms O
the O
state O
- O
of O
- O
the O
- O
arts O
. O

TOF B-MethodName
( O
Zhang O
et O
al. O
, O
2021 O
) O
transfers O
knowledge O
from O
three O
aspects O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
. O

RIKD B-MethodName
( O
Liang O
et O
al. O
, O
2021 O
) O
develops O
a O
reinforced O
iterative O
knowledge O
distillation O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
. O

AdvPicker B-MethodName
( O
Chen O
et O
al. O
, O
2021 O
) O
proposes O
a O
adversarial O
discriminator O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
. O

Unitrans B-MethodName
( O
Wu O
et O
al. O
, O
2020b O
) O
unifies O
a O
data O
transfer O
and O
model O
transfer O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
. O

TSL B-MethodName
( O
Wu O
et O
al. O
, O
2020c O
) O
proposes O
a O
teacher O
- O
student O
learning O
model O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
. O

https://github.com/huggingface/transformers O
175 O

BERT B-MethodName
- I-MethodName
f I-MethodName
( O
Wu O
and O
Dredze O
, O
2019 O
) O
applys O
the O
mBERT B-MethodName
to O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
. O

AdvCE B-MethodName
( O
Keung O
et O
al. O
, O
2019 O
) O
improves O
upon O
mBERT B-MethodName
via O
adversarial O
learning O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
. O

TMP B-MethodName
( O
Jain O
et O
al. O
, O
2019 O
) O
leverages O
machine O
translation O
to O
improve O
annotation O
projection O
approaches O
to O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
. O

WS B-MethodName
( O
Ni O
et O
al. O
, O
2017 O
) O
presents O
two O
weakly O
supervised O
approaches O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
. O

Wiki B-MethodName
( O
Tsai O
et O
al. O
, O
2016 O
) O
introduces O
a O
language O
independent O
model O
building O
on O
cross B-TaskName
- I-TaskName
lingual I-TaskName
wikification I-TaskName
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
. O

Moreover O
, O
we O
conduct O
each O
experiment O
5 O
times O
and O
report O
the O
mean O
F1 B-MetricName
- I-MetricName
score I-MetricName
. O

Following O
( O
Tjong O
Kim O
Sang O
, O
2002 O
) O
, O
we O
use O
the O
entity O
level O
F1 B-MetricName
- I-MetricName
score I-MetricName
as O
the O
evaluation O
metric O
. O

Note O
that O
if O
a O
word O
is O
divided O
into O
several O
subwords O
after O
tokenization O
, O
then O
only O
the O
first O
subword O
is O
considered O
in O
the O
loss O
function O
. O

For O
knowledge O
distillation O
, O
we O
use O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e-6 B-HyperparameterValue
for O
the O
student O
models O
training O
. O

For O
the O
training O
of O
recognition O
teacher O
model O
and O
similarity O
teacher O
model O
, O
we O
set O
the O
learning B-HyperparameterName
rate I-HyperparameterName
to O
be O
1e-5 B-HyperparameterValue
and O
5e-6 B-HyperparameterValue
separately O
. O

We O
set O
the O
batch B-HyperparameterName
size I-HyperparameterName
to O
be O
32 B-HyperparameterValue
, O
maximum B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
to O
be O
128 B-HyperparameterValue
, O
dropout B-HyperparameterName
rate I-HyperparameterName
to O
be O
0.2 B-HyperparameterValue
, O
and O
we O
use O
Adam B-HyperparameterValue
as O
optimizer B-HyperparameterName
( O
Kingma O
and O
Ba O
, O
2014 O
) O
. O

We O
do O
not O
freeze O
any O
layers O
and O
we O
use O
the O
output O
of O
the O
last O
layer O
as O
our O
hidden O
feature O
vector O
. O

We O
set O
our O
hyperparameters O
empirically O
following O
( O
Wu O
et O
al. O
, O
2020c O
) O
with O
some O
modifications O
. O

. O

Model O
ar O
hi O
zh O
BERT O
- O
f(Wu O
and O
Dredze O
, O
2019 O
) O
42.30 O
67.60 O
52.90 O
TSL(Wu O
et O
al. O
, O
2020a O
) O
43.12 O
69.54 O
48.12 O
RIKD(Liang O
et O
al. O
, O
2021 O
) O
45.96 O
70.28 O
50.40 O
MTMT O
52.77 O
70.76 O
52.26 O
Table O
2 O
: O
Statistics O
of O
WikiAnn O
. O
pre O
- O
trained O
mBERT O
model O
( O
Devlin O
et O
al. O
, O
2019 O
) O
in O
HuggingFace O
Transformer1 O
, O
which O
has O
12 B-HyperparameterValue
Transformer B-HyperparameterName
blocks I-HyperparameterName
, O
12 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
, O
and O
768 B-HyperparameterValue
hidden B-HyperparameterName
units I-HyperparameterName

All O
of O
the O
feature O
encoders O
mentioned O
in O
this O
paper O
use O
174 O

4.2 O
Implementation O
Details O
We O
use O
PyTorch O
1.7.1 O
to O
implement O
our O
model O
. O

Table O
1 O
and O
2 O
shows O
the O
statistics O
of O
all O
datasets O
. O

We O
trained O
the O
model O
with O
the O
labeled O
training O
set O
of O
the O
source O
language O
and O
evaluated O
the O
model O
on O
the O
test O
set O
of O
each O
target O
language O
. O

In O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
, O
the O
training O
set O
without O
entity O
label O
of O
the O
target O
language O
is O
also O
available O
when O
training O
the O
model O
. O

To O
imitate O
the O
zero B-TaskName
- I-TaskName
resource I-TaskName
cross I-TaskName
lingual I-TaskName
NER I-TaskName
case O
, O
following O
( O
Wu O
and O
Dredze O
, O
2019 O
) O
, O
we O
used O
English O
as O
the O
source O
language O
and O
other O
languages O
as O
the O
target O
language O
. O

Following O
( O
Wu O
and O
Dredze O
, O
2019 O
) O
, O
all O
datasets O
are O
annotated O
using O
the O
BIO O
entity O
labelling O
scheme O
. O

All O
datasets O
were O
annotated O
with O
four O
entity O
types O
: O
LOC O
, O
MISC O
, O
ORG O
, O
and O
PER O
. O

Each O
language O
is O
divided O
into O
a O
training O
set O
, O
a O
development O
set O
and O
a O
test O
set O
. O

CoNLL2002 B-DatasetName
includes O
Spanish O
and O
Dutch O
, O
CoNLL2003 B-DatasetName
includes O
English O
and O
German O
, O
and O
WikiAnn B-DatasetName
includes O
English O
and O
three O
non O
- O
western O
languages O
: O
Arabic O
, O
Hindi O
, O
and O
Chinese O
. O

Therefore O
, O
we O
heuristically O
devises O
the O
three O
weights O
scheduling O
as O
functions O
of O
the O
inputs O
, O
Dataset O
We O
conducted O
experiments O
on O
three O
benchmark O
datasets O
: O
CoNLL2002 B-DatasetName
( O
Tjong O
Kim O
Sang O
, O
2002 O
) O
, O
CoNLL2003 B-DatasetName
( O
Tjong O
Kim O
Sang O
and O
De O
Meulder O
, O
2003 O
) O
and O
WikiAnn B-DatasetName
( O
Pan O
et O
al. O
, O
2017 O
) O
. O

We O
want O
the O
student O
model O
to O
learn O
from O
the O
two O
teachers O
as O
follows O
: O
the O
higher O
the O
prediction O
of O
the O
entity O
recognizer O
teacher O
is O
( O
the O
further O
away O
from O
0.5 O
the O
prediction O
of O
the O
entity O
similarity O
teacher O
is O
, O
the O
higher O
the O
consistency O
level O
is O
) O
, O
the O
more O
accurate O
the O
prediction O
is O
, O
thus O
the O
more O
attention O
the O
student O
model O
pays O
attention O
to O
the O
input O
tokens O
, O
and O
vice O
versa O
. O

Note O
that O
supervision O
information O
yS O
, O
yS0 O
, O
and O
t̂S O
are O
taught O
by O
the O
three O
teacher O
models O
. O

1)2 O
Then O
for O
a O
specific O
sentence O
pair O
sample O
in O
the O
target O
siamese O
dataset O
, O
the O
student O
loss O
function O
has O
three O
breaches O
, O
LER O
( O
xT O
, O
yS O
, O
i O
) O
, O
LER O
( O
x0 O
T O
, O
yS0 O
, O
j O
) O
, O
and O
LSIM O
( O
xT O
, O
x0 O
T O
, O
i O
, O
j O
, O
t̂S O
) O
. O

σ(cos(hT O
i O
, O
h0 O
T O
j O
) O
) O

ŷT0 O
j O
= O
softmax(W O
h0 O
T O
j O
+ O
b O
) O
t̂T O
( O
xT O
, O
x0 O
T O
, O
i O
, O
j O
) O
= O

ŷTi O
= O
softmax(W O
hT O
i O
+ O
b O
) O
h0 O
T O
= O
mBERT(x0 O
T O
) O

mBERT(xT O
) O

hT O
= O

( O
b O
) O
indicates O
the O
weight O
β O
of O
LBCE O
. O

Specifically O
, O
for O
a O
sentence O
pair O
T O
−sim O
( O
xT O
, O
x0 O
T O
, O
i O
, O
j O
) O
∈ O

The O
mBERT O
is O
also O
used O
as O
an O
encoder O
for O
the O
sentence O
siamese O
pair O
, O
and O
the O
entity O
token O
feature O
is O
queried O
from O
the O
latent O
sequence O
encoding O
representation O
. O

173 O

i O
, O
j O
> O
uniformly O
sampled O
from O
the O
sentences O
therein O
. O

> O
randomly O
sample O
from O
Dtrain O
and O
the O
entity O
token O
indices O
pair O
< O

T O
< O
xT O
, O
xT O

Dtrain O
= O
{ O
( O
xT O
, O
x0 O
T O
, O
i O
, O
j O
) O
} O
, O
with O
the O
sentence O
pair O
0 O

Based O
on O
the O
original O
unlabeled O
target O
senT O
tence O
training O
data O
Dtrain O
, O
we O
again O
construct O
unlabeled O
target O
- O
language O
siamese O
pairwise O
entity O
data O
T O
−sim O

To O
this O
end O
, O
we O
propose O
a O
knowledge O
distillation O
learning O
process O
to O
train O
a O
target O
language O
student O
NER O
model O
with O
its O
supervisory O
signals O
mimicked O
by O
the O
entity O
type O
prediction O
probability O
by O
the O
entity O
recognizer O
teacher O
model O
and O
entity O
representation O
similarity O
target O
by O
the O
entity O
siamese O
similarity O
evaluator O
teacher O
model O
. O

3.3 O
Teacher O
- O
student O
Distillation O
Learning O
In O
this O
section O
, O
we O
consider O
transferring O
the O
named O
entity O
type O
and O
similarity O
knowledge O
learned O
on O
labeled O
source O
language O
corpus O
to O
unlabeled O
target O
language O
NER B-TaskName
task O
. O

Together O
with O
entity O
recognizer O
model O
, O
this O
entity O
similarity O
evaluator O
are O
used O
as O
teachers O
in O
following O
knowledge O
distillation O
learning O
process O
, O
and O
transfer O
knowledge O
from O
source O
to O
target O
lingual O
corpus O
. O

Finally O
, O
we O
can O
train O
the O
siamese O
entity O
similarS−siam O
ity O
evaluator O
on O
Dtrain O
, O
and O
evaluate O
the O
perS−siam O
formance O
on O
test O
dataset O
Dtest O
. O

LBCE O
( O
t O
, O
t̂ O
) O
. O

The O
loss O
function O
of O
the O
similarity O
prediction O
can O
be O
formulate O
as O
, O
LSIM O
( O
x O
, O
x0 O
, O
i O
, O
j O
, O
t O
) O
= O

Larger O
t̂ O
value O
indicates O
higher O
similarity O
between O
the O
two O
queried O
entities O
tokens O
. O

[ O
σ(−1 O
) O
, O
σ(1 O
) O
] O
denotes O
the O
predicted O
similarity O
of O
two O
queried O
tokens O
pair O
< O
xi O
, O
x0j O
> O
. O

where O
cos O
is O
the O
cosine O
similarity O
metric O
function O
, O
σ O
is O
the O
sigmoid O
activation O
function O
, O
t̂ O
∈ O

Linear O
Encoder O
structure O
could O
be O
formulated O
as O
, O
CELoss O
Similarity O
Score O
BCELoss O
Loss O
CELoss O
Student O
Figure O
3 O
: O
Teacher O
- O
student O
distillation O
learning O
. O

Teacher O
Inference O
Student O
Training O
Cos O
ŷi O
= O
softmax(W O
hi O
+ O
b O
) O
Encoder O
Encoder O
h O
= O
mBERT(x O
) O
h O
= O
mBERT(x O
) O
, O
h0 O
= O
mBERT(x0 O
) O

Similarity O
Score O
Evaluator O
Teacher O
Encoder O
Encoder O
We O
train O
this O
entity O
recognition O
teacher O
model O
S O
on O
the O
source O
lingual O
training O
corpus O
Dtrain O
= O
{ O
( O
x O
, O
y O
) O
} O
directly O
. O

Linear O
3.2.2 O
Siamese O
Entity O
Similarity O
Evaluator O
To O
leverage O
the O
entity O
similarity O
to O
boost O
the O
unsupervised O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
performance O
, O
we O
will O
present O
our O
entity O
pairs O
construction O
method O
and O
the O
siamese O
network O
model O
in O
the O
following O
. O

And O
the O
testing O
S−siam O
entity O
pairs O
Dtest O
is O
constructed O
likewisely O
. O

Dtrain O
= O
{ O
( O
x O
, O
x0 O
, O
i O
, O
j O
, O
t O
) O
} O
where O
the O
target O
t O
= O
1 O
indicates O
yi O
= O
yj0 O
, O
and O
0 O
otherwise O
. O

Dtrain O
, O
the O
siamese O
network O
could O
be O
formulated O
as O
, O
Cos O
Entity O
Similarity O
Pairs O
Construction O
According O
to O
entity O
labels O
, O
we O
randomly O
select O
sentences O
pair O
< O
x O
, O
x0 O
> O
with O
their O
some O
token O
pair O
< O
S O
, O
xi O
, O
x0j O
> O
and O
associated O
labels O
< O
yi O
, O
yj0 O
> O
in O
Dtrain O
to O
form O
the O
siamese O
supervision O
training O
dataset O
, O
S−siam O

( O
x O
, O
x0 O
, O
i O
, O
j O
, O
t O
) O
∈ O

More O
precisely O
, O
for O
a O
specific O
entity O
pair O
S−siam O

The O
cosine O
function O
operator O
is O
added O
to O
compute O
on O
the O
entity O
token O
latent O
vectors O
’ O
distance O
, O
s O
to O
measure O
the O
similarity O
between O
each O
siamese O
twin O
, O
which O
is O
fed O
into O
a O
single O
sigmoid O
output O
unit O
for O
target O
t̂ O
estimation O
. O

i O
, O
j O
> O
on O
the O
sequences O
representations O
. O

The O
inter O
- O
entities O
similarity O
is O
measured O
on O
the O
hidden O
representations O
hi O
and O
h0j O
of O
the O
tokens O
queried O
by O
the O
entity O
indices O
< O

Wherein O
h O
and O
h0 O
represent O
latent O
sequences O
encoding O
features O
derived O
by O
the O
two O
symmetric O
twins O
with O
respect O
to O
input O
sentence O
x O
and O
x0 O
respectively O
. O

Our O
similarity O
backbone O
model O
is O
a O
siamese O
neural O
network O
with O
mBERT O
as O
feature O
extraction O
layer O
. O

Linear O
Siamese O
Entity O
Similarity O
Network O

LCE O
( O
yi O
, O
ŷi O
) O
t̂(x O
, O
x0 O
, O
i O
, O
j O
) O
= O
σ(cos(hi O
, O
h0j O
) O
) O

Dtrain O
and O
an O
entity O
token O
query O
index O
i O
, O
the O
loss O
function O
is O
, O
LER O
( O
x O
, O
y O
, O
i O
) O
= O

For O
some O
sentence O
sample O
( O
x O
, O
y O
) O
∈ O

W O
and O
b O
are O
trainable O
paramS O
eters O
. O

ŷi O
denotes O
the O
predicted O
probability O
distribution O
for O
xi O
. O

Recognizer O
Teacher O
Unlabeled O
Target O
- O
Language O
Pairwise O
Data O
where O
h O
= O
{ O
hi O
} O
L O
i=1 O
and O
hi O
denotes O
the O
output O
of O
the O
pre O
- O
trained O
mBERT O
that O
corresponds O
to O
the O
input O
token O
xi O
. O

The O
model O
network O
172 O

And O
a O
linear O
classifier O
with O
softmax O
upon O
the O
pre O
- O
trained O
mBERT O
output O
. O

3.2.1 O
Entity O
Recognizer O
Since O
the O
cross O
- O
lingual O
NER O
task O
, O
we O
unitize O
multilingual O
mBERT B-MethodName
( O
Wu O
and O
Dredze O
, O
2019 O
) O
as O
basic O
sequence O
feature O
extractor O
backbone O
to O
derive O
the O
sequence O
embedding O
representation O
throughout O
this O
paper O
. O

The O
following O
subsections O
will O
illustrate O
the O
two O
teacher O
models O
sequentially O
. O

Figure O
2 O
illustrated O
the O
two O
teacher O
models O
training O
. O

Our O
similarity O
evaluator O
model O
, O
inspired O
by O
siamese O
network O
( O
Koch O
et O
al. O
, O
2015 O
) O
, O
are O
able O
to O
acquires O
more O
powerful O
features O
via O
capturing O
the O
invariances O
to O
transformation O
in O
the O
input O
space O
. O

To O
address O
this O
challenge O
, O
we O
propose O
a O
binary O
classifier O
called O
similarity O
evaluator O
to O
leverage O
the O
labeled O
source O
language O
data O
for O
similarity O
prediction O
. O

It O
is O
a O
non O
- O
trivial O
task O
since O
we O
lack O
golden O
labels O
to O
help O
us O
distinguish O
target O
named O
entities O
. O

We O
aim O
to O
find O
entity O
similarity O
to O
help O
the O
crosslingual B-TaskName
NER I-TaskName
model O
in O
the O
target O
language O
. O

For O
every O
two O
tokens O
, O
we O
define O
Entity O
Similarity O
Metric O
as O
a O
score O
which O
is O
the O
probability O
that O
two O
tokens O
belong O
to O
the O
same O
entity O
type O
. O

with O
Dtrain O
and O
Dtrain O
to O
perform O
well O
on O
Dtest O
3.2 O
Teacher O
Models O
Here O
we O
first O
consider O
the O
training O
of O
two O
teacher O
models O
. O

Formally O
, O
our O
goal O
is O
to O
train O
a O
model O
S O
T O
T O
. O

In O
the O
target O
language O
, O
we O
denote O
the O
T O
unlabeled O
train O
data O
as O
Dtrain O
= O
{ O
x O
} O
and O
the O
test O
T O
data O
as O
Dtest O
. O

In O
the O
source O
language O
, O
we O
denote O
the O
S O
labeled O
training O
data O
as O
Dtrain O
= O
{ O
( O
x O
, O
y O
) O
} O
and O
test O
S O
data O
as O
Dtest O
. O

= O
{ O
xi O
} O
L O
i=1 O
with O
L O
tokens O
, O
a O
NER O
model O
produces O
a O
sequence O
of O
labels O
y O
= O
{ O
yi O
} O
L O
i=1 O
, O
where O
xi O
is O
the O
i O
- O
th O
token O
and O
yi O
is O
the O
corresponding O
label O
of O
xi O
. O

To O
the O
best O
of O
our O
knowledge O
, O
we O
are O
the O
first O
to O
learn O
the O
entity O
similarity O
by O
siamese O
network O
. O

To O
handle O
this O
issue O
, O
we O
reconstruct O
the O
data O
to O
pair O
format O
. O

Siamese O
network O
assumes O
the O
input O
is O
a O
pair O
, O
and O
the O
output O
is O
a O
similarity O
score O
. O

However O
, O
there O
is O
a O
dilemma O
to O
adapt O
the O
siamese O
network O
to O
tokenlevel B-TaskName
recognition I-TaskName
tasks O
such O
as O
NER B-TaskName
. O

It O
has O
been O
successfully O
applied O
to O
transfer O
learning O
such O
as O
one O
- O
shot O
image O
recognition O
( O
Koch O
et O
al. O
, O
2015 O
) O
, O
text O
similarity O
( O
Neculoiu O
et O
al. O
, O
2016 O
) O
. O

Siamese O
Network O
is O
originally O
introduced O
by O
( O
Bromley O
et O
al. O
, O
1994 O
) O
to O
treat O
signature O
verification O
as O
a O
matching O
problem O
. O

We O
argue O
that O
the O
student O
entity O
recognition O
task O
and O
the O
student O
entity O
similarity O
evaluation O
task O
improve O
the O
representation O
learning O
of O
the O
student O
encoder O
in O
the O
siamese O
structure O
. O

To O
guarantee O
the O
student O
learning O
performance O
, O
we O
assign O
weights O
for O
each O
supervisory O
signal O
correspond O
to O
the O
output O
confidence O
of O
teacher O
sub O
- O
models O
. O

During O
the O
learning O
process O
, O
the O
samples O
from O
the O
target O
language O
are O
fed O
into O
the O
teacher O
model O
and O
the O
outputs O
are O
taken O
as O
the O
supervisory O
signal O
for O
two O
tasks O
in O
the O
student O
model O
. O

We O
note O
that O
, O
in O
this O
learning O
process O
, O
such O
a O
knowledge O
distillation O
makes O
the O
student O
model O
combine O
the O
advantages O
of O
both O
source O
language O
patterns O
of O
entity O
recognition O
and O
entity O
similarity O
evaluation O
. O

We O
then O
present O
a O
teacher O
- O
student O
distillation O
learning O
model O
to O
learn O
from O
the O
two O
learned O
teacher O
models O
simultaneously O
. O

These O
two O
models O
are O
two O
parallel O
tasks O
, O
wherein O
the O
entity O
recognition O
teacher O
focuses O
on O
identifying O
the O
named O
entities O
and O
the O
similarity O
evaluator O
teacher O
is O
to O
decide O
if O
two O
tokens O
are O
in O
the O
same O
type O
. O

In O
the O
teacher O
training O
model O
, O
there O
are O
two O
sub O
- O
models O
, O
i.e. O
an O
entity O
recognizer O
teacher O
and O
a O
similarity O
evaluator O
teacher O
. O

Our O
framework O
is O
consist O
of O
two O
models O
: O
teacher O
training O
model O
learned O
from O
the O
source O
language O
and O
teacher O
- O
student O
distillation O
learning O
model O
learned O
from O
the O
target O
language O
. O

In O
this O
section O
, O
we O
introduce O
our O
framework O
and O
its O
detailed O
implementation O
. O

Following O
standard O
practice O
, O
we O
formulate O
crosslingual B-TaskName
NER I-TaskName
as O
a O
sequence B-TaskName
labeling I-TaskName
task O
. O

In O
our O
work O
, O
the O
student O
model O
not O
only O
learns O
the O
recognizer O
teacher O
knowledge O
, O
but O
also O
171 O

Therefore O
, O
the O
student O
model O
can O
capture O
the O
extra O
knowledge O
about O
target O
languages O
. O

The O
student O
model O
learns O
from O
the O
soft O
label O
predicted O
by O
the O
teacher O
model O
on O
unlabeled O
target O
language O
data O
. O

The O
teacher O
model O
is O
trained O
on O
the O
labeled O
source O
language O
. O

Knowledge O
distillation O
based O
models O
include O
a O
teacher O
model O
and O
a O
student O
model O
( O
Wu O
et O
al. O
, O
2020c O
) O
. O

Our O
model O
achieves O
considerable O
improvement O
by O
learning O
entity O
similarity O
in O
target O
language O
data O
without O
translation O
. O

For O
example O
, O
( O
Wu O
et O
al. O
, O
2020b O
; O
Zhang O
et O
al. O
, O
2021 O
) O
gain O
an O
improvement O
by O
translating O
the O
labeled O
source O
language O
to O
the O
target O
language O
word O
- O
by O
- O
word O
. O

Translation O
based O
models O
generally O
generate O
pseudo O
- O
labeled O
target O
data O
to O
alleviate O
target O
data O
scarcity O
. O

The O
performance O
is O
still O
weak O
due O
to O
the O
lack O
of O
annotations O
of O
target O
languages O
. O

Moreover O
, O
some O
research O
introduces O
new O
components O
on O
top O
of O
the O
mBERT B-MethodName
by O
directly O
transferring O
the O
model O
learned O
from O
the O
labeled O
source O
language O
to O
that O
of O
target O
languages O
( O
Keung O
et O
al. O
, O
2019 O
) O
. O

Recently O
, O
the O
pre O
- O
trained O
multilingual B-MethodName
language I-MethodName
model I-MethodName
is O
effective O
to O
address O
the O
challenge O
( O
Devlin O
et O
al. O
, O
2019 O
) O
. O

Shared O
feature O
space O
based O
models O
generally O
train O
a O
language O
- O
independent O
encoder O
using O
source O
and O
target O
language O
data O
( O
Tsai O
et O
al. O
, O
2016 O
) O
. O

The O
existing O
models O
can O
be O
categorized O
to O
a O
) O
Shared O
feature O
space O
based O
models O
, O
b O
) O
Translation O
based O
models O
, O
c O
) O
Knowledge O
distillation O
based O
models O
. O

Cross B-TaskName
- I-TaskName
Lingual I-TaskName
NER I-TaskName
aims O
to O
extract O
entities O
from O
a O
target O
language O
but O
assumes O
only O
source O
language O
is O
annotated O
. O

2 O
Related O
Work O
Our O
approach O
is O
closely O
related O
to O
the O
existing O
works O
on O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
, O
knowledge O
distillation O
, O
and O
siamese O
network O
. O

• O
We O
conduct O
extensive O
experiments O
on O
7 O
languages O
compared O
with O
state O
- O
of O
- O
the O
- O
art O
baselines O
and O
the O
results O
confirm O
the O
effectiveness O
of O
the O
presented O
model O
. O

• O
We O
present O
a O
novel O
multiple B-MethodName
- I-MethodName
task I-MethodName
and I-MethodName
multipleteacher I-MethodName
model O
that O
introduces O
an O
entity O
similarity O
evaluator O
to O
boost O
the O
performance O
of O
student O
recognizer O
on O
target O
languages O
. O

Our O
main O
contributions O
are O
as O
follows O
: O
• O
We O
propose O
an O
unsupervised O
knowledge O
dis O
tillation O
framework O
for O
cross B-TaskName
- I-TaskName
lingual I-TaskName
named I-TaskName
entity I-TaskName
recognition I-TaskName
and O
develop O
a O
teaching O
and O
learning O
procedure O
under O
this O
framework O
. O

We O
validate O
the O
model O
performance O
on O
the O
three O
commonly O
- O
used O
datasets O
across O
7 O
languages O
and O
the O
experimental O
results O
show O
the O
superiority O
of O
our O
presented O
MTMT B-MethodName
model O
. O

In O
the O
student O
model O
, O
we O
then O
borrow O
the O
idea O
of O
multitask O
learning O
to O
incorporate O
a O
similarity O
evaluation O
task O
as O
an O
auxiliary O
task O
into O
the O
entity O
recognition O
classifier O
. O

Specifically O
, O
we O
first O
introduce O
the O
knowledge O
distillation O
to O
build O
entity O
recognizer O
and O
similarity O
evaluator O
teachers O
in O
the O
source O
language O
and O
transfer O
the O
learned O
patterns O
to O
the O
student O
in O
the O
target O
language O
. O

To O
leverage O
the O
similarity O
between O
the O
tokens O
of O
the O
source O
languages O
, O
we O
design O
an O
multiple B-MethodName
- I-MethodName
task I-MethodName
and I-MethodName
multiple I-MethodName
- I-MethodName
teacher I-MethodName
model O
( O
short O
as O
MTMT B-MethodName
, O
as O
shown O
in O
Figure O
1 O
) O
, O
which O
helps O
the O
NER O
learning O
process O
on O
the O
target O
languages O
. O

Then O
“ O
Arévalo O
” O
can O
be O
recognized O
correctly O
as O
LOC O
type O
under O
the O
supervisory O
signal O
using O
the O
similarity O
between O
“ O
Viena O
” O
and O
“ O
Madrid O
” O
. O

Also O
, O
the O
tokens O
“ O
Viena O
” O
and O
“ O
Madrid O
” O
are O
recognized O
correctly O
as O
LOC O
type O
using O
the O
same O
English O
model O
mentioned O
above O
. O

" O
, O
and O
“ O
Madrid O
” O
from O
sentence O
“ O
Madrid O
, O
23 O
may O
( O
EFE O
) O
. O
” O
. O

In O
the O
meantime O
, O
the O
token O
“ O
Arévalo O
” O
has O
high O
similarity O
scores O
with O
the O
Spanish O
tokens O
“ O
Viena O
” O
from O
sentence O
“ O
Viena O
, O
23 O
may O
( O
EFE O
) O
. O

Given O
a O
Spanish O
sentence O
“ O
Arévalo O
( O
Avila O
) O
, O
23 O
may O
( O
EFE O
) O
. O
” O
, O
the O
token O
“ O
Arévalo O
” O
is O
recognized O
as O
ORG O
type O
using O
the O
learned O
model O
from O
the O
English O
domain O
. O

Here O
we O
give O
a O
concrete O
example O
to O
illustrate O
the O
importance O
of O
similarity O
between O
every O
two O
tokens O
under O
the O
situation O
when O
only O
the O
English O
data O
is O
labeled O
. O

Due O
to O
the O
distributed O
representation O
of O
natural O
languages O
, O
the O
relatedness O
among O
the O
embedding O
of O
target O
languages O
, O
which O
is O
measured O
by O
the O
similarity O
, O
can O
be O
utilized O
to O
further O
boost O
the O
learned O
encoder O
and O
improve O
the O
final O
NER B-TaskName
performance O
on O
target O
languages O
. O

Although O
the O
above O
- O
mentioned O
models O
solve O
the O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
problem O
to O
some O
extent O
, O
the O
auxiliary O
tasks O
, O
as O
in O
multi O
- O
task O
learning O
, O
have O
not O
been O
studied O
in O
this O
problem O
. O

2021 O
) O
. O

Knowledge O
distillation O
based O
models O
train O
a O
student O
model O
using O
soft O
labels O
of O
the O
target O
language O
( O
Wu O
et O
al. O
, O
2020a O
, O
b O
; O
Chen O
et O
al. O
, O
2021 O
; O
Liang O
et O
al. O
, O
170 O
Proceedings O
of O
the O
60th O
Annual O
Meeting O
of O
the O
Association O
for O
Computational O
Linguistics O
Volume O
1 O
: O
Long O
Papers O
, O
pages O
170 O
- O
179 O
May O
22 O
- O
27 O
, O
2022 O
c O
2022 O
Association O
for O
Computational O
Linguistics O

( O
Mayhew O
et O
al. O
, O
2017 O
; O
Xie O
et O
al. O
, O
2018 O
; O
Wu O
et O
al. O
, O
2020b O
) O
. O

Translation O
based O
models O
generate O
pseudo O
labeled O
target O
language O
data O
to O
train O
the O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
model O
, O
but O
the O
noise O
from O
translation O
process O
restrains O
its O
performance O
. O

Shared O
feature O
space O
based O
models O
exploit O
language O
- O
independent O
features O
, O
which O
lacks O
the O
domain O
- O
specific O
features O
for O
the O
target O
language O
( O
Tsai O
et O
al. O
, O
2016 O
; O
Wu O
and O
Dredze O
, O
2019 O
; O
Keung O
et O
al. O
, O
2019 O
) O
. O

Existing O
models O
can O
be O
separated O
into O
three O
categories O
, O
shared O
feature O
space O
based O
, O
translation O
based O
and O
knowledge O
distillation O
based O
. O

Introduction O
∗ O
NERstu O
Many O
studies O
have O
been O
done O
to O
solve O
this O
crosslingual B-TaskName
NER I-TaskName
problem I-TaskName
. O

With O
the O
help O
of O
transfer O
learning O
( O
Ruder O
et O
al. O
, O
2019 O
) O
and O
multilingual B-MethodName
BERT I-MethodName
( O
short O
as O
mBERT B-MethodName
) O
( O
Devlin O
et O
al. O
, O
Corresponding O
author O
{ O
X O
, O
P}tgt O
NER B-TaskName

This O
situation O
is O
more O
severe O
for O
zero O
- O
resource O
languages O
. O

However O
, O
since O
deep O
neural O
networks O
highly O
rely O
on O
a O
large O
amount O
of O
labelled O
training O
data O
, O
the O
annotation O
acquiring O
process O
is O
expensive O
and O
time O
consuming O
. O

The O
exploiting O
of O
deep O
neural O
networks O
, O
such O
as O
Bi B-MethodName
- I-MethodName
LSTM I-MethodName
- I-MethodName
CRF I-MethodName
( O
Lample O
et O
al. O
, O
2016 O
) O
, O
Bi B-MethodName
- I-MethodName
LSTM I-MethodName
- I-MethodName
CNN I-MethodName
( O
Chiu O
and O
Nichols O
, O
2016 O
) O
makes O
this O
task O
achieve O
significant O
performances O
. O

Named B-TaskName
entity I-TaskName
recognition I-TaskName
, O
NER B-TaskName
in O
short O
, O
refers O
to O
identifying O
entity O
types O
, O
i.e. O
location O
, O
person O
, O
organization O
, O
etc O
. O
, O
in O
a O
given O
sentence O
. O

2019 O
) O
, O
it O
is O
possible O
to O
transfer O
the O
annotated O
training O
samples O
or O
trained O
models O
from O
a O
rich O
- O
resource O
domain O
to O
a O
zero O
- O
resource O
domain O
. O

Empirical O
studies O
on O
the O
three O
datasets O
across O
7 O
different O
languages O
confirm O
the O
effectiveness O
of O
the O
proposed O
model O
. O

Then O
, O
two O
tasks O
in O
the O
student O
model O
are O
supervised O
by O
these O
teachers O
simultaneously O
. O

Specifically O
, O
an O
entity O
recognizer O
and O
a O
similarity O
evaluator O
are O
first O
trained O
in O
parallel O
as O
two O
teachers O
from O
the O
source O
domain O
. O

In O
this O
study O
, O
based O
on O
the O
knowledge O
distillation O
framework O
and O
multitask O
learning O
, O
we O
introduce O
the O
similarity O
metric O
model O
as O
an O
auxiliary O
task O
to O
improve O
the O
cross B-TaskName
- I-TaskName
lingual I-TaskName
NER I-TaskName
performance O
on O
the O
target O
domain O
. O

However O
, O
existing O
cross O
- O
lingual O
distillation O
models O
merely O
consider O
the O
potential O
transferability O
between O
two O
identical O
single O
tasks O
across O
both O
domains O
. O

Knowledge O
distillation O
using O
pre O
- O
trained O
multilingual O
language O
models O
between O
source O
and O
target O
languages O
have O
shown O
their O
superiority O
in O
transfer O
. O

Abstract O
NERstu O
training O
Cross B-TaskName
- I-TaskName
lingual I-TaskName
named I-TaskName
entity I-TaskName
recognition I-TaskName
task O
is O
one O
of O
the O
critical O
problems O
for O
evaluating O
the O
potential O
transfer O
learning O
techniques O
on O
low O
resource O
languages O
. O

SKLSDE O
, O
School O
of O
Computer O
Science O
and O
Engineering O
, O
Beihang O
University O
, O
Beijing O
, O
China O
2 O
Hangzhou O
Innovation O
Institute O
, O
Beihang O
University O
, O
Hangzhou O
, O
China O
{ O
lizhuoranget O
, O
hucm}@buaa.edu.cn O
, O
wenyi.qin@hotmail.com O
{ O
guoxh O
, O
chenjf O
, O
zhangrc}@act.buaa.edu.cn O

An O
Unsupervised O
Multiple B-MethodName
- I-MethodName
Task I-MethodName
and I-MethodName
Multiple I-MethodName
- I-MethodName
Teacher I-MethodName
Model O
for O
Cross B-TaskName
- I-TaskName
lingual I-TaskName
Named I-TaskName
Entity I-TaskName
Recognition I-TaskName
Zhuoran O
Li1 O
, O
Chunming O
Hu1∗ O
, O
Xiaohui O
Guo2 O
Junfan O
Chen1 O
, O
Wenyi O
Qin1 O
, O
Richong O
Zhang1 O
1 O

-DOCSTART- O

17 O

16 O

15 O

14 O

A.8 O
E O
XAMPLE O
P O
REDICTIONS O
In O
Figures O
3 O
, O
4 O
, O
5 O
, O
6 O
, O
7 O
we O
show O
predictions O
by O
our O
model O
that O
shows O
the O
learned O
execution O
of O
various O
modules O
defined O
in O
the O
paper O
. O

This O
is O
exactly O
the O
same O
as O
previous O
for O
find O
- O
num O
module O
calls O
by O
compare O
- O
num O
- O
lt O
. O

3 O
. O
were O
there O
fewer O
SPAN1 O
or O
SPAN2 O
? O

Similar O
to O
above O
, O
we O
perform O
fuzzy O
matching O
to O
find O
the O
instance O
of O
EVENT1 O
and O
EVENT2 O
in O
the O
paragraph O
and O
assume O
that O
the O
closest O
dates O
should O
be O
the O
output O
of O
the O
two O
find O
- O
date O
module O
calls O
made O
by O
the O
compare O
- O
date O
- O
lt O
module O
in O
the O
gold O
program O
. O

2 O
. O
what O
happened O
first O
EVENT1 O
or O
EVENT2 O
? O

We O
find O
all O
instances O
of O
touchdown O
/ O
field O
goal O
in O
the O
passage O
and O
assume O
that O
the O
number O
appearing O
closest O
should O
be O
an O
output O
of O
the O
find O
- O
num O
module O
. O

These O
are O
as O
follows O
: O
1 O
. O
how O
many O
yards O
was O
the O
longest O
/ O
shortest O
{ O
touchdown O
, O
field O
goal O
} O
? O

As O
mentioned O
in O
Section O
4.3 O
, O
we O
heuristically O
find O
supervision O
for O
the O
output O
of O
the O
find O
- O
num O
and O
find O
- O
date O
module O
for O
a O
subset O
of O
questions O
that O
already O
contain O
question O
program O
supervision O
. O

A.7 O
H O
EURISTIC O
I O
NTERMEDIATE O
M O
ODULE O
O O
UTPUT O
S O
UPERVISION O

5 O
. O
how O
many O
{ O
field O
goals O
, O
touchdowns O
, O
passes O
} O
were O
scored O
SPAN O
? O
count(filter(find O
( O
) O
) O
): O
with O
find O
attention O
on O
{ O
field O
goals O
, O
touchdowns O
, O
passes O
} O
and O
filter O
attention O
on O
SPAN O
. O
6 O
. O
who O
{ O
kicked O
, O
caught O
, O
threw O
, O
scored O
} O
SPAN O
? O
span(relocate(filter(find O
( O
) O
) O
) O
): O
with O
relocate O
attention O
on O
{ O
kicked O
, O
caught O
, O
threw O
, O
scored O
} O
, O
find O
attention O
on O
{ O
touchdown O
/ O
field O
goal O
} O
, O
and O
filter O
attention O
on O
all O
other O
tokens O
in O
the O
SPAN O
. O
13 O

with O
find O
attention O
on O
touchdown O
/ O
field O
goal O
and O
filter O
attention O
on O
all O
SPAN O
tokens O
. O

find O
- O
num(find O
- O
max O
- O
num(filter(find O
( O
) O
) O
) O
): O

4 O
. O
how O
many O
yards O
was O
the O
longest O
{ O
touchdown O
/ O
field O
goal O
} O
SPAN O
? O

For O
shortest O
, O
the O
find O
- O
min O
- O
num O
module O
is O
used O
. O

find O
- O
num(find O
- O
max O
- O
num(find O
( O
) O
) O
): O
with O
find O
attention O
on O
touchdown O
/ O
field O
goal O
. O

3 O
. O
how O
many O
yards O
was O
the O
longest O
{ O
touchdown O
/ O
field O
goal O
} O
? O

Use O
compare O
- O
num O
- O
gt O
, O
if O
more O
instead O
of O
fewer O
. O

2 O
. O
were O
there O
fewer O
SPAN1 O
or O
SPAN2 O
? O
span(compare O
- O
num O
- O
lt(find O
( O
) O
, O
find O
( O
) O
) O
): O
with O
find O
attentions O
on O
SPAN1 O
and O
SPAN2 O
, O
respectively O
. O

Use O
compare O
- O
date O
- O
gt O
, O
if O
second O
instead O
of O
first O
. O

The O
following O
patterns O
are O
used O
to O
extract O
the O
question O
parse O
supervision O
for O
the O
training O
data O
: O
1 O
. O
what O
happened O
first O
SPAN1 O
or O
SPAN2 O
? O
span(compare O
- O
date O
- O
lt(find O
( O
) O
, O
find O
( O
) O
) O
): O
with O
find O
attentions O
on O
SPAN1 O
and O
SPAN2 O
, O
respectively O
. O

Since O
the O
predicted O
attention O
is O
a O
normalized O
distribution O
, O
the O
objective O
increases O
the O
sum O
of O
log O
- O
probabilities O
of O
the O
tokens O
in O
the O
supervision O
. O

The O
loss O
against O
the O
predicted O
attention O
n O
vector O
α O
is O
, O
Qloss O
= O
− O
i=1 O
αi∗ O
log O
αi O
. O

The O
question O
attention O
supervision O
is O
providedPas O
a O
mutli O
- O
hot O
vector O
α∗ O
∈ O
{ O
0 O
, O
1}n O
. O

We O
also O
add O
a O
loss O
for O
the O
decoder O
to O
attend O
to O
the O
tokens O
in O
the O
question O
attention O
supervision O
when O
predicting O
the O
relevant O
modules O
. O

A.6 O
AUXILIARY O
Q O
UESTION O
PARSE O
S O
UPERVISION O
For O
questions O
with O
parse O
supervision O
z∗ O
, O
we O
decouple O
the O
marginal O
likelihood O
into O
two O
maximum O
likelihood O
objectives O
, O
p(z∗ O
|q O
) O
and O
p(y O
∗ O
|z∗ O
) O
. O

A O
softmax O
operation O
on O
these O
scores O
gives O
the O
output O
probabilities O
. O

The O
input O
paragraph O
attention O
is O
first O
scaled O
using O
[ O
1 O
, O
2 O
, O
5 O
, O
10 O
] O
, O
then O
a O
bidirectional O
- O
GRU O
represents O
each O
attention O
as O
a O
hidden O
vector O
, O
and O
a O
single B-HyperparameterValue
- O
layer B-HyperparameterName
feed O
- O
forward O
network O
maps O
this O
to O
2 O
scores O
, O
for O
span O
start O
and O
end O
. O

The O
span O
module O
is O
implemented O
similar O
to O
the O
count O
module O
. O

A.5 O
S O
PAN O
M O
ODULE O

The O
final O
feed O
- O
forward O
comprises O
of O
a O
single B-HyperparameterValue
- O
layer B-HyperparameterName
to O
map O
the O
output O
of O
the O
countGRU O
into O
a O
scalar O
score O
. O

The O
countGRU O
in O
the O
count O
module O
( O
spanGRU O
– O
span O
module O
) O
is O
a O
2 B-HyperparameterValue
- O
layer B-HyperparameterName
, O
bi O
- O
directional O
GRU O
with O
input B-HyperparameterName
- I-HyperparameterName
dim I-HyperparameterName
= O
4 B-HyperparameterValue
and O
output B-HyperparameterName
- I-HyperparameterName
dim I-HyperparameterName
= O
20 B-HyperparameterValue
. O

We O
train O
the O
parameters O
of O
the O
count O
module O
using O
these O
generated O
instances O
using O
L2 O
-loss O
between O
the O
true O
count O
value O
and O
the O
predicted O
cv O
. O

We O
then O
add O
0 O
- O
mean O
, O
0.01 O
- O
variance O
gaussian O
noise O
to O
all O
elements O
in O
x O
and O
normalize O
to O
make O
the O
normalized O
attention O
vector O
that O
can O
be O
input O
to O
the O
count O
module O
. O

For O
all O
these O
y O
spans O
in O
x O
, O
we O
put O
a O
value O
of O
1.0 O
and O
zeros O
everywhere O
else O
. O

We O
then O
sample O
y O
span O
- O
lengths O
between O
5 O
− O
15 O
and O
also O
sample O
y O
non O
- O
overlapping O
span O
- O
positions O
in O
the O
attention O
vector O
x. O

This O
is O
generated O
by O
sampling O
m O
uniformly O
between O
200 O
− O
600 O
, O
then O
sampling O
a O
count O
value O
y O
uniformly O
in O
[ O
0 O
, O
9 O
] O
. O

[ O
0 O
, O
9 O
] O
. O

Rm O
and O
a O
count O
value O
y O
∈ O

We O
generate O
synthetic O
data O
to O
pre O
- O
train O
this O
module O
; O
each O
instance O
is O
a O
normalized O
- O
attention O
vector O
x O
= O

Rm O
X O
cv O
= O
countscores O
∈ O
R O

∈ O

 O
 O
countscores O
= O
σ O
F O
F O
( O
countGRU(Pscaled O
) O
) O

Published O
as O
a O
conference O
paper O
at O
ICLR O
2020 O
are O
summed O
to O
compute O
a O
count O
value O
, O
cv O
. O

These O
scores O
3 O
4 O
https://spacy.io/ O
https://github.com/scrapinghub/dateparser O
12 O

A O
single B-HyperparameterValue
- O
layer B-HyperparameterName
feed O
- O
forward O
network O
maps O
this O
representation O
to O
a O
soft O
0/1 O
score O
to O
indicate O
the O
presence O
of O
a O
span O
surrounding O
it O
. O

A O
bidirectional O
- O
GRU O
then O
represents O
each O
token O
attention O
as O
a O
hidden O
vector O
ht O
. O

The O
module O
first O
scales O
the O
attention O
using O
the O
values O
[ O
1 O
, O
2 O
, O
5 O
, O
10 O
] O
to O
convert O
it O
into O
a O
matrix O
Pscaled O
∈ O
Rm×4 O
. O

To O
re O
- O
iterate O
, O
the O
module O
gets O
as O
input O
a O
paragraph O
attention O
P O
∈ O
Rm O
. O

As O
mentioned O
in O
the O
paper O
, O
training O
the O
count O
module O
is O
challenging O
and O
found O
that O
pre O
- O
training O
the O
parameters O
of O
the O
count O
module O
helps O
. O

The O
total O
number O
of O
date O
- O
tokens O
is O
denoted O
by O
Dtokens O
A.4 O
P O
RE O
- O
TRAINING O
C O
OUNT O
M O
ODULE O

For O
example O
, O
a O
date O
mention O
“ O
19th O
November O
, O
1961 O
” O
would O
be O
normalized O
to O
( O
19 O
, O
11 O
, O
1961 O
) O
( O
day O
, O
month O
, O
year O
) O
. O

To O
normalize O
the O
date O
mentions O
we O
use O
an O
off O
- O
the O
- O
shelf O
date O
- O
parser4 O
. O

To O
extract O
dates O
from O
the O
paragraph O
, O
we O
run O
the O
spaCy O
- O
NER3 O
and O
collect O
all O
“ O
DATE O
” O
mentions O
. O

We O
do O
not O
normalize O
numbers O
based O
on O
their O
units O
and O
leave O
it O
for O
future O
work O
. O

The O
total O
number O
of O
number O
- O
tokens O
in O
the O
paragraph O
is O
denoted O
by O
Ntokens O
. O

For O
example O
, O
200 O
in O
“ O
200 O
women O
” O
. O

For O
numbers O
, O
we O
use O
a O
simple O
strategy O
where O
all O
tokens O
in O
the O
paragraph O
that O
can O
be O
parsed O
as O
a O
number O
are O
extracted O
. O

A.3 O
N O
UMBER O
AND O
DATE O
PARSING O
We O
pre O
- O
process O
the O
paragraphs O
to O
extract O
the O
numbers O
and O
dates O
in O
them O
. O

Optmization O
is O
performed O
using O
the O
Adam B-HyperparameterName
algorithm O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
0.001 B-HyperparameterValue
or O
using O
BERT O
’s O
optimizer O
with O
a O
learning B-HyperparameterName
rate I-HyperparameterName
of O
1e B-HyperparameterValue
− I-HyperparameterValue
5 I-HyperparameterValue
. O

We O
use O
a O
beam B-HyperparameterName
- I-HyperparameterName
size I-HyperparameterName
of O
4 B-HyperparameterValue
for O
the O
approximate O
maximum O
marginal O
likelihood O
objective O
. O

As O
the O
memory O
- O
state O
for O
the O
zero O
- O
eth O
time O
- O
step O
in O
the O
decoder O
, O
we O
use O
the O
last O
hidden O
- O
state O
of O
the O
question O
encoder O
GRU O
, O
or O
the O
[ O
CLS O
] O
embedding O
for O
the O
BERT O
- O
based O
model O
. O

The O
attention O
is O
computed O
as O
a O
dot O
- O
product O
between O
the O
decoder O
hidden O
- O
state O
and O
the O
encoders O
hidden O
states O
which O
is O
normalized O
using O
the O
softmax O
operation O
. O

For O
each O
module O
, O
we O
use O
a O
100 B-HyperparameterValue
- O
dimensional B-HyperparameterName
embedding O
to O
present O
it O
as O
an O
action O
in O
the O
decoder O
’s O
input O
/ O
output O
vocabulary O
. O

The O
decoder O
for O
question O
parsing O
is O
a O
single B-HyperparameterValue
- O
layer B-HyperparameterName
, O
100 B-HyperparameterValue
- O
dimensional B-HyperparameterName
, O
LSTM O
. O

A.2 O
Q O
UESTION O
PARSER O
D O
ECODER O

We O
use O
‘ O
bert O
- O
base O
- O
uncased O
‘ O
model O
for O
all O
out O
experiments O
. O

We O
separate O
the O
question O
and O
context O
representation O
from O
the O
output O
of O
BERT O
as O
Q O
and O
P O
, O
respectively O
. O

The O
question O
and O
context O
tokens O
input O
to O
the O
BERT O
model O
are O
sub O
- O
words O
extracted O
by O
using O
BERT O
’s O
tokenizer O
. O

Context O
[ O
SEP O
] O
. O

[ O
CLS O
] O
Question O
[ O
SEP O
] O

The O
input O
to O
the O
BERT O
model O
is O
the O
concatenation O
of O
the O
question O
and O
paragraph O
in O
the O
following O
format O
: O

BERT O
: O

The O
CNN O
uses O
filters O
of O
size=5 B-HyperparameterName
and O
character O
embeddings O
of O
64 B-HyperparameterValue
- O
d. B-HyperparameterName
The O
pre O
- O
trained O
glove O
embeddings O
are O
fixed O
, O
but O
the O
character O
embeddings O
and O
the O
parameters O
for O
the O
CNN O
are O
jointly O
learned O
with O
the O
rest O
of O
the O
model O
. O

The O
token O
embeddings O
input O
to O
the O
contextual O
encoder O
are O
a O
concatenation O
of O
100 B-HyperparameterValue
- O
d B-HyperparameterName
pre O
- O
trained O
GloVe O
embeddings O
, O
and O
200 B-HyperparameterValue
- O
d B-HyperparameterName
embeddings O
output O
from O
a O
CNN O
over O
the O
token O
’s O
characters O
. O

The O
same O
GRU O
is O
used O
for O
both O
, O
the O
question O
and O
the O
paragraph O
. O

GRU O
: O
We O
use O
a O
2 B-HyperparameterValue
- O
layer B-HyperparameterName
, O
64 B-HyperparameterValue
- O
dimensional B-HyperparameterName
( O
d B-HyperparameterName
= O
128 B-HyperparameterValue
, O
effectively O
) O
, O
bi O
- O
directional O
GRU O
. O

These O
embeddings O
are O
either O
produced O
using O
a O
multi O
- O
layer O
GRU O
network O
that O
is O
trained O
from O
scratch O
, O
or O
a O
pr O
- O
trained O
BERT O
model O
that O
is O
fine O
- O
tuned O
during O
training O
. O

A O
A O
PPENDIX O
A.1 O
Q O
UESTION O
AND O
PARAGRAPH O
E O
NCODER O
Our O
model O
represents O
the O
question O
q O
as O
Q O
∈ O
Rn×d O
and O
paragraph O
p O
as O
P O
∈ O
Rm×d O
using O
contextualized O
token O
embeddings O
. O

N660011924033 O
with O
the O
United O
States O
Office O
Of O
Naval O
Research O
, O
an O
ONR O
award O
, O
the O
LwLL O
DARPA O
program O
, O
and O
a O
grant O
from O
AI2 O
. O

This O
material O
is O
based O
upon O
work O
sponsored O
in O
part O
by O
the O
DARPA O
MCS O
program O
under O
Contract O
No O
. O

ACKNOWLEDGMENTS O
We O
would O
like O
to O
thank O
Daniel O
Deutsch O
and O
the O
anonymous O
reviewers O
for O
their O
helpful O
comments O
. O

Future O
research O
is O
necessary O
to O
continue O
bridging O
these O
reasoning B-TaskName
gaps O
. O

NMNs B-MethodName
provide O
interpretability O
, O
compositionality O
, O
and O
improved O
generalizability O
, O
but O
at O
the O
cost O
of O
restricted O
expressivity O
as O
compared O
to O
more O
black O
box O
models O
. O

While O
we O
have O
demonstrated O
marked O
success O
in O
broadening O
the O
scope O
of O
neural O
modules O
and O
applying O
them O
to O
open O
- O
domain O
text O
, O
it O
remains O
a O
significant O
challenge O
to O
extend O
these O
models O
to O
the O
full O
range O
of O
reasoning O
required O
even O
just O
for O
the O
DROP B-DatasetName
dataset O
. O

Additionally O
, O
we O
show O
that O
injecting O
inductive O
bias O
using O
unsupervised O
auxiliary O
losses O
significantly O
helps O
learning O
. O

We O
define O
probabilistic O
modules O
that O
propagate O
uncertainty O
about O
symbolic B-TaskName
reasoning I-TaskName
operations O
in O
a O
way O
that O
is O
end O
- O
to O
- O
end O
differentiable O
. O

We O
show O
how O
to O
use O
neural B-MethodName
module I-MethodName
networks I-MethodName
to O
answer B-TaskName
compositional I-TaskName
questions I-TaskName
requiring O
symbolic B-TaskName
reasoning I-TaskName
against O
natural O
language O
text O
. O

8 O
C O
ONCLUSION O

Combining O
these O
black O
- O
box O
operations O
with O
the O
interpretable O
modules O
that O
we O
have O
presented O
is O
an O
interesting O
and O
important O
challenge O
for O
future O
work O
. O

Additionally O
, O
due O
to O
our O
lack O
of O
supervised O
programs O
, O
training O
the O
network O
to O
use O
the O
interpretable O
modules O
instead O
of O
a O
black O
- O
box O
shortcut O
module O
is O
challenging O
, O
further O
compounding O
the O
issue O
. O

This O
also O
harms O
the O
ability O
of O
the O
model O
to O
use O
the O
interpretable O
modules O
even O
when O
they O
would O
be O
sufficient O
to O
answer O
the O
question O
. O

Allowing O
black O
- O
box O
operations O
inside O
of O
a O
neural B-MethodName
module I-MethodName
network I-MethodName
significantly O
harms O
the O
interpretability O
— O
e.g. O
, O
an O
operation O
that O
directly O
answers O
a O
question O
after O
an O
encoder O
, O
mimicking O
BERT O
- O
QA O
- O
style O
models O
, O
encourages O
the O
encoder O
to O
perform O
complex B-TaskName
reasoning I-TaskName
in O
a O
non O
- O
interpretable O
way O
. O

It O
is O
not O
trivial O
to O
combine O
the O
two O
approaches O
, O
however O
. O

To O
solve O
both O
of O
these O
classes O
of O
errors O
, O
one O
could O
use O
black O
- O
box O
models O
, O
which O
gain O
performance O
on O
some O
questions O
at O
the O
expense O
of O
limited O
interpretability O
. O

Direct O
transfer O
of O
reasoning O
capability O
in O
black O
- O
box O
models O
is O
not O
so O
straight O
- O
forward O
. O

Published O
as O
a O
conference O
paper O
at O
ICLR O
2020 O
using O
indirect O
or O
distant O
supervision O
from O
different O
tasks O
. O

This O
additionally O
opens O
up O
avenues O
for O
transfer O
learning O
where O
modules O
can O
be O
independently O
trained O
9 O

For O
mistakes O
such O
as O
these O
, O
our O
NMN B-MethodName
based O
approach O
allows O
for O
identifying O
the O
cause O
of O
mistakes O
and O
supervising O
these O
modules O
using O
additional O
auxiliary O
supervision O
that O
is O
not O
possible O
in O
black O
- O
box O
models O
. O

For O
example O
, O
incorrect O
grounding O
by O
the O
find O
module O
, O
or O
incorrect O
argument O
extraction O
by O
the O
find O
- O
num O
module O
. O

In O
part O
due O
to O
this O
training O
problem O
, O
some O
other O
mistakes O
of O
our O
model O
relative O
to O
MTMSN B-MethodName
on O
the O
full O
dataset O
are O
due O
to O
incorrect O
execution O
of O
the O
intermediate O
modules O
. O

This O
is O
why O
we O
focus O
on O
only O
a O
subset O
of O
the O
data O
when O
training O
our O
model O
. O

The O
modules O
in O
the O
predicted O
program O
get O
updated O
to O
try O
to O
perform O
the O
reasoning B-TaskName
anyway O
, O
which O
harms O
their O
ability O
to O
execute O
their O
intended O
operations O
( O
cf O
. O
§ O
2.2 O
) O
. O

It O
is O
worth O
emphasizing O
here O
what O
happens O
when O
we O
try O
to O
train O
our O
model O
on O
these O
questions O
for O
which O
our O
modules O
ca O
n’t O
express O
the O
correct O
reasoning O
. O

It O
is O
not O
always O
clear O
how O
to O
design O
interpretable O
modules O
for O
certain O
operations O
; O
for O
example O
, O
for O
the O
last O
two O
cases O
above O
. O

would O
require O
IE O
for O
implicit O
argument O
( O
points O
scored O
by O
the O
other O
team O
) O
. O

would O
require O
designing O
modules O
that O
considers O
some O
key O
- O
value O
representation O
of O
the O
paragraph O
; O
( O
c O
) O
How O
many O
points O
did O
the O
packers O
fall O
behind O
during O
the O
game O
? O

and O
In O
which O
quarter O
did O
the O
teams O
both O
score O
the O
same O
number O
of O
points O
? O

would O
require O
pruning O
passage O
spans O
based O
on O
the O
numerical O
comparison O
mentioned O
in O
the O
question O
; O
( O
b O
) O
Which O
quarterback O
threw O
the O
most O
touchdown O
passes O
? O

and O
Which O
racial O
groups O
are O
smaller O
than O
2 O
% O
? O

For O
example O
, O
questions O
such O
as O
( O
a O
) O
How O
many O
languages O
each O
had O
less O
than O
115 O
, O
000 O
speakers O
in O
the O
population O
? O

Manual O
analysis O
of O
predictions O
reveals O
that O
a O
significant O
majority O
of O
mistakes O
are O
due O
to O
insufficient O
reasoning O
capability O
in O
our O
model O
and O
would O
require O
designing O
additional O
modules O
. O

The O
resulting O
model O
achieves O
a O
score O
of O
65.4 B-MetricValue
F1 B-MetricName
on O
the O
complete O
validation O
data O
of O
DROP B-DatasetName
, O
as O
compared O
to O
MTMSN B-MethodName
that O
achieves O
72.8 B-MetricValue
F1 B-MetricName
. O

7 O
F O
UTURE O
D O
IRECTIONS O
We O
try O
a O
trivial O
extension O
to O
our O
model O
by O
adding O
a O
module O
that O
allows O
for O
addition O
& O
subtraction O
between O
two O
paragraph O
numbers O
. O

Concurrently O
, O
Jiang O
& O
Bansal O
( O
2019 O
) O
apply O
NMN B-MethodName
to O
HotpotQA B-DatasetName
( O
Yang O
et O
al. O
, O
2018 O
) O
but O
their O
model O
comprises O
of O
only O
3 O
modules O
and O
is O
not O
capable O
of O
performing O
symbolic B-TaskName
reasoning I-TaskName
. O

All O
these O
approaches O
perform O
reasoning B-TaskName
on O
synthetic O
domains O
, O
while O
our O
model O
is O
applied O
to O
natural O
language O
. O

Recent O
works O
( O
Gupta O
& O
Lewis O
, O
2018 O
; O
Mao O
et O
al. O
, O
2019 O
) O
also O
use O
domain O
- O
knowledge O
to O
alleviate O
issues O
in O
learning O
by O
using O
curriculum O
learning O
to O
train O
the O
executor O
first O
on O
simple O
questions O
for O
which O
parsing O
is O
not O
an O
issue O
. O

Gupta O
& O
Lewis O
( O
2018 O
) O
propose O
a O
NMN B-MethodName
model O
for O
QA B-TaskName
against O
knowledge O
graphs O
and O
learn O
execution O
for O
semantic O
operators O
from O
QA O
supervision O
alone O
. O

N2NMNs B-MethodName
( O
Hu O
et O
al. O
, O
2017 O
) O
simultaneously O
learn O
to O
parse O
and O
execute O
but O
require O
pre O
- O
training O
the O
parser O
. O

For O
combining O
learned O
execution O
modules O
with O
semantic O
parsing O
, O
many O
variations O
to O
NMNs B-MethodName
have O
been O
proposed O
; O
NMN B-MethodName
( O
Andreas O
et O
al. O
, O
2016 O
) O
use O
a O
PCFG O
parser O
to O
parse O
the O
question O
and O
only O
learn O
module O
parameters O
. O

Our O
model O
on O
the O
other O
hand O
provides O
an O
interpretable O
, O
compositional O
parse O
of O
the O
question O
and O
exposes O
its O
intermediate O
reasoning O
steps O
. O

proposed O
( O
Hu O
et O
al. O
, O
2019 O
; O
Andor O
et O
al. O
, O
2019 O
; O
Kinley O
& O
Lin O
, O
2019 O
) O
, O
but O
all O
these O
models O
essentially O
perform O
a O
multiclass O
classification O
over O
pre O
- O
defined O
programs O
. O

Recently O
, O
BERT O
- O
based O
models O
for O
DROP B-DatasetName
have O
been O
been O
8 O

These O
have O
also O
been O
extended O
for O
QA B-TaskName
using O
symbolic B-TaskName
reasoning I-TaskName
against O
semi O
- O
structured O
tables O
( O
Pasupat O
& O
Liang O
, O
2015 O
; O
Krishnamurthy O
et O
al. O
, O
2017 O
; O
Neelakantan O
et O
al. O
, O
2016 O
) O
. O

Approaches O
have O
used O
labeled O
logical O
- O
forms O
( O
Zelle O
& O
Mooney O
, O
1996 O
; O
Zettlemoyer O
& O
Collins O
, O
2005 O
) O
, O
or O
weak O
QA O
supervision O
( O
Clarke O
et O
al. O
, O
2010 O
; O
Berant O
et O
al. O
, O
2013 O
; O
Reddy O
et O
al. O
, O
2014 O
) O
to O
learn O
parsers O
to O
answer O
questions O
against O
structured O
knowledge O
bases O
. O

6 O
R O
ELATED O
W O
ORK O
Semantic O
parsing O
techniques O
have O
been O
used O
for O
a O
long O
time O
for O
compositional B-TaskName
question I-TaskName
understanding I-TaskName
. O

Such O
questions O
, O
that O
require O
nested O
counting O
, O
are O
out O
of O
scope O
of O
our O
defined O
modules O
because O
the O
model O
would O
first O
need O
to O
to O
count O
the O
passes O
caught O
by O
each O
player O
. O

is O
incorrect O
since O
the O
correct O
answer O
requires O
natural O
language O
inference O
about O
the O
order O
of O
events O
and O
not O
symbolic O
comparison O
between O
dates O
. O

- O
relocate(find O
- O
max O
- O
num(find O
) O
) O
) O
. O

3 O
. O
Who O
caught O
the O
most O
touchdown O
passes O
? O

date O
- O
compare O
- O
gt(find O
, O
find O
) O
) O

2 O
. O
Which O
happened O
last O
, O
failed O
assassination O
attempt O
on O
Lenin O
, O
or O
the O
Red O
Terror O
? O

- O
count(find O
) O
is O
incorrect O
since O
the O
correct O
answer O
requires O
a O
simple O
lookup O
from O
the O
paragraph O
. O

How O
many O
touchdown O
passes O
did O
Tom O
Brady O
throw O
in O
the O
season O
? O

Here O
we O
show O
few O
mistakes O
of O
the O
first O
type O
that O
highlight O
the O
need O
to O
parse O
the O
question O
in O
a O
context O
conditional O
manner O
: O
1 O
. O

Mistakes O
by O
our O
model O
can O
be O
classified O
into O
two O
types O
; O
incorrect O
program O
prediction O
and O
incorrect O
execution O
. O

Incorrect O
Program O
Predictions O
. O

This O
shows O
that O
by O
explicitly O
modeling O
compositionality O
, O
our O
model O
is O
able O
to O
use O
additional O
auxiliary O
supervision O
effectively O
and O
achieves O
improved O
model O
generalization O
. O

Figure O
2b O
shows O
that O
our O
model O
significantly O
outperforms O
MTMSN B-MethodName
when O
training O
using O
less O
data O
, O
especially O
using O
10 O
- O
25 O
% O
of O
the O
available O
supervision O
. O

Effect O
of O
Training O
Data O
Size O
. O

Additionally O
, O
the O
intermediate O
module O
output O
supervision O
has O
slight O
positive O
effect O
on O
the O
model O
performance O
. O

The O
model O
using O
BERT O
diverges O
while O
training O
without O
the O
auxiliary O
objective O
. O

Figure O
2a O
shows O
that O
the O
unsupervised O
auxiliary O
objective O
significantly O
improves O
model O
performance O
( O
from O
57.3 B-MetricValue
to O
73.1 B-MetricValue
F1 B-MetricName
) O
. O

Effect O
of O
Additional O
Supervision O
. O

We O
believe O
this O
is O
because O
feedback O
from O
count O
questions O
is O
weak O
, O
i.e. O
, O
the O
model O
only O
gets O
feedback O
about O
the O
count O
value O
and O
not O
what O
the O
underlying O
set O
is O
; O
and O
because O
it O
was O
challenging O
to O
define O
a O
categorical O
count O
distribution O
given O
a O
passage O
attention O
distribution O
— O
finding O
a O
better O
way O
to O
parameterize O
this O
function O
is O
an O
interesting O
problem O
for O
future O
work O
. O

Even O
after O
pre O
- O
training O
the O
count O
module O
using O
synthetic O
data O
, O
training O
it O
is O
particularly O
unstable O
. O

Our O
model O
outperforms O
MTMSN B-MethodName
on O
majority O
of O
question O
types O
but O
struggles O
with O
counting O
questions O
; O
it O
outperforms O
MTMSN B-MethodName
on O
only O
some O
of O
the O
runs O
. O

Table O
2b O
shows O
the O
performance O
for O
different O
question O
types O
as O
identified O
by O
our O
heuristic O
labeling O
. O

Performance O
by O
Question O
Type O
. O

Additionally O
, O
this O
shows O
that O
structured O
models O
still O
benefit O
when O
used O
over O
representations O
from O
large O
pretrained O
- O
LMs O
, O
such O
as O
BERT O
. O

reasoning B-TaskName
over O
natural O
language O
text O
. O

This O
shows O
the O
efficacy O
of O
our O
proposed O
model O
in O
understanding O
complex O
compositional O
questions O
and O
performing O
multi O
- O
step O
2 O
Our O
code O
is O
available O
at O
https://github.com/nitishgupta/nmn-drop O
7 O

Using O
BERT O
representations O
, O
our O
model O
’s O
performance O
increases O
to O
77.4 B-MetricValue
F1 B-MetricName
and O
outperforms O
SoTA O
models O
that O
use O
BERT O
representations O
, O
such O
as O
MTMSN B-MethodName
( O
76.5 B-MetricValue
F1 B-MetricName
) O
. O

Our O
model O
achieves O
an O
F1 B-MetricName
score O
of O
73.1 B-MetricValue
( O
w/ O
GRU O
) O
and O
significantly O
outperforms O
NAQANet B-MethodName
( O
62.1 B-MetricValue
F1 B-MetricName
) O
. O

Table O
2a O
compares O
our O
model O
’s O
performance O
to O
state O
- O
of O
- O
the O
- O
art O
models O
on O
our O
full O
test O
set O
. O

Overall O
. O

All O
results O
are O
reported O
as O
an O
average O
of O
4 O
model O
runs O
. O

The O
hyperparameters O
used O
for O
our O
model O
are O
described O
in O
the O
appendix O
. O

2 O

We O
implement O
our O
model O
using O
AllenNLP O
( O
Gardner O
et O
al. O
, O
2018 O
) O
. O

5.2 O
R O
ESULTS O
We O
compare O
to O
publicly O
available O
best O
performing O
models O
: O
NAQANet B-MethodName
( O
Dua O
et O
al. O
, O
2019 O
) O
, O
NABERT+ B-MethodName
( O
Kinley O
& O
Lin O
, O
2019 O
) O
, O
TAG B-MethodName
- I-MethodName
NABERT+ I-MethodName
( O
Avia O
Efrat O
& O
Shoham O
, O
2019 O
) O
, O
and O
MTMSN B-MethodName
( O
Hu O
et O
al. O
, O
2019 O
) O
, O
all O
trained O
on O
the O
same O
data O
as O
our O
model O
. O

We O
use O
curriculum O
learning O
( O
Bengio O
et O
al. O
, O
2009 O
) O
where O
the O
model O
is O
trained O
only O
on O
heuristically O
- O
supervised O
non O
- O
count O
questions O
for O
the O
first O
5 B-HyperparameterValue
epochs B-HyperparameterName
. O

Auxiliary O
Supervision O
Out O
of O
the O
20 O
, O
000 O
training O
questions O
, O
we O
provide O
question O
program O
supervision O
for O
10 O
% O
( O
2000 O
) O
, O
and O
intermediate O
module O
output O
supervision O
for O
5 O
% O
( O
1000 O
) O
of O
training O
questions O
. O

Extract O
- O
Argument O
e.g. O
Who O
threw O
the O
longest O
touchdown O
pass O
in O
the O
first O
quarter O
? O

Count O
e.g. O
How O
many O
touchdowns O
did O
the O
Vikings O
score O
in O
the O
first O
half O
? O

Extract O
- O
Number O
e.g. O
How O
many O
yards O
was O
Kasay O
’s O
shortest O
field O
goal O
during O
the O
second O
half O
? O

Number O
- O
Compare O
e.g. O
Were O
there O
more O
of O
cultivators O
or O
main O
agricultural O
labourers O
in O
Sweden O
? O

Date O
- O
Difference O
e.g. O
How O
many O
years O
after O
his O
attempted O
assassination O
was O
James O
II O
coronated O
? O

Based O
on O
the O
manual O
analysis O
we O
classify O
these O
questions O
into O
different O
categories O
, O
which O
are O
: O
Date O
- O
Compare O
e.g. O
What O
happened O
last O
, O
commission O
being O
granted O
to O
Robert O
or O
death O
of O
his O
cousin O
? O

We O
make O
our O
subset O
and O
splits O
available O
publicly O
with O
the O
code O
. O

Though O
this O
is O
a O
subset O
of O
the O
full O
DROP B-DatasetName
dataset O
it O
is O
still O
a O
significantly O
- O
sized O
dataset O
that O
allows O
drawing O
meaningful O
conclusions O
. O

Since O
the O
DROP B-DatasetName
test O
set O
is O
hidden O
, O
this O
test O
set O
is O
extracted O
from O
the O
validation O
data O
. O

The O
dataset O
we O
construct O
contains O
20 O
, O
000 O
questions O
for O
training O
/ O
validation O
, O
and O
1800 O
questions O
for O
testing O
( O
25 O
% O
of O
DROP B-DatasetName
) O
. O

These O
n O
- O
grams O
were O
selected O
by O
performing O
manual O
analysis O
on O
a O
small O
set O
of O
questions O
. O

Our O
model O
possesses O
diverse O
but O
limited O
reasoning B-TaskName
capability O
; O
hence O
, O
we O
try O
to O
automatically O
extract O
questions O
in O
the O
scope O
of O
our O
model O
based O
on O
their O
first O
n O
- O
gram O
. O

5 O
E O
XPERIMENTS O
5.1 O
DATASET O
We O
perform O
experiments O
on O
a O
portion O
of O
the O
recently O
released O
DROP B-DatasetName
dataset O
( O
Dua O
et O
al. O
, O
2019 O
) O
, O
which O
to O
the O
best O
of O
our O
knowledge O
is O
the O
only O
dataset O
that O
requires O
the O
kind O
of O
compositional B-TaskName
and I-TaskName
symbolic I-TaskName
reasoning I-TaskName
that O
our O
model O
aims O
to O
solve O
. O

We O
follow O
the O
same O
procedure O
for O
a O
few O
other O
question O
types O
involving O
dates O
and O
numbers O
; O
see O
§ O
A.7 O
for O
details O
. O

We O
supervise O
this O
as O
a O
multi O
- O
hot O
vector O
N O
∗ O
and O
use O
an O
auxiliary O
loss O
, O
similar O
to O
question O
- O
attention O
loss O
, O
against O
the O
output O
distribution O
N O
of O
find O
- O
num O
. O

For O
questions O
like O
“ O
how O
many O
yards O
was O
the O
longest O
/ O
shortest O
touchdown O
? O
” O
, O
we O
identify O
all O
instances O
of O
the O
token O
“ O
touchdown O
” O
in O
the O
paragraph O
and O
assume O
the O
closest O
number O
to O
it O
should O
be O
an O
output O
of O
the O
find O
- O
num O
module O
. O

We O
provide O
heuristically O
- O
obtained O
noisy O
supervision O
for O
the O
output O
of O
the O
find O
- O
num O
and O
find O
- O
date O
modules O
for O
a O
subset O
of O
the O
questions O
( O
5 O
% O
) O
for O
which O
we O
also O
provide O
question O
program O
supervision O
. O

Such O
feedback O
biases O
the O
model O
in O
predicting O
incorrect O
values O
for O
intermediate O
modules O
( O
only O
the O
shortest O
goal O
instead O
of O
all O
in O
find O
- O
num O
) O
which O
in O
turn O
hurts O
model O
generalization O
. O

The O
model O
only O
gets O
feedback O
for O
how O
long O
the O
shortest O
goal O
is O
, O
but O
not O
for O
other O
goals O
. O

Consider O
the O
question O
, O
“ O
how O
many O
yards O
was O
the O
shortest O
goal O
? O
” O
. O

Intermediate O
Module O
Output O
Supervision O
. O

For O
example O
, O
for O
program O
find O
- O
num(find O
- O
max O
- O
num(find O
( O
) O
) O
) O
, O
we O
provide O
supervision O
for O
question O
tokens O
to O
attend O
to O
when O
predicting O
the O
find O
module O
. O

Published O
as O
a O
conference O
paper O
at O
ICLR O
2020 O
of O
the O
questions O
; O
see O
§ O
A.6 O
) O
. O

In O
order O
to O
bootstrap O
the O
parser O
, O
we O
analyze O
some O
questions O
manually O
and O
come O
up O
with O
a O
few O
heuristic O
patterns O
to O
get O
program O
and O
corresponding O
question O
attention O
supervision O
( O
for O
modules O
that O
require O
it O
) O
for O
a O
subset O
of O
the O
training O
data O
( O
10 O
% O
6 O

For O
DROP B-DatasetName
, O
we O
have O
no O
such O
external O
supervision O
. O

For O
example O
, O
even O
though O
the O
questions O
in O
CLEVR B-DatasetName
are O
programmatically O
generated O
, O
Hu O
et O
al. O
( O
2017 O
) O
needed O
to O
pre O
- O
train O
their O
parser O
using O
external O
supervision O
for O
all O
questions O
. O

Learning O
to O
parse O
questions O
in O
a O
noisy O
feedback O
environment O
is O
very O
challenging O
. O

AND O
I O
NTERMEDIATE O
M O
ODULE O
O O
UTPUT O
S O
UPERVISION O
Question O
Parse O
Supervision O
. O

4.2 O
Q O
UESTION O
PARSE O

Hloss O
+ O
Hloss O
+ O
Hloss O
. O

n O
d O
r O
The O
final O
auxiliary O
loss O
is O
Hloss O
= O

We O
compute O
a O
similar O
loss O
for O
the O
date O
- O
attention O
map O
Adate O
( O
Hloss O
) O
and O
the O
relocate O
- O
map O
R O
( O
Hloss O
) O
. O

r O

Anum O
ij O
 O
j=0 O
d O

 O
NX O
1nj O
∈[i±W O
] O

The O
objective O
for O
the O
find O
- O
num O
is O
n O
Hloss O
= O
− O
m O
X O
i=1 O
log O
tokens O

For O
any O
token O
, O
the O
objective O
increases O
the O
sum O
of O
the O
attention O
probabilities O
for O
output O
tokens O
that O
appear O
within O
a O
window O
W O
= O
10 O
, O
letting O
the O
model O
distribute O
the O
mass O
within O
that O
window O
however O
it O
likes O
. O

We O
introduce O
an O
auxiliary O
objective O
to O
induce O
the O
idea O
that O
the O
arguments O
of O
a O
mention O
should O
appear O
near O
it O
. O

In O
our O
initial O
experiments O
we O
found O
that O
these O
modules O
would O
often O
spuriously O
predict O
a O
high O
attention O
score O
for O
output O
tokens O
that O
appear O
far O
away O
from O
their O
corresponding O
inputs O
. O

4.1 O
U O
NSUPERVISED O
AUXILIARY O
L O
OSS O
FOR O
IE O
The O
find O
- O
num O
, O
find O
- O
date O
, O
and O
relocate O
modules O
perform O
information O
extraction O
by O
finding O
relevant O
arguments O
for O
entities O
and O
events O
mentioned O
in O
the O
context O
. O

To O
overcome O
issues O
in O
learning O
, O
( O
a O
) O
we O
introduce O
an O
unsupervised O
auxiliary O
loss O
to O
provide O
an O
inductive O
bias O
to O
the O
execution O
of O
find O
- O
num O
, O
find O
- O
date O
, O
and O
relocate O
modules O
( O
§ O
4.1 O
) O
; O
and O
( O
b O
) O
provide O
heuristically O
- O
obtained O
supervision O
for O
question O
program O
and O
intermediate O
module O
output O
( O
§ O
4.2 O
) O
for O
a O
subset O
of O
questions O
( O
5–10 O
% O
) O
. O

4 O
AUXILIARY O
S O
UPERVISION O
As O
mentioned O
in O
§ O
2.2 O
, O
jointly O
learning O
the O
parameters O
of O
the O
parser O
and O
the O
modules O
using O
only O
endtask O
QA O
supervision O
is O
extremely O
challenging O
. O

This O
module O
is O
implemented O
similar O
to O
the O
count O
module O
( O
see O
§ O
A.5 O
) O
. O

The O
module O
outputs O
two O
probability O
distributions O
, O
Ps O
and O
Pe O
∈ O
Rm O
, O
denoting O
the O
probability O
of O
a O
token O
being O
the O
start O
and O
end O
of O
a O
span O
, O
respectively O
. O

This O
module O
is O
used O
to O
convert O
a O
paragraph O
attention O
into O
a O
contiguous O
answer O
span O
and O
only O
appears O
as O
the O
outermost O
module O
in O
a O
program O
. O

→ O
S O

span(P O
) O

By O
picking O
the O
set O
size O
n B-HyperparameterName
= O
3 B-HyperparameterValue
as O
a O
hyperparameter O
, O
we O
can O
analytically O
( O
and O
differentiably O
) O
convert O
the O
expected O
distribution O
over O
number O
tokens O
, O
T O
, O
into O
a O
distribution O
over O
the O
maximum O
value O
T O
max O
. O

The O
probability O
that O
Nj O
is O
the O
largest O
number O
in O
this O
set O
is O
p(x O
≤ O
Nj O
) O
n O
− O
p(x O
≤ O
Nj−1 O
) O
n O
i.e. O
all O
numbers O
in O
S O
are O
less O
than O
or O
equal O
to O
Nj O
, O
and O
at O
least O
one O
number O
is O
Nj O
. O

Say O
we O
sample O
a O
set O
S O
( O
size O
n O
) O
of O
numbers O
from O
this O
distribution O
. O

Computing O
T O
max O
: O
Consider O
a O
distribution O
over O
numbers O
N O
, O
sorted O
in O
an O
increasing O
order O
. O

Pi O
· O
Anum O
ij O
. O

To O
compute O
the O
new O
attention O
value O
for O
token O
i O
, O
we O
re O
- O
weight O
this O
Tjmax O
/ O
Tj O
and O
marginalize O
across O
the O
number O
tokens O
to O
get O
the O
new O
token O
contribution O
based O
on O
the O
P O
ratio O
max O
T O
attention O
value O
: O
P̄i O
= O
j O
j O
/Tj O
· O

The O
contribution O
from O
the O
i O
- O
th O
paragraph O
token O
to O
the O
j O
- O
th O
number O
token O
, O
Tj O
, O
was O
Pi O
· O
Anum O
ij O
. O

We O
then O
re O
- O
distribute O
this O
distribution O
back O
to O
the O
original O
passage O
tokens O
associated O
with O
those O
numbers O
. O

We O
first O
compute O
an O
expected O
number O
token O
distribution O
T O
using O
find O
- O
num O
, O
then O
use O
this O
to O
compute O
the O
expected O
probability O
that O
each O
number O
token O
is O
the O
one O
with O
the O
maximum O
value O
, O
T O
max O
∈ O
RNtokens O
( O
explained O
below O
) O
. O

Given O
a O
passage O
attention O
attending O
to O
multiple O
spans O
, O
this O
module O
outputs O
an O
attention O
for O
the O
span O
associated O
with O
the O
largest O
( O
or O
smallest O
) O
number O
. O

find O
- O
max O
- O
num(P O
) O
→ O
P O
, O
find O
- O
min O
- O
num(P O
) O
→ O
P O

i O
, O
j O
1(di O
−dj O
= O
td O
) O
D1i O
D2j O
. O

The O
probability O
of O
the O
difference O
being O
td O
is O
computed O
by O
marginalizing O
over O
the O
joint O
P O
probability O
for O
the O
dates O
that O
yield O
this O
value O
, O
as O
p(td O
) O
= O

Published O
as O
a O
conference O
paper O
at O
ICLR O
2020 O
D1 O
and O
D2 O
. O

The O
module O
internally O
calls O
the O
find O
- O
date O
module O
to O
get O
a O
date O
distribution O
for O
the O
two O
paragraph O
attentions O
, O
5 O

time O
- O
diff(P1 O
, O
P2 O
) O
→ O
TD O
The O
module O
outputs O
the O
difference O
between O
the O
dates O
associated O
with O
the O
two O
paragraph O
attentions O
as O
a O
distribution O
over O
all O
possible O
difference O
values O
. O

We O
similarly O
include O
the O
comparison O
modules O
compare O
- O
num O
- O
gt O
, O
compare O
- O
date O
- O
lt O
, O
and O
compare O
- O
date O
- O
gt O
, O
defined O
in O
an O
essentially O
identical O
manner O
, O
but O
for O
greater O
- O
than O
and O
for O
dates O
. O

When O
the O
the O
predicted O
number O
distributions O
are O
peaky O
, O
p(N1 O
< O
N2 O
) O
or O
p(N2 O
< O
N1 O
) O
is O
close O
to O
1 O
, O
and O
the O
output O
is O
either O
P1 O
or O
P2 O
. O

P1 O
+ O
p(N2 O
< O
N1 O
) O
∗ O
P2 O
. O

The O
final O
output O
is O
, O
Pout O
= O
p(N1 O
< O
N2 O
) O
∗ O

i O
2 O
2 O
j O
i O
1 O
j O

1N O
i O
< O
N O
j O
N2i O
N1j O
1 O

N1i O
N2j O
p(N2 O
< O
N1 O
) O
= O

1N O
i O
< O
N O
j O

The O
boolean O
values O
are O
computed O
by O
marginalizing O
the O
relevant O
joint O
probabilities O
: O
XX O
XX O
p(N1 O
< O
N2 O
) O
= O

It O
then O
computes O
two O
soft O
boolean O
values O
, O
p(N1 O
< O
N2 O
) O
and O
p(N2 O
< O
N1 O
) O
, O
and O
outputs O
a O
weighted O
sum O
of O
the O
input O
paragraph O
attentions O
. O

This O
module O
internally O
calls O
the O
find O
- O
num O
module O
to O
get O
a O
number O
distribution O
for O
each O
of O
the O
input O
paragraph O
attentions O
, O
N1 O
and O
N2 O
. O

For O
example O
, O
to O
find O
the O
city O
with O
fewer O
people O
, O
cityA O
or O
cityB O
, O
the O
module O
would O
output O
a O
linear O
combination O
of O
the O
two O
input O
attentions O
weighted O
by O
which O
city O
was O
associated O
with O
a O
lower O
number O
. O

This O
module O
performs O
a O
soft O
less O
- O
than O
operation O
between O
two O
passage O
distributions O
. O

→ O
P O

compare O
- O
num O
- O
lt(P1 O
, O
P2 O
) O

Pretraining O
this O
module O
by O
generating O
synthetic O
data O
of O
attention O
and O
count O
values O
helps O
( O
see O
§ O
A.4 O
) O
. O

[ O
0 O
, O
9 O
] O
. O

∀c O
∈ O

We O
hypothesize O
that O
the O
output O
count O
value O
is O
normally O
distributed O
with O
cv O
as O
mean O
, O
and O
a O
constant O
variance O
v O
= O
0.5 O
, O
and O
compute O
a O
categorical O
distribution O
2 O
over O
the O
supported O
count O
values O
, O
as O
p(c O
) O
∝ O
exp(−(c−cv O
) O
/2v2 O
) O

R. O

∈ O

These O
scores O
are O
summed O
to O
compute O
a O
count O
value O
, O
cv O
= O
σ O
( O
F O
F O
( O
countGRU(Pscaled O
) O
) O
) O

A O
single O
- O
layer O
feed O
- O
forward O
network O
maps O
this O
representation O
to O
a O
soft O
0/1 O
score O
to O
indicate O
P O
the O
presence O
of O
a O
span O
surrounding O
it O
. O

A O
bidirectional O
- O
GRU O
then O
represents O
each O
token O
attention O
as O
a O
hidden O
vector O
ht O
. O

The O
module O
first O
scales O
the O
attention O
using O
the O
values O
[ O
1 O
, O
2 O
, O
5 O
, O
10 O
] O
to O
convert O
it O
into O
a O
matrix O
Pscaled O
∈ O
Rm×4 O
. O

For O
example O
, O
if O
an O
attention O
vector O
is O
[ O
0 O
, O
0 O
, O
0.3 O
, O
0.3 O
, O
0 O
, O
0.4 O
] O
, O
the O
count O
module O
should O
produce O
an O
output O
of O
2 O
. O

The O
idea O
is O
to O
learn O
a O
module O
that O
detects O
contiguous O
spans O
of O
attention O
values O
and O
counts O
each O
as O
one O
. O

This O
module O
is O
used O
to O
count O
the O
number O
of O
attended O
paragraph O
spans O
. O

count(P O
) O
→ O
C O

The O
corresponding O
learnable O
parameter O
matrix O
is O
Wdate O
∈ O
Rd×d O
. O

find O
- O
date(P O
) O
→ O
D O
follows O
the O
same O
process O
as O
above O
to O
compute O
a O
distribution O
over O
dates O
for O
the O
input O
paragraph O
attention O
. O

For O
example O
, O
if O
the O
values O
of O
the O
number O
- O
tokens O
are O
[ O
2 O
, O
2 O
, O
3 O
, O
4 O
] O
and O
T O
= O
[ O
0.1 O
, O
0.4 O
, O
0.3 O
, O
0.2 O
] O
, O
the O
output O
will O
be O
a O
distribution O
over O
{ O
2 O
, O
3 O
, O
4 O
} O
with O
N O
= O
[ O
0.5 O
, O
0.3 O
, O
0.2 O
] O
. O

We O
compute O
an O
expected O
distribution O
over O
the O
number O
tokens O
num O
T O
= O
P O
· O
A O
and O
aggregate O
the O
probabilities O
for O
number O
- O
tokens O
with O
the O
same O
value O
to O
i O
i O
: O
i O
compute O
the O
output O
distribution O
N O
. O

i O
: O
) O
. O

AP O
i O
: O
= O
softmax(S O

Wnum O
Pnj O
: O
, O
where O
nj O
is O
the O
index O
of O
the O
j O
- O
th O
number O
token O
and O
Wnum O
∈ O
Rd×d O
is O
a O
learnable O
paramenum O
num O
ter O
. O

Rm×Ntokens O
as O
, O
Snum O
i O
, O
j O
= O
PTi O
: O

We O
first O
compute O
a O
token O
- O
to O
- O
number O
similarity O
matrix O
Snum O
∈ O

Rm×Ntokens O
whose O
i O
- O
th O
row O
is O
probability O
distribution O
over O
number O
- O
containing O
tokens O
for O
the O
i O
- O
th O
paragraph O
token O
. O

We O
use O
a O
paragraph O
token O
- O
to O
- O
number O
- O
token O
attention O
map O
Anum O
∈ O

This O
module O
finds O
a O
number O
distribution O
associated O
with O
the O
input O
paragraph O
attention O
. O

i O
Pi O
· O
Ri O
: O
find O
- O
num(P O
) O
→ O
N O

The O
output O
P O
attention O
is O
a O
weighted O
sum O
of O
the O
rows O
R O
weighted O
by O
the O
input O
paragraph O
attention O
, O
Prelocated O
= O

Each O
row O
of O
R O
is O
also O
normalized O
using O
the O
softmax O
operation O
. O

Qi O
: O
∈ O
Rd O
, O
and O
wrelocate O
∈ O
R3d O
is O
a O
learnable O
parameter O
vector O
. O

Pj O
: O
] O
, O
where O
q O
= O
i O
Qi O
· O

Pj O
: O
; O
( O
q O
+ O
Pi O
: O
) O
◦ O

wrelocate O
T O
[ O
( O
q O
+ O
Pi O
: O
) O
; O

We O
first O
compute O
a O
paragraph O
- O
to O
- O
paragraph O
attention O
matrix O
R O
∈PRm×m O
based O
on O
the O
question O
, O
as O
Rij O
= O

This O
module O
re O
- O
attends O
to O
the O
paragraph O
based O
on O
the O
question O
and O
is O
used O
to O
find O
the O
arguments O
for O
paragraph O
spans O
( O
e.g. O
, O
shifting O
the O
attention O
from O
“ O
field O
goals O
” O
to O
“ O
who O
kicked O
” O
them O
) O
. O

relocate(Q O
, O
P O
) O
→ O
P O

The O
output O
is O
a O
normalized O
masked O
input O
paragraph O
attention O
, O
Pfiltered O
= O
normalize(M O
◦ O
P O
) O
. O

Qi O
: O
∈ O
R O
, O
is O
a O
weighted O
sum O
of O
question O
- O
token O
embeddings O
, O
wfilter O
∈ O
R O
is O
a O
learnable O
parameter O
vector O
, O
and O
σ O
is O
the O
sigmoid O
non O
- O
linearity O
function O
. O

Here O
q O
= O
i O
Qi O
· O

P O
score O
for O
thed O
j O
- O
th O
paragraph O
token O
computed O
as O
Mj O
= O
σ(wfilter O
[ O
q O
; O
PTj O
: O
; O
q O
◦ O
3d O

We O
compute O
a O
locally O
- O
normalized O
paragraph O
- O
token O
mask O
M O
∈ O
Rm O
where O
Mj O
T O
is O
the O
masking O
Pj O
: O
] O
) O
. O

Published O
as O
a O
conference O
paper O
at O
ICLR O
2020 O
quarter O
” O
in O
Fig O
. O
1 O
) O
. O

This O
module O
masks O
the O
input O
paragraph O
attention O
conditioned O
on O
the O
question O
, O
selecting O
a O
subset O
of O
the O
attended O
paragraph O
( O
e.g. O
, O
selecting O
fields O
goals O
“ O
in O
the O
second O
1 O
We O
extract O
numbers O
and O
dates O
as O
a O
pre O
- O
processing O
step O
explained O
in O
the O
Appendix O
( O
§ O
A.3 O
) O
4 O

filter(Q O
, O
P O
) O
→ O
P O

Sij O
= O
wf O
T O
[ O
Qi O
: O
; O
Pj O
: O
; O
Qi O
: O
◦ O
Pj O
: O
] O
, O
where O
wf O
∈ O
R3d O
is O
a O
learnable O
parameter O
vector O
of O
this O
module O
, O
[ O
; O
] O
denotes O
the O
concatenation O
operation O
, O
and O
◦ O
is O
elementwise O
multiplication O
. O

Here O
Sij O
is O
the O
similarity O
between O
the O
contextual O
embeddings O
of O
the O
i O
- O
th O
question O
token O
and O
the O
j O
- O
th O
paragraph O
token O
computed O
as O
, O

A O
is O
computed O
by O
normalizing O
( O
using O
softmax O
) O
the O
rows O
of O
a O
question O
- O
to O
- O
paragraph O
similarity O
matrix O
S O
∈ O
Rn×m O
. O

Ai O
: O
∈ O
Rm O
. O

The O
output O
is O
an O
expected O
paragraph O
attention O
; O
a O
weighted O
- O
sum O
of O
the O
rows O
of O
A O
, O
weighed O
by O
the O
P O
input O
question O
attention O
, O
P O
= O
i O
Qi O
· O

We O
use O
a O
question O
- O
to O
- O
paragraph O
attention O
matrix O
A O
∈ O
Rn×m O
whose O
i O
- O
th O
row O
is O
the O
distribution O
of O
similarity O
over O
the O
paragraph O
tokens O
for O
the O
i O
- O
th O
question O
token O
. O

This O
module O
is O
used O
to O
ground O
attended O
question O
tokens O
to O
similar O
tokens O
in O
the O
paragraph O
( O
e.g. O
, O
“ O
field O
goal O
” O
in O
Figure O
1 O
) O
. O

→ O
P O

find(Q O
) O

The O
question O
attention O
computed O
by O
the O
decoder O
during O
the O
timestep O
the O
module O
was O
produced O
is O
also O
available O
to O
the O
module O
as O
a O
side O
argument O
, O
as O
described O
in O
§ O
2.1 O
. O

A B-TaskName
NSWERING I-TaskName
The O
question O
and O
paragraph O
contextualized O
embeddings O
( O
Q O
and O
P O
) O
are O
available O
as O
global O
variables O
to O
all O
modules O
in O
the O
program O
. O

3.2 O
N O
EURAL O
M O
ODULES O
FOR O
Q B-TaskName
UESTION I-TaskName

• O
Span O
( O
S O
): O
span O
- O
type O
answers O
as O
two O
probability O
values O
( O
start O
/ O
end O
) O
for O
each O
paragraph O
token O
. O

In O
this O
work O
, O
we O
consider O
differences O
in O
terms O
of O
years O
. O

• O
Time O
Delta O
( O
TD O
): O
a O
value O
amongst O
all O
possible O
unique O
differences O
between O
dates O
in O
the O
paragraph O
. O

1 O
• O
Count O
Number O
( O
C O
): O
count O
value O
as O
a O
distribution O
over O
the O
supported O
count O
values O
( O
0 O
− O
9 O
) O
. O

• O
Number O
( O
N O
) O
and O
Date O
( O
D O
): O
soft O
subset O
of O
unique O
numbers O
and O
dates O
from O
the O
passage O
. O

• O
Question O
( O
Q O
) O
and O
Paragraph O
( O
P O
) O
attentions O
: O
soft O
subsets O
of O
relevant O
tokens O
in O
the O
text O
. O

Each O
data O
type O
represents O
its O
underlying O
value O
as O
a O
normalized O
distribution O
over O
the O
relevant O
support O
. O

The O
modules O
operate O
over O
the O
following O
data O
types O
. O

3.1 O
DATA O
T O
YPES O

Table O
1 O
gives O
an O
overview O
of O
representative O
modules O
and O
§ O
3.2 O
describes O
them O
in O
detail O
. O

One O
of O
the O
main O
contributions O
of O
our O
work O
is O
introducing O
differentiable O
modules O
that O
perform O
reasoning B-TaskName
over O
text O
and O
symbols O
in O
a O
probabilistic O
manner O
. O

Since O
the O
module O
parameters O
will O
be O
learned O
jointly O
with O
the O
rest O
of O
the O
model O
, O
we O
would O
like O
the O
modules O
to O
maintain O
uncertainties O
about O
their O
decisions O
and O
propagate O
them O
through O
the O
decision O
making O
layers O
via O
end O
- O
to O
- O
end O
differentiability O
. O

We O
identify O
a O
set O
of O
tasks O
that O
need O
to O
be O
performed O
to O
support O
diverse O
enough O
reasoning B-TaskName
capabilities O
over O
text O
, O
numbers O
, O
and O
dates O
, O
and O
define O
modules O
accordingly O
. O

Published O
as O
a O
conference O
paper O
at O
ICLR O
2020 O
3 O
M O
ODULES O
FOR O
R O
EASONING O
OVER O
T O
EXT O
Modules O
are O
designed O
to O
perform O
basic O
independent B-TaskName
reasoning I-TaskName
tasks O
and O
form O
the O
basis O
of O
the O
compositional B-TaskName
reasoning I-TaskName
that O
the O
model O
is O
capable O
of O
. O

3 O

On O
the O
next O
iteration O
, O
incorrect O
program O
execution O
would O
provide O
the O
wrong O
feedback O
to O
the O
question O
parser O
and O
lead O
to O
its O
incorrect O
training O
, O
and O
learning O
fails O
. O

E.g. O
, O
if O
the O
parser O
predicts O
the O
program O
relocate(find O
( O
) O
) O
for O
the O
question O
in O
Fig O
. O
1 O
, O
then O
the O
associated O
modules O
would O
be O
incorrectly O
trained O
to O
predict O
the O
gold O
answer O
. O

Additionally O
, O
joint O
learning O
is O
challenging O
as O
prediction O
errors O
from O
one O
component O
lead O
to O
incorrect O
training O
of O
the O
other O
. O

Jointly O
training O
the O
parser O
and O
executor O
increases O
the O
latent O
choices O
available O
to O
the O
model O
by O
many O
folds O
while O
the O
only O
supervision O
available O
is O
the O
gold O
answer O
. O

Joint O
Learning O
. O

Differentiable O
modules O
that O
propagate O
uncertainties O
in O
intermediate O
decisions O
help O
here O
, O
such O
as O
attention O
on O
pixels O
in O
CLEVR B-DatasetName
, O
but O
do O
not O
fully O
solve O
the O
learning O
challenges O
. O

The O
absence O
of O
any O
direct O
feedback O
to O
the O
intermediate O
modules O
complicates O
learning O
since O
the O
errors O
of O
one O
module O
would O
be O
passed O
on O
to O
the O
next O
. O

The O
output O
of O
each O
intermediate O
module O
in O
the O
program O
is O
a O
latent O
decision O
by O
the O
model O
since O
the O
only O
feedback O
available O
is O
for O
the O
final O
output O
of O
the O
program O
. O

Program O
Executor O
. O

Additionally O
, O
many O
incorrect O
programs O
can O
yield O
the O
same O
correct O
answer O
thus O
training O
the O
question O
parser O
to O
highly O
score O
incorrect O
interpretations O
. O

This O
is O
challenging O
since O
the O
questions O
are O
not O
generated O
from O
a O
small O
fixed O
grammar O
( O
unlike O
CLEVR B-DatasetName
) O
, O
involve O
lexical O
variability O
, O
and O
have O
no O
program O
supervision O
. O

Our O
model O
needs O
to O
parse O
free O
- O
form O
real O
- O
world O
questions O
into O
the O
correct O
program O
structure O
and O
identify O
its O
arguments O
( O
e.g. O
” O
who O
kicked O
” O
, O
” O
field O
goal O
” O
, O
etc O
. O
) O
. O

Question O
Parser O
. O

Each O
of O
them O
is O
challenging O
to O
learn O
in O
its O
own O
right O
and O
joint O
training O
further O
exacerbates O
the O
situation O
. O

As O
mentioned O
above O
, O
the O
question O
parser O
and O
the O
program O
executor O
both O
contain O
learnable O
parameters O
. O

2.2 O
L O
EARNING O
C O
HALLENGES O
IN O
NMN B-MethodName
FOR O
T O
EXT O

Since O
the O
space O
of O
all O
programs O
is O
intractable O
, O
we O
run O
beam O
search O
to O
enumerate O
top O
- O
K O
programs O
and O
maximize O
the O
approximate O
marginal O
- O
likelihood O
. O

Combined O
with O
the O
likelihood O
of O
the O
program O
under O
the O
question O
- O
parser O
model O
p(z|q O
) O
, O
Pwe O
can O
maximize O
the O
marginal O
likelihood O
of O
the O
answer O
by O
enumerating O
all O
possible O
programs O
; O
J O
= O
z O
p(y O
∗ O
|z)p(z|q O
) O
. O

We O
define O
our O
model O
probabilistically O
, O
i.e. O
, O
for O
any O
given O
program O
z O
, O
we O
can O
compute O
the O
likelihood O
of O
the O
gold O
- O
answer O
p(y O
∗ O
|z O
) O
. O

Learning O
. O

See O
§ O
A.2 O
for O
details O
. O

The O
output O
of O
the O
decoder O
is O
a O
linearized O
abstract O
syntax O
tree O
( O
in O
an O
in O
- O
order O
traversal O
) O
. O

For O
example O
, O
if O
a O
module O
f1 O
inputs O
a O
number O
, O
and O
f2 O
outputs O
a O
date O
, O
then O
f1 O
( O
f2 O
) O
is O
invalid O
and O
would O
not O
be O
explored O
while O
decoding O
. O

For O
example O
, O
if O
a O
module O
f1 O
inputs O
a O
number O
, O
and O
f2 O
outputs O
a O
date O
, O
then O
f1 O
( O
f2 O
) O
is O
invalid O
and O
would O
not O
be O
explored O
while O
decoding O
. O

This O
ensures O
that O
the O
decoder O
always O
produces O
well O
- O
typed O
programs O
. O

In O
our O
model O
, O
the O
data O
types O
of O
the O
inputs O
and O
output O
of O
modules O
automatically O
induce O
a O
typeconstrained O
grammar O
which O
lends O
itself O
to O
top O
- O
down O
grammar O
- O
constrained O
decoding O
as O
performed O
by O
Krishnamurthy O
et O
al. O
( O
2017 O
) O
. O

This O
lets O
the O
modules O
have O
access O
to O
question O
information O
without O
making O
hard O
decisions O
about O
which O
question O
words O
to O
put O
into O
the O
program O
. O

Similar O
to O
N2NMN B-MethodName
( O
Hu O
et O
al. O
, O
2017 O
) O
, O
at O
each O
timestep O
of O
decoding O
, O
the O
attention O
that O
the O
parser O
puts O
on O
the O
question O
is O
available O
as O
a O
side O
argument O
to O
the O
module O
produced O
at O
that O
timestep O
during O
execution O
. O

We O
use O
an O
encoder O
- O
decoder O
model O
with O
attention O
to O
map O
the O
question O
into O
an O
executable O
program O
. O

Question O
Parser O
. O

Appendix O
§ O
A.1 O
contains O
details O
about O
how O
these O
contextual O
embeddings O
are O
produced O
. O

Here O
n O
and O
m O
are O
the O
number O
of O
tokens O
in O
the O
question O
and O
the O
paragraph O
, O
respectively O
. O

These O
are O
outputs O
of O
either O
the O
same O
bidirectional O
- O
GRU O
or O
a O
pre O
- O
trained O
BERT O
( O
Devlin O
et O
al. O
, O
2019 O
) O
model O
. O

Our O
model O
represents O
the O
question O
q O
as O
Q O
∈ O
Rn×d O
and O
the O
context O
paragraph O
p O
as O
P O
∈ O
Rm×d O
using O
contextualized O
token O
embeddings O
. O

Contextual O
Token O
Representations O
. O

We O
describe O
these O
modules O
and O
the O
data O
types O
in O
§ O
3 O
. O

To O
perform O
natural B-TaskName
language I-TaskName
and I-TaskName
symbolic I-TaskName
reasoning I-TaskName
over O
different O
types O
of O
information O
, O
such O
as O
text O
, O
numbers O
, O
and O
dates O
, O
we O
define O
a O
diverse O
set O
of O
differentiable O
modules O
to O
operate O
over O
these O
different O
data O
types O
. O

Published O
as O
a O
conference O
paper O
at O
ICLR O
2020 O
2.1 O
C O
OMPONENTS O
OF O
A O
NMN O
FOR O
T O
EXT O
Modules O
. O

2 O

For O
example O
, O
the O
find O
module O
should O
ground O
the O
question O
span O
“ O
field O
goal O
” O
to O
its O
various O
occurrences O
in O
the O
paragraph O
; O
the O
module O
find O
- O
max O
- O
num O
should O
output O
the O
span O
amongst O
its O
input O
that O
is O
associated O
with O
the O
largest O
length O
; O
and O
finally O
, O
the O
relocate O
module O
should O
find O
“ O
who O
kicked O
” O
the O
field O
goal O
corresponding O
to O
its O
input O
span O
. O

These O
programs O
capture O
the O
abstract O
compositional O
reasoning O
structure O
required O
to O
answer O
the O
question O
correctly O
and O
are O
composed O
of O
learnable O
modules O
designed O
to O
solve O
sufficiently O
independent O
reasoning B-TaskName
tasks O
. O

A O
NMN B-MethodName
would O
parse O
such O
a O
question O
into O
an O
executable O
program O
, O
such O
as O
relocate(find O
- O
max O
- O
num(filter(find O
( O
) O
) O
) O
) O
, O
whose O
execution O
against O
the O
given O
paragraph O
yields O
the O
correct O
answer O
. O

Neural B-MethodName
module I-MethodName
networks I-MethodName
( O
NMN B-MethodName
) O
capture O
this O
intuition O
naturally O
, O
which O
makes O
them O
a O
good O
fit O
to O
solve O
reasoning B-TaskName
problems I-TaskName
like O
these O
. O

We O
would O
like O
to O
develop O
machine O
reading O
models O
that O
are O
capable O
of O
understanding O
the O
context O
and O
the O
compositional O
semantics O
of O
such O
complex O
questions O
in O
order O
to O
provide O
the O
correct O
answer O
, O
ideally O
while O
also O
explaining O
the O
reasoning O
that O
led O
to O
that O
answer O
. O

Multiple O
reasoning O
steps O
are O
needed O
to O
answer O
such O
a O
question O
: O
find O
all O
instances O
of O
“ O
field O
goal O
” O
in O
the O
paragraph O
, O
select O
the O
ones O
“ O
in O
the O
second O
quarter O
” O
, O
find O
their O
lengths O
, O
compute O
the O
“ O
longest O
” O
of O
them O
, O
and O
then O
find O
“ O
who O
kicked O
” O
it O
. O

in O
Figure O
1 O
. O

2 O
N B-MethodName
EURAL I-MethodName
M I-MethodName
ODULE I-MethodName
N I-MethodName
ETWORKS I-MethodName
Consider O
the O
question O
“ O
Who O
kicked O
the O
longest O
field O
goal O
in O
the O
second O
quarter O
? O
” O

We O
conclude O
with O
a O
discussion O
of O
the O
challenges O
of O
pushing O
NMNs B-MethodName
to O
the O
entire O
DROP B-DatasetName
dataset O
, O
where O
some O
questions O
require O
reasoning O
that O
is O
hard O
to O
design O
modules O
for O
. O

We O
show O
that O
our O
model O
, O
which O
has O
interpretable O
intermediate O
outputs O
by O
design O
, O
significantly O
outperforms O
state O
- O
of O
- O
the O
- O
art O
black O
box O
models O
on O
this O
dataset O
. O

This O
is O
a O
significantly O
- O
sized O
subset O
that O
poses O
a O
wide O
variety O
of O
reasoning O
challenges O
and O
allows O
for O
controlled O
development O
and O
testing O
of O
models O
. O

We O
experiment O
on O
21,800 O
questions O
from O
the O
recently O
proposed O
DROP B-DatasetName
dataset O
( O
Dua O
et O
al. O
, O
2019 O
) O
that O
are O
heuristically O
chosen O
based O
on O
their O
first O
n O
- O
gram O
such O
that O
they O
are O
covered O
by O
our O
designed O
modules O
. O

Additionally O
, O
we O
show O
that O
providing O
heuristically O
- O
obtained O
supervision O
for O
question O
programs O
and O
outputs O
for O
intermediate O
modules O
in O
a O
program O
( O
§ O
4.2 O
) O
for O
a O
small O
subset O
of O
the O
training O
data O
( O
5–10 O
% O
) O
is O
sufficient O
for O
accurate O
learning O
. O

Specifically O
, O
we O
introduce O
an O
unsupervised O
objective O
that O
provides O
an O
inductive O
bias O
to O
perform O
accurate O
information O
extraction O
from O
the O
context O
( O
§ O
4.1 O
) O
. O

Secondly O
, O
we O
show O
that O
the O
challenges O
arising O
in O
learning O
from O
end O
- O
task O
QA O
supervision O
can O
be O
alleviated O
with O
an O
auxiliary O
loss O
over O
the O
intermediate O
latent O
decisions O
of O
the O
model O
. O

The O
modules O
we O
define O
are O
probabilistic O
and O
differentiable O
, O
which O
lets O
us O
maintain O
uncertainty O
about O
intermediate O
decisions O
and O
train O
the O
entire O
model O
via O
end O
- O
to O
- O
end O
differentiability O
. O

We O
introduce O
neural O
modules O
to O
perform O
reasoning O
over O
text O
using O
distributed O
representations O
, O
and O
perform O
symbolic B-TaskName
reasoning I-TaskName
, O
such O
as O
arithmetic B-TaskName
, O
sorting B-TaskName
, O
comparisons B-TaskName
, O
and O
counting B-TaskName
( O
§ O
3 O
) O
. O

Our O
contributions O
are O
two O
- O
fold O
: O
Firstly O
, O
we O
extend O
NMNs B-MethodName
to O
answer O
compositional O
questions O
against O
a O
paragraph O
of O
text O
as O
context O
. O

Jointly O
learning O
the O
parser O
and O
executor O
using O
only O
QA O
supervision O
is O
also O
extremely O
challenging O
( O
§ O
2.2 O
) O
. O

the O
ambiguity O
and O
variability O
of O
real O
- O
world O
text O
while O
performing O
a O
diverse O
range O
of O
reasoning B-TaskName
. O

Published O
as O
a O
conference O
paper O
at O
ICLR O
2020 O
Figure O
1 O
: O
Model O
Overview O
: O
Given O
a O
question O
, O
our O
model O
parses O
it O
into O
a O
program O
composed O
of O
neural O
modules O
. O

We O
find O
, O
however O
, O
that O
it O
is O
non O
- O
trivial O
to O
extend O
NMNs B-MethodName
for O
answering O
non O
- O
synthetic O
questions O
against O
open O
- O
domain O
text O
, O
where O
a O
model O
needs O
to O
deal O
with O
∗ O
Work O
done O
while O
at O
Allen O
Institute O
for O
AI O
. O
1 O

NMNs B-MethodName
perform O
well O
on O
synthetic B-TaskName
visual I-TaskName
question I-TaskName
answering I-TaskName
( O
VQA B-TaskName
) O
domains O
such O
as O
CLEVR B-DatasetName
( O
Johnson O
et O
al. O
, O
2017 O
) O
and O
it O
is O
appealing O
to O
apply O
them O
to O
answer O
questions O
over O
text O
due O
to O
their O
interpretable O
, O
modular O
, O
and O
inherently O
compositional O
nature O
. O

These O
modules O
are O
designed O
to O
perform O
basic O
reasoning B-TaskName
tasks O
and O
can O
be O
composed O
to O
perform O
complex O
reasoning B-TaskName
over O
unstructured O
knowledge O
. O

Neural B-MethodName
module I-MethodName
networks I-MethodName
( O
NMNs B-MethodName
; O
Andreas O
et O
al. O
, O
2016 O
) O
extend O
semantic O
parsers O
by O
making O
the O
program O
executor O
a O
learned O
function O
composed O
of O
neural O
network O
modules O
. O

Semantic O
parsing O
techniques O
, O
which O
map O
natural O
language O
utterances O
to O
executable O
programs O
, O
have O
been O
used O
for O
compositional B-TaskName
question I-TaskName
understanding I-TaskName
for O
a O
long O
time O
( O
Zelle O
& O
Mooney O
, O
1996 O
; O
Zettlemoyer O
& O
Collins O
, O
2005 O
; O
Liang O
et O
al. O
, O
2011 O
) O
, O
but O
have O
been O
limited O
to O
answering O
questions O
against O
structured O
and O
semi O
- O
structured O
knowledge O
sources O
. O

for O
the O
field O
goals O
and O
touchdowns O
) O
, O
and O
perform O
symbolic B-TaskName
reasoning I-TaskName
( O
eg O
. O
counting O
, O
sorting O
, O
etc O
. O
) O
. O

Consider O
the O
question O
in O
Figure O
1 O
; O
a O
model O
needs O
to O
understand O
the O
compositional O
reasoning O
structure O
of O
the O
questions O
, O
perform O
accurate O
information O
extraction O
from O
the O
passage O
( O
eg O
. O
extract O
lengths O
, O
kickers O
, O
etc O
. O

Answering O
complex O
compositional O
questions O
against O
text O
is O
challenging O
since O
it O
requires O
a O
comprehensive O
understanding O
of O
both O
the O
question O
semantics O
and O
the O
text O
against O
which O
the O
question O
needs O
to O
be O
answered O
. O

Recent O
models O
have O
performed O
well O
on O
certain O
QA O
datasets O
, O
sometimes O
rivaling O
humans O
( O
Zhang O
et O
al. O
, O
2019 O
) O
, O
but O
it O
has O
become O
increasingly O
clear O
that O
they O
primarily O
exploit O
surface O
level O
lexical O
cues O
( O
Jia O
& O
Liang O
, O
2017 O
; O
Feng O
et O
al. O
, O
2018 O
) O
and O
compositional B-TaskName
QA I-TaskName
still O
remains O
a O
challenge O
. O

Being O
formalism O
- O
free O
and O
close O
to O
an O
end O
- O
user O
task O
, O
QA B-TaskName
is O
increasingly O
becoming O
a O
proxy O
for O
gauging O
a O
model O
’s O
natural O
language O
understanding O
capability O
( O
He O
et O
al. O
, O
2015 O
; O
Talmor O
et O
al. O
, O
2018 O
) O
. O

I O
NTRODUCTION O

1 O

Our O
proposed O
model O
significantly O
outperforms O
state O
- O
of O
- O
the O
- O
art O
models O
on O
a O
subset O
of O
the O
DROP B-DatasetName
dataset O
that O
poses O
a O
variety O
of O
reasoning O
challenges O
that O
are O
covered O
by O
our O
modules O
. O

Additionally O
, O
we O
show O
that O
a O
limited O
amount O
of O
heuristically O
- O
obtained O
question O
program O
and O
intermediate O
module O
output O
supervision O
provides O
sufficient O
inductive O
bias O
for O
accurate O
learning O
. O

We O
extend O
NMNs B-MethodName
by O
: O
( O
a O
) O
introducing O
modules O
that O
reason O
over O
a O
paragraph O
of O
text O
, O
performing O
symbolic O
reasoning O
( O
such O
as O
arithmetic O
, O
sorting O
, O
counting O
) O
over O
numbers O
and O
dates O
in O
a O
probabilistic O
and O
differentiable O
manner O
; O
and O
( O
b O
) O
proposing O
an O
unsupervised O
auxiliary O
loss O
to O
help O
extract O
arguments O
associated O
with O
the O
events O
in O
text O
. O

However O
, O
we O
find O
that O
it O
is O
challenging O
to O
learn O
these O
models O
for O
non O
- O
synthetic O
questions O
on O
open O
- O
domain O
text O
, O
where O
a O
model O
needs O
to O
deal O
with O
the O
diversity O
of O
natural O
language O
and O
perform O
a O
broader O
range O
of O
reasoning O
. O

Neural B-MethodName
module I-MethodName
networks I-MethodName
( O
NMNs B-MethodName
) O
learn O
to O
parse O
such O
questions O
as O
executable O
programs O
composed O
of O
learnable O
modules O
, O
performing O
well O
on O
synthetic O
visual O
QA O
domains O
. O

[ O
cs O
. O
CL O
] O
15 O
Feb O
2020 O
Nitish O
Gupta1 O
, O
Kevin O
Lin2∗ O
, O
Dan O
Roth1 O
, O
Sameer O
Singh3 O
& O
Matt O
Gardner4 O
{ O
nitishg,danroth}@seas.upenn.edu O
, O
kevinlin@eecs.berkeley.edu O
, O
sameer@uci.edu O
, O
mattg@allenai.org O
1 O
University O
of O
Pennsylvania O
, O
Philadelphia O
, O
2 O
University O
of O
California O
, O
Berkeley O
, O
3 O
University O
of O
California O
, O
Irvine O
, O
4 O
Allen O
Institute O
for O
AI O
A O
BSTRACT O
Answering O
compositional O
questions O
that O
require O
multiple O
steps O
of O
reasoning O
against O
text O
is O
challenging O
, O
especially O
when O
they O
involve O
discrete O
, O
symbolic O
operations O
. O

Published O
as O
a O
conference O
paper O
at O
ICLR O
2020 O
N B-MethodName
EURAL I-MethodName
M I-MethodName
ODULE I-MethodName
N I-MethodName
ETWORKS I-MethodName
FOR I-MethodName
R I-MethodName
EASONING I-MethodName
OVER I-MethodName
T I-MethodName
EXT I-MethodName
arXiv:1912.04971v2 O

-DOCSTART- O
This O
work O
was O
partially O
supported O
by O
the O
Research O
Grants O
Council O
of O
the O
Hong O
Kong O
Special O
Administrative O
Region O
, O
China O
( O
No O
. O
CUHK O
14210717 O
, O
General O
Research O
Fund O
; O
No O
. O
CUHK O
2300174 O
, O
Collaborative O
Research O
Fund O
) O
. O

Acknowledgements O
We O
thank O
Chien O
- O
Sheng O
Wu O
, O
Jiashi O
Feng O
, O
Jiaxin O
Qi O
, O
and O
our O
anonymous O
reviewers O
for O
their O
insightful O
feedback O
on O
our O
paper O
. O

Without O
pretraining O
on O
external O
visionlanguage O
datasets O
, O
our O
model O
establishes O
new O
stateof O
- O
the O
- O
art O
performance O
in O
the O
discriminative O
setting O
and O
shows O
promising O
results O
in O
the O
generative O
setting O
on O
the O
visual B-TaskName
dialog I-TaskName
benchmarks O
. O

Besides O
, O
it O
can O
either O
rank O
or O
generate O
answers O
seamlessly O
. O

VD B-MethodName
- I-MethodName
BERT I-MethodName
is O
capable O
of O
modeling O
all O
the O
interactions O
between O
an O
image O
and O
a O
multi B-TaskName
- I-TaskName
turn I-TaskName
dialog I-TaskName
within O
a O
single O
- O
stream O
Transformer O
encoder O
and O
enables O
the O
effective O
fusion O
of O
features O
from O
both O
modalities O
via O
simple O
visually O
grounded O
training O
. O

Conclusion O
We O
have O
presented O
VD B-MethodName
- I-MethodName
BERT I-MethodName
, O
a O
unified B-MethodName
visiondialog I-MethodName
Transformer I-MethodName
model O
that O
exploits O
the O
pretrained O
BERT O
language O
models O
for O
visual B-TaskName
dialog I-TaskName
. O

We O
provide O
more O
qualitative O
examples O
in O
Figure O
6 O
and O
7 O
. O

More O
interestingly O
, O
it O
can O
even O
resolve O
visual O
pronoun O
coreference O
of O
he O
in O
the O
question O
to O
the O
man O
in O
the O
image O
( O
see O
the O
middle O
red O
box O
) O
. O

As O
shown O
in O
Figure O
5(b O
) O
, O
VD B-MethodName
- I-MethodName
BERT I-MethodName
can O
ground O
entities O
and O
discover O
some O
object O
relations O
, O
e.g. O
, O
helmet O
is O
precisely O
related O
to O
the O
man O
and O
the O
motorcycle O
in O
the O
image O
( O
see O
the O
rightmost O
red O
box O
) O
. O

In O
contrast O
to O
other O
vision O
- O
language O
tasks O
, O
visual B-TaskName
dialog I-TaskName
has O
a O
more O
complex O
multi O
- O
turn O
structure O
, O
thereby O
posing O
a O
hurdle O
for O
effective O
fusion O
. O

Next O
, O
we O
examine O
how O
our O
VD B-MethodName
- I-MethodName
BERT I-MethodName
captures O
the O
interactions O
between O
image O
and O
multi B-TaskName
- I-TaskName
turn I-TaskName
dialog I-TaskName
. O

Besides O
, O
heads O
at O
higher O
layers O
tend O
to O
have O
a O
sharper O
focus O
on O
specific O
objects O
like O
the O
man O
and O
the O
motorcycles O
in O
the O
image O
. O

We O
observe O
that O
many O
heads O
at O
different O
layers B-HyperparameterName
can O
correctly O
ground O
some O
entities O
like O
person O
and O
motorcycle O
in O
the O
image O
, O
and O
even O
reveal O
some O
high O
- O
level O
semantic O
correlations O
such O
as O
person↔motorcycle O
( O
at O
L8H2 O
) O
and O
motorcycle↔street O
( O
at O
L1H11 O
) O
. O

To O
interpret O
our O
VD B-MethodName
- I-MethodName
BERT I-MethodName
, O
we O
visualize O
the O
attention O
weights O
on O
the O
top O
10 O
detected O
objects O
from O
its O
caption O
in O
Figure O
5(a O
) O
. O

5.4 O
6 O
Attention O
Visualization O

Fine O
- O
tuning O
on O
dense O
annotations O
gives O
our O
model O
huge O
improvements O
across O
all O
the O
question O
types O
, O
especially O
for O
Others O
with O
over O
30 O
% O
absolute O
gain O
. O

Our O
model O
outperforms O
DAN B-MethodName
by O
over O
10 O
% O
in O
most O
of O
the O
question O
types O
except O
Color O
. O

For O
question O
types O
, O
we O
observe O
that O
Yes O
/ O
no O
is O
the O
major O
type O
( O
76 O
% O
) O
and O
also O
the O
easiest O
one O
, O
while O
Number O
is O
the O
most O
challenging O
and O
least O
frequent O
one O
( O
3 O
% O
) O
. O

These O
results O
validate O
that O
the O
misalignment O
of O
the O
sparse O
and O
dense O
annotations O
is O
the O
key O
reason O
for O
the O
inconsistency O
between O
NDCG B-MetricName
and O
other O
metrics O
. O

As O
the O
degree O
of O
such O
mismatch O
increases O
( O
relevance O
score O
changes O
1.0 O
→ O
0.0 O
) O
, O
both O
DAN B-MethodName
and O
our O
model O
witness O
a O
plunge O
in O
NDCG B-MetricName
( O
63.29 B-MetricValue
→ O
43.86 B-MetricValue
and O
70.25 B-MetricValue
→ O
48.07 B-MetricValue
) O
, O
while O
dense O
annotation O
fine O
- O
tuning O
significantly O
boosts O
NDCG B-MetricName
scores O
for O
all O
groups O
, O
especially O
for O
the O
most O
misaligned O
one O
( O
48.07 B-MetricValue
→ O
82.84 B-MetricValue
for O
our O
model O
) O
. O

By O
examining O
the O
distribution O
of O
the O
relevance O
scores O
, O
we O
find O
that O
only O
31 O
% O
of O
them O
are O
aligned O
well O
with O
the O
sparse O
annotations O
and O
9 O
% O
are O
totally O
misaligned O
. O

We O
choose O
DAN B-MethodName
as O
it O
achieves O
good O
NDCG B-MetricName
scores O
( O
Table O
1 O
) O
and O
provides O
the O
source O
code O
to O
reproduce O
their O
predictions O
. O

We O
then O
analyze O
the O
NDCG B-MetricName
scores O
assigned O
by O
DAN B-MethodName
( O
Kang O
et O
al. O
, O
2019 O
) O
and O
our O
VD B-MethodName
- I-MethodName
BERT I-MethodName
with O
and O
without O
dense O
annotation O
fine O
- O
tuning O
. O

We O
consider O
four O
bins O
{ O
0.0 O
, O
0.2 O
∼ O
0.4 O
, O
0.6 O
∼ O
0.8 O
, O
1.0 O
} O
for O
the O
relevance O
score O
and O
four O
question O
types O
: O
Yes O
/ O
no O
, O
Number O
, O
Color O
, O
and O
Others O
. O

For O
further O
analysis O
, O
we O
classify O
the O
2 O
, O
064 O
instances O
in O
VisDial B-DatasetName
v1.0 O
val O
set O
based O
on O
the O
ground O
- O
truth O
’s O
relevance O
score O
and O
question O
type O
( O
Table O
4 O
) O
. O

We O
observe O
that O
NDCG B-MetricName
keeps O
increasing O
with O
more O
epochs O
of O
fine O
- O
tuning O
, O
while O
other O
metrics O
such O
as O
Recall@K B-MetricName
and O
MRR B-MetricName
) O
drop O
. O

For O
this O
experiment O
, O
we O
randomly O
sample O
200 O
instances O
from O
VisDial B-DatasetName
v1.0 O
val O
as O
the O
test O
data O
and O
use O
the O
rest O
for O
fine O
- O
tuning O
with O
the O
ListNet O
ranking O
method O
. O

We O
first O
show O
how O
various O
metrics O
change O
for O
finetuning O
in O
Figure O
4 O
. O

Relevance O
Score O
and O
Question O
Type O
Analysis O
. O

In O
this O
case O
, O
fine O
- O
tuning O
instead O
makes O
our O
model O
fail O
to O
predict O
the O
correct O
answer O
despite O
the O
increase O
of O
NDCG B-MetricName
score O
. O

In O
the O
example O
at O
the O
bottom O
, O
we O
spot O
a O
mismatch O
between O
the O
sparse O
and O
dense O
annotations O
: O
the O
ground O
truth O
answer O
“ O
no O
, O
it O
’s O
empty O
” O
is O
only O
given O
a O
0.4 O
relevance O
score O
, O
while O
uncertain O
answers O
like O
“ O
i O
do O
n’t O
know O
” O
are O
considered O
to O
be O
more O
relevant O
. O

For O
the O
example O
at O
the O
top O
, O
fine O
- O
tuning O
helps O
our O
model O
to O
assign O
higher O
ranks O
to O
the O
answers O
that O
share O
similar O
semantics O
with O
the O
ground O
truth O
answer O
and O
should O
also O
be O
regarded O
as O
correct O
( O
“ O
yes O
, O
it O
is O
” O
and O
“ O
yep O
” O
vs. O
“ O
yes O
” O
) O
. O

We O
provide O
two O
examples O
to O
qualitatively O
demonstrate O
how O
dense O
annotation O
finetuning O
results O
in O
better O
NDCG B-MetricName
scores O
in O
Figure O
3 O
. O

Case O
Study O
. O

5.3 O
Fine O
- O
tuning O
on O
Dense O
Annotations O
In O
this O
section O
, O
we O
focus O
on O
the O
effect O
of O
dense O
annotation O
fine O
- O
tuning O
and O
try O
to O
analyze O
the O
reason O
of O
the O
inconsistency O
issue O
between O
NDCG B-MetricName
and O
other O
ranking O
metrics O
( O
see O
Table O
1 O
) O
in O
the O
following O
. O

The O
diverse O
set O
of O
them O
leads O
to O
the O
best O
performance O
. O

We O
observe O
that O
EPOCH B-HyperparameterName
contributes O
the O
least O
to O
the O
ensemble O
performance O
while O
RANK O
models O
are O
more O
helpful O
than O
LENGTH O
models O
. O

We O
use O
four O
predictions O
from O
each O
criterion O
and O
combine O
their O
diverse O
predictions O
( O
DIVERSE O
) O
by O
summing O
up O
their O
normalized O
ranking O
scores O
. O

We O
consider O
three O
criteria O
, O
E B-HyperparameterName
POCH I-HyperparameterName
, O
L O
ENGTH O
, O
and O
R O
ANK O
that O
respectively O
refer O
to O
predictions O
from O
different O
epochs B-HyperparameterName
of O
a O
single O
model O
, O
from O
different O
models O
trained O
with O
varying O
context O
lengths O
and O
with O
different O
ranking O
methods O
in O
Table O
3(b)-(c O
) O
. O

We O
also O
explore O
ways O
to O
achieve O
the O
best O
ensemble O
performance O
with O
various O
model O
selection O
criteria O
in O
Table O
3(d O
) O
. O

Therefore O
, O
we O
employ O
the O
ListNet O
as O
our O
ranking O
module O
. O

Among O
these O
methods O
, O
ListNet O
yields O
the O
best O
NDCG B-MetricName
and O
Mean B-MetricName
Rank I-MetricName
, O
while O
the O
approxNDCG O
achieves O
the O
best O
MRR B-MetricName
and O
Recall B-MetricName
on O
VisDial B-DatasetName
v1.0 O
test O
- O
std O
. O

In O
Table O
3(c O
) O
, O
we O
compare O
Cross O
Entropy O
( O
CE O
) O
training O
with O
a O
bunch O
of O
other O
listwise O
ranking O
optimization O
methods O
: O
ListNet O
( O
Cao O
et O
al. O
, O
2007 O
) O
, O
ListMLE O
( O
Xia O
et O
al. O
, O
2008 O
) O
, O
and O
approxNDCG O
( O
Qin O
et O
al. O
, O
2010 O
) O
. O

However O
, O
this O
version O
still O
obtains O
comparable O
results O
to O
the O
“ O
No O
history O
” O
variant O
, O
revealing O
that O
textual O
information O
dominates O
the O
VisDial B-DatasetName
task O
. O

If O
we O
remove O
the O
visual O
cues O
from O
the O
“ O
Full O
history O
” O
model O
, O
we O
see O
a O
drop O
in O
all O
metrics O
, O
especially O
, O
on O
NDCG B-MetricName
. O

dicates O
that O
dense O
relevance O
scores O
might O
be O
annotated O
with O
less O
consideration O
of O
dialog O
history O
. O

GT O
: O
ground O
truth O
. O

NDCG=91.80 B-MetricName
Figure O
3 O
: O
The O
effects O
of O
dense O
annotation O
fine O
- O
tuning O
in O
our O
VD B-MethodName
- I-MethodName
BERT I-MethodName
for O
two O
examples O
. O

not O
that O
i O
can O
see O
( O
0.6 O
) O
Base O
Model O
W/ O
Fine O
- O
tuning O
NDCG=42.19 B-MetricName

definitely O
( O
0.6 O
) O
W/ O
Fine O
- O
tuning O
NDCG=41.31 B-MetricName
NDCG=97.06 B-MetricName
1 O
. O

With O
longer O
dialog O
history O
( O
“ O
Full O
history O
” O
) O
, O
our O
model O
indeed O
yields O
better O
results O
in O
most O
of O
the O
ranking O
metrics O
, O
while O
the O
one O
without O
using O
any O
dialog O
history O
obtains O
the O
highest O
NDCG B-MetricName
score O
. O

We O
then O
examine O
the O
impact O
of O
varying O
the O
dialog O
context O
used O
for O
training O
in O
Table O
3(b O
) O
. O

We O
also O
find O
that O
the O
visually O
grounded O
MLM O
is O
crucial O
for O
transferring O
BERT O
into O
the O
multimodal O
setting O
, O
indicated O
by O
a O
large O
performance O
drop O
when O
using O
only O
NSP O
. O

Another O
possible O
reason O
might O
be O
that O
the O
VisDial B-DatasetName
data O
with O
more O
than O
one O
million O
image O
- O
dialog O
turn O
pairs O
can O
provide O
adequate O
contexts O
to O
adapt O
BERT O
for O
effective O
vision O
and O
dialog O
fusion O
. O

It O
might O
be O
due O
to O
the O
domain O
discrepancy O
between O
image O
captions O
and O
multi B-MethodName
- I-MethodName
turn I-MethodName
dialogs I-MethodName
, O
as O
well O
as O
the O
slightly O
different O
experiment O
settings O
( O
e.g. O
, O
we O
extract O
36 O
objects O
from O
image O
compared O
to O
their O
100 O
objects O
) O
. O

Surprisingly O
, O
the O
model O
initialized O
with O
the O
weights O
from O
VLP O
that O
was O
pretrained O
on O
Conceptual B-DatasetName
Captions I-DatasetName
( O
Sharma O
et O
al. O
, O
2018 O
) O
, O
does O
not O
work O
better O
than O
the O
one O
initialized O
from O
BERT O
. O

We O
observe O
that O
initializing O
the O
model O
with O
weights O
from O
BERT O
indeed O
benefits O
the O
visual O
dialog O
task O
a O
lot O
, O
increasing O
the O
NDCG B-MetricName
score O
by O
about O
7 B-MetricValue
% I-MetricValue
absolute O
over O
the O
model O
trained O
from O
scratch O
. O

5.2 O
Ablation O
Study O
We O
first O
study O
how O
different O
training O
settings O
influence O
the O
results O
in O
Table O
3(a O
) O
. O

By O
contrast O
, O
VisDial B-MethodName
- I-MethodName
BERT I-MethodName
can O
only O
support O
the O
discriminative O
setting O
. O

This O
validates O
the O
effectiveness O
of O
our O
VD B-MethodName
- I-MethodName
BERT I-MethodName
in O
both O
settings O
using O
a O
unified O
Transformer O
encoder O
. O

Our O
model O
continues O
to O
yield O
much O
better O
results O
in O
the O
discriminative O
setting O
( O
e.g. O
, O
70.04 B-MetricValue
MRR B-MetricName
compared O
to O
DVAN B-MethodName
’s O
66.67 B-MetricValue
) O
Table O
3 O
: O
Extensive O
ablation O
studies O
: O
training O
with O
( O
a O
) O
various O
settings O
and O
( O
b O
) O
contexts O
on O
v1.0 O
val O
; O
dense O
annotation O
fine O
- O
tuning O
with O
( O
c O
) O
varying O
ranking O
methods O
and O
( O
d O
) O
various O
ensemble O
strategies O
on O
v1.0 O
test O
- O
std O
. O
and O
comparable O
results O
with O
the O
state O
of O
the O
art O
in O
the O
generative O
setting O
( O
e.g. O
, O
55.95 B-MetricValue
MRR B-MetricName
score O
vs. O
DVAN B-MethodName
’s O
55.94 B-MetricValue
) O
. O

These O
models O
employ O
dual O
decoders O
for O
each O
setting O
separately O
. O

For O
comparison O
, O
we O
choose O
LF O
, O
HRE O
, O
HREA O
, O
MN O
( O
Das O
et O
al. O
, O
2017 O
) O
, O
HCIAE O
( O
Lu O
et O
al. O
, O
2017 O
) O
, O
CoAtt O
( O
Wu O
et O
al. O
, O
2018 O
) O
, O
RvA O
, O
and O
DVAN O
as O
they O
contain O
results O
in O
both O
settings O
on O
the O
v0.9 O
val O
split O
. O

We O
further O
show O
both O
discriminative O
and O
generative O
results O
on O
v0.9 O
val O
split O
in O
Table O
2 O
. O

Results O
on O
VisDial B-DatasetName
v0.9 O
val O
. O

Besides O
, O
while O
VisDial B-MethodName
- I-MethodName
BERT I-MethodName
does O
not O
observe O
improvements O
by O
ensembling O
, O
we O
endeavor O
to O
design O
an O
effective O
ensemble O
strategy O
to O
increase O
the O
NDCG B-MetricName
score O
to O
75.35 B-MetricValue
for O
VD B-MethodName
- I-MethodName
BERT I-MethodName
. O

Compare O
to O
that O
, O
our O
VD B-MethodName
- I-MethodName
BERT I-MethodName
achieves O
slightly O
better O
results O
( O
74.54 B-MetricValue
NDCG B-MetricName
) O
, O
however O
, O
note O
that O
we O
did O
not O
pretrain O
on O
large O
- O
scale O
external O
vision O
- O
language O
datasets O
like O
Conceptual B-DatasetName
Captions I-DatasetName
( O
Sharma O
et O
al. O
, O
2018 O
) O
and O
VQA B-DatasetName
( O
Antol O
et O
al. O
, O
2015 O
) O
as O
VisDialBERT B-MethodName
does O
. O

It O
only O
reports O
the O
single O
- O
model O
performance O
of O
74.47 B-MetricValue
NDCG B-MetricName
. O

VisDial B-MethodName
- I-MethodName
BERT I-MethodName
is O
a O
concurrent O
work O
to O
ours O
that O
also O
exploits O
vision O
- O
language O
pretrained O
models O
for O
visual B-TaskName
dialog I-TaskName
. O

• O
Our O
VD B-MethodName
- I-MethodName
BERT I-MethodName
is O
simpler O
and O
more O
effective O
than O
VisDial B-MethodName
- I-MethodName
BERT I-MethodName
. O

We O
provide O
a O
detailed O
analysis O
of O
this O
phenomenon O
in O
§ O
5.3 O
. O

Such O
a O
phenomenon O
has O
also O
been O
observed O
in O
other O
recent O
models O
, O
such O
as O
MReal B-MethodName
- I-MethodName
BDAI I-MethodName
, O
VisDialBERT B-MethodName
, O
Tohoku B-MethodName
- I-MethodName
CV I-MethodName
Lab O
, O
and O
P1 B-MethodName
P2 I-MethodName
, O
whose O
NDCG B-MetricName
scores O
surpass O
others O
without O
dense O
annotation O
finetuning O
by O
at O
least O
around O
10 O
% O
absolute O
points O
while O
other O
metrics O
drop O
dramatically O
. O

While O
dense O
annotation O
fine O
- O
tuning O
yields O
huge O
improvements O
on O
NDCG B-MetricName
, O
we O
also O
notice O
that O
it O
has O
a O
severe O
countereffect O
on O
other O
metrics O
, O
e.g. O
, O
reducing O
the O
MRR B-MetricName
score O
from O
65.44 B-MetricValue
to O
46.72 B-MetricValue
for O
VD B-MethodName
- I-MethodName
BERT I-MethodName
. O

• O
Inconsistency O
between O
NDCG B-MetricName
and O
other O
metrics O
. O

Moreover O
, O
our O
designed O
ensemble O
version O
yields O
new O
state O
of O
the O
art O
( O
75.35 O
NDCG O
) O
, O
outperforming O
the O
2019 O
VisDial B-DatasetName
challenge O
winner O
MRealBDAI B-MethodName
( O
74.02 B-MetricValue
NDCG B-MetricName
) O
by O
over O
1.3 O
absolute O
points O
. O

scores O
. O

This O
indicates O
that O
dense O
annotation O
finetuning O
plays O
a O
crucial O
role O
in O
boosting O
the O
NDCG B-MetricName
1 O
https://evalai.cloudcv.org/web/ O
challenges O
/ O
challenge O
- O
page/161/ O
leaderboard/483#leaderboardrank-1 O
3330 O

With O
further O
fine O
- O
tuning O
on O
dense O
annotations O
, O
the O
NDCG B-MetricName
score O
increases O
quite O
sharply O
, O
from O
59.96 B-MetricValue
to O
74.54 B-MetricValue
with O
nearly O
15 O
% O
absolute O
improvement O
, O
setting O
a O
new O
state O
of O
the O
art O
in O
the O
single O
- O
model O
setting O
. O

Our O
single O
- O
model O
VD B-MethodName
- I-MethodName
BERT I-MethodName
significantly O
outperforms O
all O
of O
its O
single O
- O
model O
counterparts O
across O
various O
metrics O
, O
even O
including O
some O
ensemble O
variants O
such O
as O
Synergistic B-MethodName
, O
DAN B-MethodName
( O
except O
R@10 B-MetricName
) O
, O
and O
ReDAN B-MethodName
( O
except O
NDCG B-MetricName
) O
. O

• O
New O
state O
of O
the O
art O
for O
both O
single O
- O
model O
and O
ensemble O
settings O
. O

We O
report O
the O
comparison O
results O
on O
VisDial B-DatasetName
v1.0 O
test O
- O
std O
split O
in O
Table O
1 O
and O
make O
the O
following O
observations O
. O

Results O
on O
VisDial B-DatasetName
v1.0 O
test O
- O
std O
. O

sults O
from O
the O
leaderboard1 O
for O
a O
more O
up O
- O
to O
- O
date O
comparison O
, O
where O
some O
can O
be O
found O
in O
the O
arXiv O
, O
such O
as O
MVAN B-MethodName
( O
Park O
et O
al. O
, O
2020 O
) O
, O
SGLNs B-MethodName
( O
Kang O
et O
al. O
, O
2020 O
) O
, O
VisDial B-MethodName
- I-MethodName
BERT I-MethodName
( O
Murahari O
et O
al. O
, O
2019 O
) O
, O
and O
Tohoku B-MethodName
- I-MethodName
CV I-MethodName
( O
Nguyen O
et O
al. O
, O
2019 O
) O
. O

The O
best O
and O
second O
- O
best O
results O
in O
each O
column O
are O
in O
bold O
and O
underlined O
respectively O
. O

The O
“ O
↑ O
” O
denotes O
higher O
value O
for O
better O
performance O
and O
“ O
↓ O
” O
is O
the O
opposite O
. O

“ O
† O
” O
denotes O
ensemble O
model O
and O
“ O
∗ O
” O
indicates O
fine O
- O
tuning O
on O
dense O
annotations O
. O

The O
results O
are O
reported O
by O
the O
test O
server O
. O

Since O
the O
2018 O
VisDial B-DatasetName
challenge O
( O
after O
the O
acquisition O
of O
dense O
annotations O
) O
, O
NDCG B-MetricName
metric O
that O
considers O
the O
relevance O
degree O
of O
each O
answer O
candidate O
, O
has O
been O
adopted O
as O
the O
main O
metric O
to O
determine O
the O
winner O
. O

For O
dense O
annotation O
fine O
- O
tuning O
in O
the O
discriminative O
setting O
, O
we O
train O
with O
the O
ListNet O
loss O
for O
5 B-HyperparameterValue
epochs B-HyperparameterName
. O

After O
that O
, O
we O
train O
for O
another O
10 B-HyperparameterValue
epochs B-HyperparameterName
with O
full O
dialog O
history O
using O
either O
NSP O
in O
the O
discriminative O
setting O
or O
MLM O
on O
the O
answer O
sequence O
in O
the O
generative O
setting O
. O

For O
instances O
where O
the O
appended O
answer O
candidate O
is O
incorrect O
, O
we O
do O
not O
conduct O
MLM O
on O
the O
answer O
sequence O
to O
reduce O
the O
noise O
introduced O
by O
the O
negative O
samples O
. O

Here O
we O
only O
utilize O
one O
previous O
dialog O
turn O
for O
training O
efficiency O
. O

We O
first O
train O
VD B-MethodName
- I-MethodName
BERT I-MethodName
for O
30 B-HyperparameterValue
epochs B-HyperparameterName
on O
a O
cluster O
of O
4 O
V100 O
GPUs O
with O
16 O
G O
memory O
using O
MLM O
and O
NSP O
losses O
( O
with O
equal O
coefficients O
) O
. O

A O
linear B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
decay I-HyperparameterName
schedule I-HyperparameterName
with O
a O
warmup O
of O
0.1 B-HyperparameterValue
is O
employed O
. O

We O
use O
Adam B-HyperparameterName
( O
Kingma O
and O
Ba O
, O
2015 O
) O
with O
an O
initial O
learning B-HyperparameterName
rate I-HyperparameterName
of O
3e B-HyperparameterValue
− I-HyperparameterValue
5 I-HyperparameterValue
and O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
32 B-HyperparameterValue
to O
train O
our O
model O
. O

We O
keep O
the O
max B-HyperparameterName
input I-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
( O
including O
36 O
visual O
objects O
) O
to O
250 B-HyperparameterValue
. O

We O
use O
BERTBASE O
as O
the O
backbone O
, O
which O
consists O
of O
12 B-HyperparameterValue
Transformer B-HyperparameterName
blocks I-HyperparameterName
, O
each O
with O
12 B-HyperparameterValue
attention B-HyperparameterName
heads I-HyperparameterName
and O
a O
hidden B-HyperparameterName
state I-HyperparameterName
dimensions I-HyperparameterName
of O
768 B-HyperparameterValue
. O

Mean B-MetricName
↓ O

We O
further O
report O
re O
MRR↑ B-MetricName
R@1↑ B-MetricName
R@5↑ B-MetricName
R@10↑ B-MetricName

We O
consider O
state O
- O
of O
- O
the O
- O
art O
published O
baselines O
, O
including O
NMN B-MethodName
( O
Hu O
et O
al. O
, O
2017 O
) O
, O
CorefNMN B-MethodName
( O
Kottur O
et O
al. O
, O
2018 O
) O
, O
GNN B-MethodName
( O
Zheng O
et O
al. O
, O
2019 O
) O
, O
FGA B-MethodName
( O
Schwartz O
et O
al. O
, O
2019 O
) O
, O
DVAN B-MethodName
( O
Guo O
et O
al. O
, O
2019b O
) O
, O
RvA B-MethodName
( O
Niu O
et O
al. O
, O
2019 O
) O
, O
DualVD B-MethodName
( O
Jiang O
et O
al. O
, O
2020 O
) O
, O
HACAN B-MethodName
( O
Yang O
et O
al. O
, O
2019 O
) O
, O
Synergistic B-MethodName
( O
Guo O
et O
al. O
, O
2019a O
) O
, O
DAN B-MethodName
( O
Kang O
et O
al. O
, O
2019 O
) O
, O
ReDAN B-MethodName
( O
Gan O
et O
al. O
, O
2019 O
) O
, O
CAG B-MethodName
( O
Guo O
et O
al. O
, O
2020 O
) O
, O
Square B-MethodName
( O
Kim O
et O
al. O
, O
2020 O
) O
, O
MCA B-MethodName
( O
Agarwal O
et O
al. O
, O
2020 O
) O
, O
MReal B-MethodName
- I-MethodName
BDAI I-MethodName
and O
P1 B-MethodName
P2 I-MethodName
( O
Qi O
et O
al. O
, O
2020 O
) O
. O

5.1 O
Main O
Results O
Comparison O
. O

Lastly O
, O
we O
interpret O
how O
it O
attains O
the O
effective O
fusion O
of O
vision O
and O
dialog O
via O
attention O
visualization O
( O
§ O
5.4 O
) O
. O

Then O
we O
conduct O
ablation O
studies O
to O
examine O
various O
aspects O
of O
our O
model O
( O
§ O
5.2 O
) O
, O
followed O
by O
an O
in O
- O
depth O
analysis O
of O
fine O
- O
tuning O
on O
dense O
annotations O
( O
§ O
5.3 O
) O
. O

5 O
Results O
and O
Analysis O
We O
first O
compare O
VD B-MethodName
- I-MethodName
BERT I-MethodName
with O
state O
- O
of O
- O
the O
- O
art O
models O
on O
VisDial B-DatasetName
datasets O
( O
§ O
5.1 O
) O
. O

Following O
Das O
et O
al. O
( O
2017 O
) O
, O
we O
evaluate O
our O
model O
using O
the O
ranking O
metrics O
like O
Recall@K B-MetricName
( O
K O
∈ O
{ O
1 O
, O
5 O
, O
10 O
} O
) O
, O
Mean B-MetricName
Reciprocal I-MetricName
Rank I-MetricName
( O
MRR B-MetricName
) O
, O
and O
Mean B-MetricName
Rank I-MetricName
, O
where O
only O
one O
3329 O

Evaluation O
Metric O
. O

The O
dense O
annotation O
specifies O
a O
relevance O
score O
for O
each O
answer O
candidate O
based O
on O
the O
fact O
that O
some O
candidates O
with O
similar O
semantics O
to O
the O
ground O
truth O
answer O
can O
also O
be O
considered O
as O
correct O
or O
partially O
correct O
, O
e.g. O
, O
“ O
brown O
and O
tan O
” O
and O
“ O
brown O
” O
in O
Figure O
2 O
. O

For O
the O
v1.0 O
validation O
split O
and O
a O
part O
of O
v1.0 O
train O
split O
( O
2,000 O
images O
) O
, O
extra O
dense O
annotations O
for O
the O
answer O
candidates O
are O
provided O
to O
make O
the O
evaluation O
more O
reasonable O
. O

For O
each O
question O
, O
it O
is O
paired O
with O
a O
list O
of O
100 O
answer O
candidates O
, O
one O
of O
which O
is O
regarded O
as O
the O
correct O
answer O
. O

Each O
image O
is O
associated O
with O
one O
caption O
and O
10 O
question O
- O
answer O
pairs O
. O

The O
v1.0 O
dataset O
combines O
the O
training O
and O
validation O
sets O
of O
v0.9 O
into O
one O
training O
set O
and O
adds O
another O
2,064 O
images O
for O
validation O
and O
8 O
, O
000 O
images O
for O
testing O
( O
hosted O
blindly O
in O
the O
task O
organizers O
’ O
server O
) O
. O

Specifically O
, O
v0.9 O
contains O
a O
training O
set O
of O
82,783 O
images O
and O
a O
validation O
set O
of O
40 O
, O
504 O
images O
. O

We O
evaluate O
our O
model O
on O
the O
VisDial B-DatasetName
v0.9 O
and O
v1.0 O
datasets O
( O
Das O
et O
al. O
, O
2017 O
) O
. O

To O
fine O
- O
tune O
on O
this O
, O
we O
combine O
the O
Datasets O
. O

As O
some O
answer O
candidates O
may O
be O
semantically O
similar O
( O
e.g. O
, O
“ O
brown O
and O
tan O
” O
vs O
“ O
brown O
” O
in O
Figure O
2 O
) O
, O
VisDial B-DatasetName
v1.0 O
additionally O
provides O
dense O
annotations O
that O
specify O
real O
- O
valued O
relevance O
scores O
for O
the O
100 O
answer O
candidates O
, O
[ O
s1 O
, O
... O
, O
s100 O
] O
with O
si O
∈ O
[ O
0 O
, O
1 O
] O
. O

Fine O
- O
tuning O
with O
Rank O
Optimization O

| O
{ O
z O
} O
N O
X O

3.3 O
f O
( O
si O
) O
log(f O
( O
pi O
) O
) O
, O
i=1 O
4 O
x O
, O
( O
I O
, O
w O
) O
= O
( O
I O
, O
Ht O
, O
Qt O
, O
Ât O
) O
. O

The O
decoding O
process O
is O
based O
on O
greedy O
sampling O
and O
terminated O
when O
a O
[ O
SEP O
] O
is O
emitted O
, O
and O
the O
resulting O
log O
- O
likelihood O
scores O
will O
be O
used O
for O
ranking O
the O
answer O
candidates O
. O

Specifically O
, O
we O
recursively O
append O
a O
[ O
MASK O
] O
token O
to O
the O
end O
of O
the O
sequence O
to O
trigger O
a O
one O
- O
step O
prediction O
and O
then O
replace O
it O
with O
the O
predicted O
token O
for O
the O
next O
token O
prediction O
. O

During O
inference O
, O
we O
rely O
on O
the O
same O
unified O
Transformer O
encoder O
with O
sequential O
MLM O
operations O
without O
an O
explicit O
decoder O
. O

For O
the O
answer O
sequence O
, O
we O
mask O
out O
( O
by O
setting O
−∞ O
in O
M O
) O
the O
“ O
future O
” O
tokens O
to O
get O
autoregressive O
attentions O
( O
see O
the O
red O
dots O
in O
Figure O
2 O
) O
. O

Experimental O
Setup O
( O
6 O
) O
context O
We O
allow O
tokens O
in O
the O
context O
to O
be O
fully O
visible O
for O
attending O
by O
setting O
the O
left O
part O
of O
M O
to O
all O
0s O
. O

To O
better O
leverage O
the O
contrastive O
signals O
from O
the O
dense O
annotations O
, O
the O
sub O
- O
sampling O
method O
first O
picks O
randomly O
the O
candidates O
with O
non O
- O
zero O
relevance O
scores O
, O
and O
then O
it O
picks O
the O
ones O
from O
zero O
scores O
( O
about O
12 O
% O
of O
candidates O
are O
non O
- O
zero O
on O
average O
) O
. O

For O
training O
efficiency O
, O
we O
sub O
- O
sample O
the O
candidate O
list O
and O
use O
only O
N B-HyperparameterName
= O
30 B-HyperparameterValue
answers O
( O
out O
of O
100 O
) O
for O
each O
instance O
. O

PN O
, O
i O
= O
1 O
, O
... O
, O
N. O
exp O
( O
x O
) O
j O
j=1 O
( O
8) O
Here O
N O
is O
the O
number O
of O
answer O
candidates O
. O

We O
adopt O
ListNet O
( O
Cao O
et O
al. O
, O
2007 O
) O
with O
the O
top-1 O
approximation O
as O
the O
ranking O
module O
for O
VD B-MethodName
- I-MethodName
BERT I-MethodName
: O
LListN O
et O
= O
− O
( O
7 O
) O
exp O
( O
xi O
) O
f O
( O
xi O
) O
= O

As O
dense O
annotation O
fine O
- O
tuning O
is O
typically O
a O
Learning O
to O
Rank O
( O
LTR O
) O
problem O
, O
we O
can O
make O
use O
of O
some O
ranking O
optimization O
methods O
( O
see O
the O
Appendix O
B.1 O
for O
more O
details O
) O
. O

[ O
p1 O
, O
... O
, O
p100 O
] O
. O

For O
this O
, O
we O
divide O
the O
input O
sequence O
to O
each O
Transformer O
block O
into O
two O
subsequences O
, O
context O
and O
answer O
: O
NSP O
scores O
from O
the O
model O
for O
all O
answer O
candidates O
together O
into O
a O
vector O

In O
order O
to O
autoregressively O
generate O
an O
answer O
, O
we O
also O
train O
VD B-MethodName
- I-MethodName
BERT I-MethodName
with O
the O
sequence O
- O
to O
- O
sequence O
( O
seq2seq O
) O
self O
- O
attention O
mask O
( O
Dong O
et O
al. O
, O
2019 O
) O
. O

Generative O
Setting O
. O

During O
inference O
, O
we O
rank O
the O
answer O
candidates O
according O
to O
the O
positive O
class O
score O
of O
their O
NSP O
heads O
. O

every O
positive O
one O
at O
different O
epochs B-HyperparameterName
. O

To O
encourage O
the O
model O
to O
penalize O
more O
on O
negative O
instances O
, O
we O
randomly O
resample O
a O
negative O
example O
from O
the O
pool O
of O
99 O
negatives O
w.r.t O
. O

To O
avoid O
imbalanced O
class O
distribution O
, O
we O
keep O
the O
ratio O
of O
positive O
and O
negative O
instances O
to O
1:1 O
in O
each O
epoch B-HyperparameterName
. O

We O
employ O
the O
bidirectional O
self O
- O
attention O
mask O
to O
allow O
all O
the O
tokens O
to O
attend O
to O
each O
other O
by O
setting O
the O
mask O
matrix O
M O
in O
Eq O
. O
( O
2 O
) O
to O
all O
0s O
. O

Specifically O
, O
we O
sample O
an O
answer O
Ât O
from O
the O
candidate O
pool O
and O
append O
it O
to O
the O
input O
sequence O
, O
and O
ask O
the O
NSP O
head O
to O
distinguish O
whether O
the O
sampled O
answer O
is O
correct O
or O
not O
. O

For O
training O
in O
the O
discriminative O
setting O
, O
we O
transform O
the O
task O
of O
selecting O
an O
answer O
into O
a O
point B-TaskName
- I-TaskName
wise I-TaskName
binary I-TaskName
classification I-TaskName
problem O
. O

Discriminative O
Setting O
. O

Below O
we O
introduce O
the O
discriminative O
and O
generative O
settings O
of O
VD B-MethodName
- I-MethodName
BERT I-MethodName
. O

As O
for O
NSP O
, O
instead O
of O
modeling O
the O
relationship O
between O
two O
sentences O
( O
as O
in O
BERT O
) O
or O
the O
matching O
of O
an O
image O
- O
text O
pair O
( O
as O
in O
other O
visionlanguage O
pretraining O
models O
like O
ViLBERT B-MethodName
) O
, O
VDBERT B-MethodName
aims O
to O
predict O
whether O
the O
appended O
answer O
candidate O
Ât O
is O
correct O
or O
not O
based O
on O
the O
joint O
understanding O
of O
the O
image O
and O
dialog O
history O
: O
LN O
SP O
= O
−E(I O
, O
w)∼D O
log O
P O
( O
y|S(I O
, O
w O
) O
) O
, O
( O
5 O
) O
where O
y O
∈ O
{ O
0 O
, O
1 O
} O
indicates O
whether O
Ât O
is O
correct O
, O
and O
S O
( O
· O
) O
is O
a O
binary O
classifier O
to O
predict O
the O
probability O
based O
on O
the O
[ O
CLS O
] O
representation O
T[CLS O
] O
at O
the O
final O
layer O
. O

Following O
Zhou O
et O
al. O
( O
2020 O
) O
, O
we O
do O
not O
conduct O
similar O
masked O
object O
/ O
region O
modeling O
in O
the O
image O
segment O
. O

The O
model O
is O
then O
required O
to O
recover O
them O
based O
not O
only O
on O
the O
surrounding O
tokens O
w\m O
but O
also O
on O
the O
image O
I O
: O
LM O
LM O
= O
−E(I O
, O
w)∼D O
log O
P O
( O
wm O
|w\m O
, O
I O
) O
, O
( O
4 O
) O
where O
wm O
refers O
to O
the O
masked O
token O
and O
D O
denotes O
the O
training O
set O
. O

[ O
MASK O
] O
. O

Similar O
to O
MLM O
in O
BERT O
, O
15 O
% O
tokens O
in O
the O
text O
segment O
( O
including O
special O
tokens O
like O
[ O
EOT O
] O
and O
[ O
SEP O
] O
) O
are O
randomly O
masked O
out O
and O
replaced O
with O
a O
special O
token O

Particularly O
, O
we O
aim O
to O
capture O
dense O
interactions O
among O
both O
inter O
- O
modality O
( O
i.e. O
, O
image O
- O
dialog O
) O
and O
intra O
- O
modality O
( O
i.e. O
, O
image O
- O
image O
, O
dialog O
- O
dialog O
) O
. O

3.2 O
Visually O
Grounded O
Training O
Objectives O
We O
use O
two O
visually O
grounded O
training O
objectives O
— O
masked O
language O
modeling O
( O
MLM O
) O
and O
next O
sentence O
prediction O
( O
NSP O
) O
to O
train O
our O
VD B-MethodName
- I-MethodName
BERT I-MethodName
. O

Then O
Al O
is O
passed O
into O
a O
feedforward O
layer O
to O
compute O
Hl O
for O
the O
next O
layer O
. O

Rdh O
×dk O
are O
learnable O
weights O
for O
computing O
the O
queries O
, O
keys O
, O
and O
values O
respectively O
, O
and O
M O
∈ O
R|x|×|x| O
is O
the O
self O
- O
attention O
mask O
that O
determines O
whether O
tokens O
from O
two O
layers O
can O
attend O
each O
other O
. O

Inside O
each O
Transformer O
block O
, O
the O
previous O
layer O
’s O
output O
Hl−1 O
∈ O
R|x|×dh O
is O
aggregated O
using O
the O
multi O
- O
head O
selfattention O
( O
Vaswani O
et O
al. O
, O
2017 O
): O
Q O
= O
Hl−1 O
WlQ O
, O
K O
= O
Hl−1 O
WlK O
, O
V O
= O
Hl−1 O
WlV O
, O
( O
( O
1 O
) O
0 O
, O
allow O
to O
attend O
, O
Mij O
= O
( O
2 O
) O
−∞ O
, O
prevent O
from O
attending O
, O
QKT O
Al O
= O
softmax O
( O
√ O
+ O
M)V O
, O
( O
3 O
) O
dk O
where O
WlQ O
, O
WlK O
, O
WlV O
∈ O

[ O
1 O
, O
L O
] O
. O

[ O
hl1 O
, O
... O
, O
hl|x| O
] O
using O
L O
- O
stacked O
Transformer O
blocks O
, O
where O
the O
l O
- O
th O
Transformer O
block O
is O
denoted O
as O
Hl O
= O
Transformer(Hl−1 O
) O
, O
l O
∈ O

Hl O
= O

and O
then O
encode O
them O
into O
multiple O
levels O
of O
contextual O
representations O

[ O
e1 O
, O
... O
, O
e|x| O
] O

We O
denote O
the O
embedded O
vision O
- O
language O
inputs O
as O
H0 O
= O

Transformer O
Backbone O
. O

Finally O
, O
each O
input O
token O
embedding O
is O
combined O
with O
its O
position O
embedding O
and O
segment O
embedding O
( O
0 O
or O
1 O
, O
indicating O
whether O
it O
is O
image O
or O
text O
) O
with O
layer B-HyperparameterName
normalization I-HyperparameterName
( O
Ba O
et O
al. O
, O
2016 O
) O
. O

To O
notify O
the O
model O
for O
the O
answer O
prediction O
, O
we O
further O
insert O
a O
[ O
PRED O
] O
token O
between O
the O
Qt O
Ât O
pair O
. O

As O
such O
, O
we O
prepare O
the O
input O
sequence O
into O
the O
format O
as O
x O
= O
( O
[ O
CLS O
] O
, O
o1 O
, O
... O
, O
ok O
, O
[ O
SEP O
] O
, O
C O
, O
[ O
EOT O
] O
, O
Q1 O
A1 O
, O
[ O
EOT O
] O
, O
... O
, O
Qt O
Ât O
, O
[ O
SEP O
] O
) O
. O

Moreover O
, O
to O
inject O
the O
multi B-TaskName
- I-TaskName
turn I-TaskName
dialog I-TaskName
structure O
into O
the O
model O
, O
we O
utilize O
a O
special O
token O
[ O
EOT O
] O
to O
denote O
end O
of O
turn O
( O
Whang O
et O
al. O
, O
2019 O
) O
, O
which O
informs O
the O
model O
when O
the O
dialog O
turn O
ends O
. O

Similar O
to O
BERT O
, O
we O
use O
special O
tokens O
like O
[ O
CLS O
] O
to O
denote O
the O
beginning O
of O
the O
sequence O
, O
and O
[ O
SEP O
] O
to O
separate O
the O
two O
modalities O
. O

To O
feed O
both O
image O
and O
text O
into O
the O
Transformer O
encoder O
, O
we O
integrate O
the O
image O
objects O
with O
language O
elements O
into O
a O
whole O
input O
sequence O
. O

Cross O
- O
Modality O
Encoding O
. O

We O
employ O
WordPiece O
tokenizer O
( O
Wu O
et O
al. O
, O
2016 O
) O
to O
split O
it O
into O
a O
word O
sequence O
w O
, O
where O
each O
word O
is O
embedded O
with O
an O
absolute O
positional O
code O
following O
Devlin O
et O
al. O
( O
2019 O
) O
. O

We O
pack O
all O
the O
textual O
elements O
( O
caption O
and O
multi B-TaskName
- I-TaskName
turn I-TaskName
dialog I-TaskName
) O
into O
a O
long O
sequence O
. O

Language O
Features O
. O

We O
extend O
pi O
with O
its O
class O
i O
d O
and O
confidence O
score O
for O
a O
richer O
representation O
. O

Specifically O
, O
let O
( O
x1 O
, O
y1 O
) O
and O
( O
x2 O
, O
y2 O
) O
be O
the O
coordinates O
of O
the O
bottom O
- O
left O
and O
top O
- O
right O
corner O
of O
the O
i O
- O
th O
object O
, O
its O
location O
information O
is O
encoded O
into O
a O
5 O
- O
d O
vecx1 O
y1 O
x2 O
y2 O
( O
x2 O
−x1 O
) O
( O
y2 O
−y1 O
) O
tor O
: O
pi O
= O
( O
W O
, O
H O
, O
W O
, O
H O
, O
) O
, O
where O
WH O
W O
and O
H O
respectively O
denote O
the O
width O
and O
height O
of O
the O
input O
image O
, O
and O
the O
last O
element O
is O
the O
relative O
area O
of O
the O
object O
. O

As O
there O
is O
no O
natural O
orders O
among O
these O
objects O
, O
we O
adopt O
normalized O
bounding O
box O
coordinates O
as O
the O
spatial O
location O
. O

Let O
OI O
= O
{ O
o1 O
, O
... O
, O
ok O
} O
denote O
the O
vision O
features O
for O
an O
image O
I O
, O
where O
each O
object O
feature O
oi O
is O
a O
2048 B-HyperparameterValue
- O
d B-HyperparameterName
Region O
- O
of O
- O
Interest O
( O
RoI O
) O
feature O
and O
k O
is O
the O
number O
of O
the O
detected O
objects O
( O
fixed O
to O
36 O
in O
our O
setting O
) O
. O

Following O
previous O
work O
, O
we O
employ O
Faster B-MethodName
R I-MethodName
- I-MethodName
CNN I-MethodName
( O
Ren O
et O
al. O
, O
2015 O
) O
pretrained O
on O
Visual B-DatasetName
Genome I-DatasetName
( O
Krishna O
et O
al. O
, O
2017 O
) O
to O
extract O
the O
object O
- O
level O
vision O
features O
. O

3.1 O
Vision B-MethodName
- I-MethodName
Dialog I-MethodName
Transformer I-MethodName
Encoder O
Vision O
Features O
. O

Lastly O
, O
we O
devise O
a O
ranking O
optimization O
module O
to O
further O
fine O
- O
tune O
on O
the O
dense O
annotations O
( O
§ O
3.3 O
) O
. O

This O
allows O
our O
unified O
model O
to O
work O
in O
both O
discriminative O
and O
generative O
settings O
( O
§ O
3.2 O
) O
. O

Next O
, O
we O
adopt O
visually O
grounded O
MLM O
and O
NSP O
objectives O
to O
train O
the O
model O
for O
effective O
vision O
and O
dialog O
fusion O
using O
two O
types O
of O
self O
- O
attention O
masks O
– O
bidirectional O
and O
seq2seq O
. O

First O
, O
we O
employ O
a O
unified B-MethodName
vision I-MethodName
- I-MethodName
dialog I-MethodName
Transformer I-MethodName
to O
encode O
both O
the O
image O
and O
dialog O
history O
, O
where O
we O
append O
an O
answer O
candidate O
Ât O
in O
the O
input O
to O
model O
their O
interactions O
in O
an O
early O
fusion O
manner O
( O
§ O
3.1 O
) O
. O

Figure O
2 O
shows O
the O
overview O
of O
our O
approach O
. O

In O
general O
, O
there O
are O
two O
types O
of O
decoder O
to O
predict O
the O
answer O
: O
a O
discriminative O
decoder O
that O
ranks O
the O
answer O
candidates O
and O
is O
trained O
with O
a O
cross O
entropy O
loss O
, O
or O
a O
generative O
decoder O
that O
synthesizes O
an O
answer O
and O
is O
trained O
with O
a O
maximum O
log O
- O
likelihood O
loss O
. O

Given O
a O
question O
Qt O
grounded O
on O
an O
image O
I O
at O
t O
- O
th O
turn O
, O
as O
well O
as O
its O
dialog O
history O
formulated O
as O
Ht O
= O
{ O
C O
, O
( O
Q1 O
, O
A1 O
) O
, O
... O
, O
( O
Qt−1 O
, O
At−1 O
) O
} O
( O
where O
C O
denotes O
the O
image O
caption O
) O
, O
the O
agent O
is O
asked O
to O
predict O
its O
answer O
At O
by O
ranking O
a O
list O
of O
100 O
answer O
candidates O
{ O
Â1 O
t O
, O
Â2 O
t O
, O
... O
, O
Â100 O
t O
} O
. O

The O
VD B-MethodName
- I-MethodName
BERT I-MethodName
Model O
We O
first O
formally O
describe O
the O
visual B-TaskName
dialog I-TaskName
task O
. O

Our O
work O
has O
two O
major O
advantages O
over O
VisDial B-MethodName
- I-MethodName
BERT I-MethodName
: O
first O
, O
VDBERT B-MethodName
supports O
both O
discriminative O
and O
generative O
settings O
while O
theirs O
is O
restricted O
to O
only O
the O
discriminative O
setting O
; O
second O
, O
we O
do O
not O
require O
to O
pretrain O
on O
large O
- O
scale O
external O
vision O
- O
language O
datasets O
like O
theirs O
and O
still O
yield O
better O
performance O
( O
§ O
5.1 O
) O
. O

Most O
closely O
related O
to O
this O
paper O
is O
the O
concurrent O
work O
VisDial B-MethodName
- I-MethodName
BERT I-MethodName
by O
Murahari O
et O
al. O
( O
2019 O
) O
, O
who O
also O
employ O
pretrained O
models O
( O
i.e. O
, O
ViLBERT B-MethodName
) O
for O
visual O
dialog O
. O

Our O
model O
is O
inspired O
by O
VLP B-MethodName
and O
specifically O
tailored O
for O
the O
visual B-TaskName
dialog I-TaskName
task O
. O

Their O
model O
was O
proposed O
for O
VQA B-TaskName
and O
image B-TaskName
captioning I-TaskName
. O

More O
recently O
, O
Zhou O
et O
al. O
( O
2020 O
) O
proposed O
VLP B-MethodName
which O
also O
allows O
generation O
using O
a O
unified O
Transformer O
with O
various O
self O
- O
attention O
masks O
( O
Dong O
et O
al. O
, O
2019 O
) O
. O

These O
models O
yield O
prominent O
improvements O
mainly O
on O
vision B-TaskName
- I-TaskName
language I-TaskName
understanding I-TaskName
tasks O
like O
VQA B-TaskName
, O
image B-TaskName
retrieval I-TaskName
( O
Young O
et O
al. O
, O
2014 O
) O
, O
and O
visual B-TaskName
reasoning I-TaskName
( O
Suhr O
et O
al. O
, O
2019 O
; O
Zellers O
et O
al. O
, O
2019 O
) O
. O

Our O
VD B-MethodName
- I-MethodName
BERT I-MethodName
belongs O
to O
the O
second O
group O
. O

They O
typically O
employ O
the O
Transformer O
encoder O
as O
the O
backbone O
with O
either O
a O
two O
- O
stream O
architecture O
to O
encode O
text O
and O
image O
independently O
such O
as O
ViLBERT B-MethodName
( O
Lu O
et O
al. O
, O
2019 O
) O
and O
LXMERT B-MethodName
( O
Tan O
and O
Bansal O
, O
2019 O
) O
, O
or O
a O
single O
- O
stream O
architecture O
to O
encode O
both O
text O
and O
image O
together O
, O
such O
as O
B2T2 B-MethodName
( O
Alberti O
et O
al. O
, O
2019 O
) O
, O
Unicoder B-MethodName
- I-MethodName
VL I-MethodName
( O
Li O
et O
al. O
, O
2020 O
) O
, O
VisualBERT B-MethodName
( O
Li O
et O
al. O
, O
2019 O
) O
, O
VL B-MethodName
- I-MethodName
BERT I-MethodName
( O
Su O
et O
al. O
, O
2020 O
) O
, O
and O
UNITER B-MethodName
( O
Chen O
et O
al. O
, O
2019 O
) O
. O

In O
order O
to O
benefit O
from O
the O
pretraining O
, O
there O
are O
many O
recent O
works O
on O
extending O
BERT O
for O
vision O
and O
language O
pretraining O
. O

Pretrained O
language O
models O
like O
BERT O
( O
Devlin O
et O
al. O
, O
2019 O
) O
have O
boosted O
performance O
greatly O
in O
a O
broad O
set O
of O
NLP O
tasks O
. O

Pretraining O
in O
Vision O
and O
Language O
. O

Regarding O
the O
architecture O
, O
our O
model O
mainly O
differs O
from O
previous O
work O
in O
two O
facets O
: O
first O
, O
unlike O
most O
prior O
work O
that O
considers O
answer O
candidates O
only O
at O
the O
final O
similarity O
computation O
layer O
, O
our O
VD B-MethodName
- I-MethodName
BERT I-MethodName
integrates O
each O
answer O
candidate O
at O
the O
input O
layer O
to O
enable O
its O
early O
and O
deep O
fusion O
with O
other O
entities O
, O
similar O
to O
Schwartz O
et O
al. O
( O
2019 O
) O
; O
second O
, O
existing O
models O
adopt O
an O
encoderdecoder O
framework O
( O
Sutskever O
et O
al. O
, O
2014 O
) O
with O
two O
types O
of O
decoder O
for O
the O
discriminative O
and O
generative O
settings O
separately O
, O
while O
we O
instead O
adopt O
a O
unified O
Transformer O
encoder O
with O
two O
different O
self O
- O
attention O
masks O
( O
Dong O
et O
al. O
, O
2019 O
) O
to O
seamlessly O
support O
both O
settings O
without O
extra O
decoders O
. O

However O
, O
their O
models O
neglect O
the O
important O
early O
interaction O
of O
the O
answer O
entity O
and O
can O
not O
naturally O
leverage O
the O
pretrained O
language O
representations O
from O
BERT O
like O
ours O
. O

There O
are O
recent O
works O
( O
Nguyen O
et O
al. O
, O
2019 O
; O
Agarwal O
et O
al. O
, O
2020 O
) O
also O
applying O
the O
Transformer O
to O
model O
the O
interactions O
among O
many O
entities O
. O

Similar O
to O
this O
, O
Schwartz O
et O
al. O
( O
2019 O
) O
proposed O
FGA B-MethodName
, O
a O
general O
factor O
graph O
attention O
that O
can O
model O
interactions O
between O
any O
two O
entities O
but O
in O
a O
pairwise O
manner O
. O

Different O
from O
them O
, O
we O
rely O
on O
the O
selfattention O
mechanism O
within O
a O
single O
- O
stream O
Transformer O
encoder O
to O
capture O
such O
interactions O
in O
a O
unified O
manner O
and O
derive O
a O
“ O
holistic O
” O
contextualized O
representation O
for O
all O
the O
entities O
. O

ReDAN B-MethodName
, O
proposed O
by O
Gan O
et O
al. O
( O
2019 O
) O
, O
further O
explores O
the O
interactions O
between O
image O
and O
dialog O
history O
via O
multi O
- O
step O
reasoning O
. O

For O
example O
, O
Kang O
et O
al. O
( O
2019 O
) O
proposed O
DAN B-MethodName
, O
a O
dual O
attention O
module O
to O
first O
refer O
to O
relevant O
contexts O
in O
the O
dialog O
history O
, O
and O
then O
find O
indicative O
image O
regions O
. O

Previous O
work O
( O
Lu O
et O
al. O
, O
2017 O
; O
Seo O
et O
al. O
, O
2017 O
; O
Wu O
et O
al. O
, O
2018 O
; O
Kottur O
et O
al. O
, O
2018 O
; O
Jiang O
et O
al. O
, O
2020 O
; O
Yang O
et O
al. O
, O
2019 O
; O
Guo O
et O
al. O
, O
2019a O
; O
Niu O
et O
al. O
, O
2019 O
) O
focuses O
on O
developing O
a O
variety O
of O
attention O
mechanisms O
to O
model O
the O
interactions O
among O
entities O
including O
image O
, O
question O
, O
and O
dialog O
history O
. O

It O
is O
one O
of O
the O
most O
challenging O
vision O
- O
language O
tasks O
that O
require O
not O
only O
to O
understand O
the O
image O
content O
according O
to O
texts O
, O
but O
also O
to O
reason O
through O
the O
dialog O
history O
. O

The O
Visual B-TaskName
Dialog I-TaskName
task O
has O
been O
recently O
proposed O
by O
Das O
et O
al. O
( O
2017 O
) O
, O
where O
a O
dialog O
agent O
needs O
to O
answer O
a O
series O
of O
questions O
grounded O
by O
an O
image O
. O

2 O
Related O
Work O
Visual B-TaskName
Dialog I-TaskName
. O

• O
Without O
the O
need O
to O
pretrain O
on O
external O
visionlanguage O
data O
, O
our O
model O
yields O
new O
state O
- O
of O
- O
theart O
results O
in O
discriminative O
setting O
and O
promising O
results O
in O
generative O
setting O
on O
the O
visual B-TaskName
dialog I-TaskName
benchmarks O
( O
§ O
5.1 O
) O
. O

• O
We O
conduct O
extensive O
experiments O
not O
only O
to O
analyze O
how O
our O
model O
performs O
with O
various O
training O
aspects O
( O
§ O
5.2 O
) O
and O
fine O
- O
tuning O
on O
dense O
annotations O
( O
§ O
5.3 O
) O
, O
but O
also O
to O
interpret O
it O
via O
attention O
visualization O
( O
§ O
5.4 O
) O
, O
shedding O
light O
on O
future O
transfer O
learning O
research O
for O
VisDial B-DatasetName
tasks O
. O

Besides O
, O
our O
VD B-MethodName
- I-MethodName
BERT I-MethodName
is O
the O
first O
unified O
model O
that O
supports O
both O
discriminative O
and O
generative O
training O
settings O
without O
explicit O
decoders O
. O

We O
showcase O
that O
BERT O
can O
be O
effectively O
adapted O
to O
this O
task O
with O
simple O
visually O
grounded O
training O
for O
capturing O
the O
intricate O
vision O
- O
dialog O
interactions O
. O

In O
summary O
, O
we O
make O
the O
following O
contributions O
: O
• O
To O
the O
best O
of O
our O
knowledge O
, O
our O
work O
serves O
as O
one O
of O
the O
first O
attempts O
to O
explore O
pretrained O
language O
models O
for O
visual B-TaskName
dialog I-TaskName
. O

We O
further O
fine O
- O
tune O
our O
model O
on O
dense O
annotations O
that O
specify O
the O
relevance O
score O
for O
each O
answer O
candidate O
with O
a O
ranking O
optimization O
module O
. O

During O
inference O
, O
our O
VD B-MethodName
- I-MethodName
BERT I-MethodName
either O
ranks O
the O
answer O
candidates O
according O
to O
their O
NSP O
scores O
or O
generates O
the O
answer O
sequence O
by O
recursively O
applying O
the O
MLM O
operations O
. O

Instead O
of O
employing O
two O
types O
of O
decoders O
like O
prior O
work O
, O
we O
rely O
on O
a O
unified O
Transformer O
architecture O
with O
two O
different O
self O
- O
attention O
masks O
( O
Dong O
et O
al. O
, O
2019 O
) O
to O
seamlessly O
support O
both O
settings O
. O

In O
the O
discriminative O
setting O
, O
the O
model O
ranks O
a O
pool O
of O
answer O
candidates O
, O
whereas O
the O
generative O
setting O
additionally O
allows O
the O
model O
to O
generate O
the O
answers O
. O

VisDial B-DatasetName
models O
have O
been O
trained O
in O
one O
of O
two O
settings O
: O
discriminative O
or O
generative O
. O

Different O
from O
the O
original O
MLM O
and O
NSP O
in O
BERT O
, O
we O
additionally O
take O
the O
visual O
information O
into O
account O
when O
predicting O
the O
masked O
tokens O
or O
the O
next O
answer O
. O

To O
effectively O
fuse O
features O
from O
the O
two O
modalities O
, O
we O
make O
use O
of O
two O
visually O
grounded O
training O
objectives O
– O
Masked O
Language O
Modeling O
( O
MLM O
) O
and O
Next O
Sentence O
Prediction O
( O
NSP O
) O
. O

We O
initialize O
the O
encoder O
with O
BERT O
for O
better O
leveraging O
the O
pretrained O
language O
representations O
. O

Specifically O
, O
we O
first O
encode O
the O
image O
into O
a O
series O
of O
detected O
objects O
and O
feed O
them O
into O
a O
Transformer O
encoder O
together O
with O
the O
image O
caption O
and O
multi B-TaskName
- I-TaskName
turn I-TaskName
dialog I-TaskName
. O

In O
this O
paper O
, O
we O
present O
VD B-MethodName
- I-MethodName
BERT I-MethodName
, O
a O
novel O
unified B-MethodName
vision I-MethodName
- I-MethodName
dialog I-MethodName
Transformer I-MethodName
framework O
for O
VisDial B-DatasetName
tasks O
. O

Specifically O
, O
each O
image O
in O
the O
VisDial B-DatasetName
dataset O
is O
associated O
with O
up O
to O
10 O
dialog O
turns O
, O
which O
contain O
much O
longer O
contexts O
than O
either O
VQA B-TaskName
or O
image B-TaskName
captioning I-TaskName
. O

However O
, O
it O
is O
still O
unclear O
how O
visual B-TaskName
dialog I-TaskName
may O
benefit O
from O
such O
vision O
- O
language O
pretraining O
due O
to O
its O
unique O
multiturn O
conversational O
structure O
. O

This O
has O
led O
to O
compelling O
results O
in O
tasks O
such O
as O
VQA B-TaskName
, B-TaskName
image I-TaskName
captioning I-TaskName
, O
image B-TaskName
retrieval I-TaskName
( O
Young O
et O
al. O
, O
2014 O
) O
, O
and O
visual B-TaskName
reasoning I-TaskName
( O
Suhr O
et O
al. O
, O
2019 O
) O
. O

They O
often O
use O
self O
- O
supervised O
objectives O
to O
pretrain O
BERT O
- O
like O
models O
on O
large O
- O
scale O
external O
vision O
- O
language O
data O
and O
then O
fine O
- O
tune O
on O
downstream O
tasks O
. O

2019 O
; O
Lu O
et O
al. O
, O
2019 O
; O
Tan O
and O
Bansal O
, O
2019 O
; O
Zhou O
et O
al. O
, O
2020 O
) O
. O

c O
2020 O
Association O
for O
Computational O
Linguistics O

Recently O
several O
emerging O
works O
have O
attempted O
to O
adapt O
BERT O
for O
multimodal B-TaskName
tasks O
( O
Sun O
et O
al. O
, O
3325 O
Proceedings O
of O
the O
2020 O
Conference O
on O
Empirical O
Methods O
in O
Natural O
Language O
Processing O
, O
pages O
3325–3338 O
, O
November O
16–20 O
, O
2020 O
. O

Inspired O
by O
its O
recent O
success O
in O
vision O
- O
language O
pretraining O
, O
we O
further O
extend O
BERT O
to O
achieve O
simple O
yet O
effective O
fusion O
of O
vision O
and O
dialog O
contents O
in O
VisDial B-DatasetName
tasks O
. O

We O
employ O
the O
Transformer O
as O
the O
encoding O
backbone O
due O
to O
its O
powerful O
representation O
learning O
capability O
exhibited O
in O
pretrained O
language O
models O
like O
BERT O
( O
Devlin O
et O
al. O
, O
2019 O
) O
. O

In O
this O
way O
, O
all O
the O
entities O
simultaneously O
play O
the O
role O
of O
an O
“ O
information O
seeker O
” O
( O
query O
) O
and O
an O
“ O
information O
provider O
” O
( O
key O
- O
value O
) O
, O
thereby O
fully O
unleashing O
the O
potential O
of O
attention O
similar O
to O
Schwartz O
et O
al. O
( O
2019 O
) O
. O

By O
contrast O
, O
in O
this O
work O
, O
we O
allow O
for O
bidirectional O
attention O
flow O
between O
all O
the O
entities O
using O
a O
unified O
Transformer O
( O
Vaswani O
et O
al. O
, O
2017 O
) O
encoder O
, O
as O
shown O
in O
Figure O
1(c O
) O
. O

In O
other O
words O
, O
the O
attention O
flow O
in O
these O
methods O
is O
unidirectional O
– O
from O
question O
to O
the O
other O
components O
( O
Figure O
1(b O
) O
) O
. O

Typically O
, O
most O
of O
previous O
work O
( O
Niu O
et O
al. O
, O
2019 O
; O
Gan O
et O
al. O
, O
2019 O
; O
Kang O
et O
al. O
, O
2019 O
) O
uses O
the O
question O
as O
a O
query O
to O
attend O
to O
relevant O
image O
regions O
and O
dialog O
history O
, O
where O
their O
interactions O
are O
usually O
exploited O
to O
obtain O
better O
visual O
- O
historical O
cues O
for O
predicting O
the O
answer O
. O

Compared O
to O
VQA B-TaskName
that O
predicts O
an O
answer O
based O
only O
on O
the O
question O
about O
the O
image O
( O
Figure O
1(a O
) O
) O
, O
VisDial B-DatasetName
needs O
to O
additionally O
consider O
the O
dialog O
history O
. O

ter O
fusion O
of O
vision O
and O
dialog O
contents O
. O

This O
work O
was O
mainly O
done O
when O
Yue O
Wang O
was O
an O
intern O
at O
Salesforce O
Research O
Asia O
, O
Singapore O
. O

The O
primary O
research O
direction O
in O
VisDial B-DatasetName
has O
been O
mostly O
focusing O
on O
developing O
various O
attention O
mechanisms O
( O
Bahdanau O
et O
al. O
, O
2015 O
) O
for O
a O
bet O
* O

Unlike O
the O
traditional O
single B-TaskName
- I-TaskName
turn I-TaskName
Visual I-TaskName
Question I-TaskName
Answering I-TaskName
( O
VQA B-TaskName
) O
( O
Antol O
et O
al. O
, O
2015 O
) O
, O
the O
agent O
in O
VisDial B-DatasetName
requires O
to O
answer O
questions O
through O
multiple O
rounds O
of O
interactions O
together O
with O
visual O
content O
understanding O
. O

Introduction O
Visual B-TaskName
Dialog I-TaskName
( O
or O
VisDial B-DatasetName
) O
aims O
to O
build O
an O
AI O
agent O
that O
can O
answer O
a O
human O
’s O
questions O
about O
visual O
content O
in O
a O
natural O
conversational O
setting O
( O
Das O
et O
al. O
, O
2017 O
) O
. O

Our O
code O
and O
pretrained O
models O
are O
released O
at O
https O
: O
//github.com O
/ O
salesforce O
/ O
VD B-MethodName
- I-MethodName
BERT I-MethodName
. O

Without O
the O
need O
of O
pretraining O
on O
external O
vision O
- O
language O
data O
, O
our O
model O
yields O
new O
state O
of O
the O
art O
, O
achieving O
the O
top O
position O
in O
both O
single O
- O
model O
and O
ensemble O
settings O
( O
74.54 B-MetricValue
and O
75.35 B-MetricValue
NDCG B-MetricName
scores O
) O
on O
the O
visual B-TaskName
dialog I-TaskName
leaderboard O
. O

More O
crucially O
, O
we O
adapt O
BERT O
for O
the O
effective O
fusion O
of O
vision O
and O
dialog O
contents O
via O
visually O
grounded O
training O
. O

The O
model O
is O
unified O
in O
that O
( O
1 O
) O
it O
captures O
all O
the O
interactions O
between O
the O
image O
and O
the O
multi B-TaskName
- I-TaskName
turn I-TaskName
dialog I-TaskName
using O
a O
single O
- O
stream O
Transformer O
encoder O
, O
and O
( O
2 O
) O
it O
supports O
both O
answer O
ranking O
and O
answer O
generation O
seamlessly O
through O
the O
same O
architecture O
. O

By O
contrast O
, O
in O
this O
work O
, O
we O
propose O
VD B-MethodName
- I-MethodName
BERT I-MethodName
, O
a O
simple O
yet O
effective O
framework O
of O
unified B-MethodName
vision I-MethodName
- I-MethodName
dialog I-MethodName
Transformer I-MethodName
that O
leverages O
the O
pretrained O
BERT O
language O
models O
for O
Visual B-TaskName
Dialog I-TaskName
tasks O
. O

Prior O
work O
has O
mostly O
focused O
on O
various O
attention O
mechanisms O
to O
model O
such O
intricate O
interactions O
. O

Wang1∗ O
, O
Shafiq O
Joty2 O
, O
Michael O
R. O
Lyu1 O
, O
Irwin O
King1 O
, O
Caiming O
Xiong2 O
, O
and O
Steven O
C.H. O
Hoi2 O
1 O
Department O
of O
Computer O
Science O
and O
Engineering O
The O
Chinese O
University O
of O
Hong O
Kong O
, O
HKSAR O
, O
China O
2 O
Salesforce O
Research O
1 O
{ O
yuewang,lyu,king}@cse.cuhk.edu.hk O
2 O
{ O
sjoty,cxiong,shoi}@salesforce.com O
Abstract O
Q O
Visual B-TaskName
dialog I-TaskName
is O
a O
challenging O
vision O
- O
language O
task O
, O
where O
a O
dialog O
agent O
needs O
to O
answer O
a O
series O
of O
questions O
through O
reasoning O
on O
the O
image O
content O
and O
dialog O
history O
. O

VD B-MethodName
- I-MethodName
BERT I-MethodName
: O
A O
Unified B-MethodName
Vision I-MethodName
and I-MethodName
Dialog I-MethodName
Transformer I-MethodName
with I-MethodName
BERT I-MethodName
Yue O

-DOCSTART- O
4 O
5 O
The O
problem O
of O
language O
modeling O
is O
essentially O
density O
estimation O
for O
text O
data O
. O

Visualizing O
Memory O
and O
Permutation O
In O
this O
section O
, O
we O
provide O
a O
detailed O
visualization O
of O
the O
proposed O
permutation O
language O
modeling O
objective O
, O
including O
the O
mechanism O
of O
reusing O
memory O
( O
aka O
the O
recurrence O
mechanism O
) O
, O
how O
we O
use O
attention O
masks O
to O
permute O
the O
factorization O
order O
, O
and O
the O
difference O
of O
the O
two O
attention O
streams O
. O

On O
the O
other O
hand O
, O
the O
proposed O
permutation O
LM O
objective O
mostly O
contributes O
to O
a O
better O
data O
efficiency O
, O
whose O
effects O
may O
not O
be O
obvious O
from O
qualitative O
visualization O
. O

Moreover O
, O
it O
becomes O
possible O
to O
leverage O
the O
rapid O
progress O
of O
language O
modeling O
research O
for O
pretraining O
. O

As O
a O
result O
, O
it O
further O
“ O
justifies O
” O
language O
modeling O
research O
. O

XLNet B-MethodName
generalizes O
language O
modeling O
and O
bridges O
such O
a O
gap O
. O

It O
has O
even O
been O
challenged O
by O
some O
machine O
learning O
practitioners O
whether O
language O
modeling O
is O
a O
meaningful O
pursuit O
if O
it O
does O
not O
directly O
improve O
downstream O
tasks O
5 O
. O

However O
, O
there O
has O
been O
a O
gap O
between O
language O
modeling O
and O
pretraining O
due O
to O
the O
lack O
of O
the O
capability O
of O
bidirectional O
context O
modeling O
, O
as O
analyzed O
in O
Section O
A.5.2 O
. O

The O
representations O
of O
“ O
Thom O
Yorke O
” O
are O
not O
dependent O
on O
“ O
Radiohead O
” O
with O
AR O
language O
modeling O
and O
thus O
they O
will O
not O
be O
chosen O
as O
the O
answer O
by O
the O
standard O
approach O
that O
employs O
softmax O
over O
all O
token O
representations O
. O

Such O
a O
limitation O
of O
AR O
language O
modeling O
can O
be O
critical O
in O
real O
- O
world O
applications O
. O

A.5 O
A.5.1 O
Yelp-5 O
512 O
128 O
1e-5 O
10 O

Number O
of O
steps O
12 O
K O
8 O
K O
10 O
K O
Learning O
rate O
decay O
linear O
Weight O
decay O
0.01 O
Adam O
epsilon O
1e-6 O
1e-6 O
1e-6 O
Layer O
- O
wise O
lr O
decay O
1.0 O
0.75 O
1.0 O
Table O
8 O
: O
Hyperparameters O
for O
finetuning O
. O

2e-5 O
3e-5 O
2e-5 O

Hparam O
RACE B-DatasetName
SQuAD B-DatasetName
MNLI B-DatasetName
Dropout B-HyperparameterName
0.1 B-HyperparameterValue
Attention B-HyperparameterName
dropout I-HyperparameterName
0.1 B-HyperparameterValue
Max B-HyperparameterName
sequence I-HyperparameterName
length I-HyperparameterName
512 B-HyperparameterValue
512 B-HyperparameterValue
128 B-HyperparameterValue
Batch B-HyperparameterName
size I-HyperparameterName
32 B-HyperparameterValue
48 B-HyperparameterValue
128 B-HyperparameterValue
Learning B-HyperparameterName
rate I-HyperparameterName

For O
example O
, O
suppose O
the O
24 O
- O
th O
layer O
uses O
a O
learning B-HyperparameterName
rate I-HyperparameterName
l B-HyperparameterName
, O
and O
the O
Layer B-HyperparameterName
- I-HyperparameterName
wise I-HyperparameterName
decay I-HyperparameterName
rate I-HyperparameterName
is O
α B-HyperparameterName
, O
then O
the O
learning B-HyperparameterName
rate I-HyperparameterName
of O
layer O
m O
is O
lα24−m B-HyperparameterValue
. O

“ O
Layer B-HyperparameterName
- I-HyperparameterName
wise I-HyperparameterName
decay I-HyperparameterName
” O
means O
exponentially O
decaying O
the O
learning B-HyperparameterName
rates I-HyperparameterName
of O
individual O
layers O
in O
a O
top O
- O
down O
manner O
. O

Learning O
rate O
decay O
linear O
Adam O
epsilon O
1e-6 O
Weight O
decay O
0.01 O
Table O
7 O
: O
Hyperparameters O
for O
pretraining O
. O

40,000 O

Dropout O
0.0 O
Attention O
dropout O
0.1 O
Partial O
prediction O
K O
6 O
Max O
sequence O
length O
512 O
Batch O
size O
8192 O
Learning O
rate O
4e-4 O
Number O
of O
steps O
500 O
K O
Warmup O
steps O

Hidden O
size O
1024 O
Number O
of O
attention O
heads O
16 O
Attention O
head O
size O
64 O
FFN O
inner O
hidden O
size O
4096 O
Hidden O
Dropout O
0.1 O
GeLU O

A.4 O
A.4.1 O
Hyperparameters O
Pretraining O
Hyperparameters O
Hparam O
Value O
Number O
of O
layers O
24 O

Here O
, O
we O
provide O
the O
implementation O
details O
of O
the O
two O
- O
stream O
attention O
with O
a O
Transformer B-MethodName
- I-MethodName
XL I-MethodName
backbone O
. O

A O
Target O
- O
Aware O
Representation O
via O
Two O
- O
Stream O
Self O
- O
Attention O
A.1 O

The O
neural O
architecture O
of O
XLNet B-MethodName
is O
developed O
to O
work O
seamlessly O
with O
the O
AR O
objective O
, O
including O
integrating O
Transformer B-MethodName
- I-MethodName
XL I-MethodName
and O
the O
careful O
design O
of O
the O
two O
- O
stream O
attention O
mechanism O
. O

4 O
Conclusions O
XLNet B-MethodName
is O
a O
generalized O
AR O
pretraining O
method O
that O
uses O
a O
permutation O
language O
modeling O
objective O
to O
combine O
the O
advantages O
of O
AR O
and O
AE O
methods O
. O

Hence O
, O
we O
exclude O
the O
next O
- O
sentence O
prediction O
objective O
from O
XLNet B-MethodName
. O

Finally O
, O
we O
unexpectedly O
find O
the O
the O
next O
- O
sentence O
prediction O
objective O
proposed O
in O
the O
original O
BERT B-MethodName
does O
not O
necessarily O
lead O
to O
an O
improvement O
in O
our O
setting O
. O

Examining O
rows O
1 O
- O
4 O
of O
Table O
6 O
, O
we O
can O
see O
both O
Transformer B-MethodName
- I-MethodName
XL I-MethodName
and O
the O
permutation O
LM O
clearly O
contribute O
the O
superior O
performance O
of O
XLNet B-MethodName
over O
BERT B-MethodName
. O

DAE O
+ O
Transformer O
- O
XL O
3 O
XLNet O
- O
Base O
( O
K O
= O
7 O
) O
66.05 O
81.33 O
78.46 O
85.84/85.43 O
92.66 O
4 O
XLNet O
- O
Base O
( O
K O
= O
6 O
) O
66.66 O
80.98 O
78.18 O
85.63/85.12 O
93.35 O
5 O
- O
memory O
65.55 O
80.15 O
77.27 O
85.32/85.05 O
92.78 O
6 O
- O
span O
- O
based O
pred O
65.95 O
80.61 O
77.91 O
85.49/85.02 O
93.12 O
7 O
- O
bidirectional O
data O
66.34 O
80.65 O
77.87 O
85.31/84.99 O
92.66 O
66.76 O
79.83 O
76.94 O
85.32/85.09 O
92.89 O
8 O
+ O
next O
- O
sent O
pred O
Table O
6 O
: O
The O
results O
of O
BERT O
on O
RACE O
are O
taken O
from O
[ O
38 O
] O
. O

m O
/ O
mm O
SST-2 O
1 O
BERT O
- O
Base O
64.3 O
76.30 O
73.66 O
84.34/84.65 O
92.78 O
65.03 O
79.56 O
76.80 O
84.88/84.45 O
92.60 O
2 O

# O
Model O
RACE B-DatasetName
SQuAD2.0 B-DatasetName
F1 B-MetricName
EM B-MetricName
MNLI B-DatasetName

All O
results O
reported O
are O
the O
median O
of O
5 B-HyperparameterValue
runs B-HyperparameterName
. O

With O
these O
purposes O
in O
mind O
, O
in O
Table O
6 O
, O
we O
compare O
6 O
XLNet B-MethodName
- O
Base O
variants O
with O
different O
implementation O
details O
( O
rows O
3 O
- O
8) O
, O
the O
original O
BERT B-MethodName
- I-MethodName
Base I-MethodName
model O
( O
row O
1 O
) O
, O
and O
an O
additional O
Transformer B-MethodName
- I-MethodName
XL I-MethodName
baseline O
trained O
with O
the O
denoising O
auto O
- O
encoding O
( O
DAE O
) O
objective O
used O
in O
BERT B-MethodName
but O
with O
the O
bidirectional O
input O
pipeline O
( O
row O
2 O
) O
. O

• O
The O
necessity O
of O
some O
implementation O
details O
including O
span O
- O
based O
prediction O
, O
the O
bidirectional O
input O
pipeline O
, O
and O
next O
- O
sentence O
prediction O
. O

Specifically O
, O
there O
are O
three O
main O
aspects O
we O
hope O
to O
study O
: O
• O
The O
effectiveness O
of O
the O
permutation O
language O
modeling O
objective O
alone O
, O
especially O
compared O
to O
the O
denoising O
auto O
- O
encoding O
objective O
used O
by O
BERT B-MethodName
. O

90.4† O
88.5 O
97.1† O
92.9 O
70.2 O
93.0 O
92.5 O
Table O
5 O
: O
Results O
on O
GLUE O
. O

[ O
2 O
] O
86.6/92.3 O
RoBERTa O
[ O
21 O
] O
90.2/90.2 O
94.7 O
XLNet O
90.8/90.8 O
94.9 O
91.3 O
92.2 O
92.3 O
70.4 O
86.6 O
85.9 O
93.2 O
96.4 O
97.0 O
88.0 O
90.9 O
90.8 O
60.6 O
68.0 O
69.0 O
90.0 O
92.4 O
92.5 O
Multi O
- O
task O
ensembles O
on O
test O
( O
from O
leaderboard O
as O
of O
Oct O
28 O
, O
2019 O
) O
MT O
- O
DNN∗ O
[ O
20 O
] O
87.9/87.4 O
96.0 O
89.9 O
86.3 O
96.5 O
92.7 O
68.4 O
91.1 O
89.0 O
RoBERTa∗ O
[ O
21 O
] O
90.8/90.2 O
98.9 O
90.2 O
88.2 O
96.7 O
92.3 O
67.8 O
92.2 O
89.0 O
XLNet∗ O
90.9/90.9† O
99.0† O

Single O
- O
task O
single O
models O
on O
dev O
BERT O

QNLI B-DatasetName
QQP B-DatasetName
RTE B-DatasetName
SST-2 B-DatasetName
MRPC B-DatasetName
CoLA B-DatasetName
STS B-DatasetName
- I-DatasetName
B I-DatasetName
WNLI B-DatasetName

Model O
MNLI B-DatasetName

All O
BERT B-MethodName
and O
XLNet B-MethodName
results O
are O
obtained O
with O
a O
24 B-HyperparameterValue
- O
layer B-HyperparameterName
architecture O
with O
similar O
model O
sizes O
( O
aka O
BERT B-MethodName
- I-MethodName
Large I-MethodName
) O
. O

[ O
35 O
] O
4.32 B-MetricValue
4.6 B-MetricValue
4.51 B-MetricValue
2.90 B-MetricValue
2.64 B-MetricValue
2.16 B-MetricValue
1.89 B-MetricValue
32.39 B-MetricValue
30.58 B-MetricValue
29.98 B-MetricValue
29.32 B-MetricValue
0.84 B-MetricValue
0.88 B-MetricValue
0.70 B-MetricValue
0.80 B-MetricValue
0.64 B-MetricValue
6.57 B-MetricValue
6.87 B-MetricValue
4.95 B-MetricValue
5.01 B-MetricValue
3.79 B-MetricValue
3.32 B-MetricValue
2.63 B-MetricValue
36.24 B-MetricValue
34.81 B-MetricValue
34.17 B-MetricValue
XLNet B-MethodName
3.20 B-MetricValue
1.37 B-MetricValue
27.05 B-MetricValue
0.60 B-MetricValue
4.45 B-MetricValue
2.11 B-MetricValue
31.67 B-MetricValue
Table O
4 O
: O
Comparison O
with O
state O
- O
of O
- O
the O
- O
art O
error O
rates O
on O
the O
test O
sets O
of O
several O
text B-TaskName
classification B-TaskName
datasets O
. O

[ O
15 O
] O
Mixed B-MethodName
VAT I-MethodName
[ O
31 O
, O
23 O
] O
ULMFiT B-MethodName
[ O
14 O
] O
BERT B-MethodName

Model O
IMDB B-DatasetName
Yelp-2 B-DatasetName
Yelp-5 B-DatasetName
DBpedia B-DatasetName
AG B-DatasetName
Amazon-2 B-DatasetName
Amazon-5 B-DatasetName
CNN B-MethodName
[ O
15 O
] O
DPCNN B-MethodName

[ O
10 O
] O
80.005 B-MetricValue
83.061 B-MetricValue
BERT B-MethodName
[ O
10 O
] O
85.083 B-MetricValue
91.835 B-MetricValue
RoBERTa B-MethodName
[ O
21 O
] O
86.820 B-MetricValue
89.795 B-MetricValue
BERT∗ B-MethodName
[ O
10 O
] O
87.433 B-MetricValue
93.294 B-MetricValue
XLNet B-MethodName
87.926 B-MetricValue
90.689 B-MetricValue
XLNet B-MethodName
89.898‡ B-MetricValue
95.080‡ B-MetricValue
Table O
3 O
: O
Results O
on O
SQuAD B-DatasetName
, O
a O
reading B-TaskName
comprehension B-TaskName
dataset O
. O

[ O
10 O
] O
RoBERTa B-MethodName
[ O
21 O
] O
XLNet B-MethodName
84.1 B-MetricValue
88.9 B-MetricValue
89.7 B-MetricValue
90.9 B-MetricValue
94.6 B-MetricValue
95.1 B-MetricValue
Test O
set O
results O
on O
leaderboard O
( O
single O
model O
, O
as O
of O
Dec O
14 O
, O
2019 O
) O
BERT B-MethodName

SQuAD2.0 B-DatasetName
EM B-MetricName
F1 B-MetricName
Dev O
set O
results O
( O
single O
model O
) O
BERT B-MethodName
[ O
10 O
] O
78.98 B-MetricValue
81.77 B-MetricValue
RoBERTa B-MethodName
[ O
21 O
] O
86.5 B-MetricValue
89.4 B-MetricValue
XLNet B-MethodName
87.9 B-MetricValue
90.6 B-MetricValue
SQuAD1.1 B-DatasetName
EM B-MetricName
F1 B-MetricValue
BERT† B-MethodName

Since O
ALBERT B-MethodName
involves O
increasing O
the O
model O
hidden B-HyperparameterName
size I-HyperparameterName
from O
1024 B-HyperparameterValue
to O
2048/4096 B-HyperparameterValue
and O
thus O
substantially O
increases O
the O
amount O
of O
computation O
in O
terms O
of O
FLOPs O
, O
we O
exclude O
ALBERT B-MethodName
from O
the O
following O
results O
as O
it O
is O
hard O
to O
lead O
to O
scientific O
conclusions O
. O

All O
BERT O
, O
RoBERTa O
, O
and O
XLNet O
results O
are O
obtained O
with O
a O
24 O
- O
layer O
architecture O
with O
similar O
model O
sizes O
( O
aka O
BERT O
- O
Large O
) O
. O

Table O
2 O
: O
Comparison O
with O
state O
- O
of O
- O
the O
- O
art O
results O
on O
the O
test O
set O
of O
RACE B-DatasetName
, O
a O
reading B-TaskName
comprehension B-TaskName
task O
, O
and O
on O
ClueWeb09 B-DatasetName
- O
B O
, O
a O
document B-TaskName
ranking B-TaskName
task O
. O

3.3 O
Comparison O
with O
RoBERTa B-MethodName
: O
Scaling O
Up O
RACE B-DatasetName
GPT O
[ O
28 O
] O
BERT B-MethodName
[ O
25 O
] O

We O
use O
the O
best O
of O
3 O
BERT B-MethodName
variants O
for O
comparison O
; O
i.e. O
, O
the O
original O
BERT B-MethodName
, O
BERT B-MethodName
with O
whole O
word O
masking O
, O
and O
BERT B-MethodName
without O
next O
sentence O
prediction O
. O

Since O
the O
recurrence O
mechanism O
is O
introduced O
, O
we O
use O
a O
bidirectional O
data O
input O
pipeline O
where O
each O
of O
the O
forward O
and O
backward O
directions O
takes O
half B-HyperparameterValue
of O
the O
batch B-HyperparameterName
size I-HyperparameterName
. O

Specifically O
, O
we O
train O
on O
512 O
TPU O
v3 O
chips O
for O
500 B-HyperparameterValue
K I-HyperparameterValue
steps B-HyperparameterName
with O
an O
Adam B-HyperparameterName
weight I-HyperparameterName
decay I-HyperparameterName
optimizer I-HyperparameterName
, O
linear B-HyperparameterName
learning I-HyperparameterName
rate I-HyperparameterName
decay I-HyperparameterName
, O
and O
a O
batch B-HyperparameterName
size I-HyperparameterName
of O
8192 B-HyperparameterValue
, O
which O
takes O
about O
5.5 O
days O
. O

Firstly O
, O
to O
provide O
a O
fair O
comparison O
with O
BERT B-MethodName
( O
section O
3.2 O
) O
, O
we O
also O
trained O
XLNet B-MethodName
- I-MethodName
Large I-MethodName
- I-MethodName
wikibooks I-MethodName
on O
BooksCorpus B-DatasetName
and O
Wikipedia B-DatasetName
only O
, O
where O
we O
reuse O
all O
pretraining O
hyper O
- O
parameters O
as O
in O
the O
original O
BERT B-MethodName
. O

During O
pretraining O
, O
we O
always O
use O
a O
full O
sequence B-HyperparameterName
length I-HyperparameterName
of O
512 B-HyperparameterValue
. O

After O
tokenization O
with O
SentencePiece O
[ O
17 O
] O
, O
we O
obtain O
2.78B O
, O
1.09B O
, O
4.75B O
, O
4.30B O
, O
and O
19.97B O
subword O
pieces O
for O
Wikipedia B-DatasetName
, O
BooksCorpus B-DatasetName
, O
Giga5 B-DatasetName
, O
ClueWeb B-DatasetName
, O
and O
Common B-DatasetName
Crawl I-DatasetName
respectively O
, O
which O
are O
32.89B O
in O
total O
. O

In O
addition O
, O
for O
both O
BERT B-MethodName
and O
XLNet B-MethodName
, O
partial O
prediction O
plays O
a O
role O
of O
reducing O
optimization O
difficulty O
by O
only O
predicting O
tokens O
with O
sufficient O
context O
. O

2.6 O
Discussion O
Comparing O
Eq O
. O
( O
2 O
) O
and O
( O
5 O
) O
, O
we O
observe O
that O
both O
BERT B-MethodName
and O
XLNet B-MethodName
perform O
partial O
prediction O
, O
i.e. O
, O
only O
predicting O
a O
subset O
of O
tokens O
in O
the O
sequence O
. O

we O
follow O
the O
two O
- O
segment O
data O
format O
, O
XLNet B-MethodName
- I-MethodName
Large I-MethodName
does O
not O
use O
the O
objective O
of O
next O
sentence O
prediction O
[ O
10 O
] O
as O
it O
does O
not O
show O
consistent O
improvement O
in O
our O
ablation O
study O
( O
see O
Section O
3.4 O
) O
. O

During O
the O
pretraining O
phase O
, O
following O
BERT B-MethodName
, O
we O
randomly O
sample O
two O
segments O
( O
either O
from O
the O
same O
context O
or O
not O
) O
and O
treat O
the O
concatenation O
of O
two O
segments O
as O
one O
sequence O
to O
perform O
permutation O
language O
modeling O
. O

We O
now O
discuss O
how O
we O
pretrain O
XLNet B-MethodName
to O
model O
multiple O
segments O
in O
the O
autoregressive O
framework O
. O

2.5 O
Modeling O
Multiple O
Segments O
Many O
downstream O
tasks O
have O
multiple O
input O
segments O
, O
e.g. O
, O
a O
question O
and O
a O
context O
paragraph O
in O
question B-TaskName
answering I-TaskName
. O

Finally O
, O
Figure O
1 O
( O
c O
) O
presents O
an O
overview O
of O
the O
proposed O
permutation O
language O
modeling O
with O
two O
- O
stream O
attention O
( O
see O
Appendix O
A.7 O
for O
more O
detailed O
illustration O
) O
. O

While O
the O
permutation O
language O
modeling O
objective O
( O
3 O
) O
has O
several O
benefits O
, O
it O
is O
a O
much O
more O
challenging O
optimization O
problem O
due O
to O
the O
permutation O
and O
causes O
slow O
convergence O
in O
preliminary O
experiments O
. O

, O
M O
, O
the O
two O
streams O
of O
representations O
are O
schematically2 O
updated O
2 O
To O
avoid O
clutter O
, O
we O
omit O
the O
implementation O
details O
including O
multi O
- O
head O
attention O
, O
residual O
connection O
, O
layer O
normalization O
and O
position O
- O
wise O
feed O
- O
forward O
as O
used O
in O
Transformer(-XL B-MethodName
) I-MethodName
. O

While O
the O
idea O
of O
target O
- O
aware O
representations O
removes O
the O
ambiguity O
in O
target O
prediction O
, O
how O
to O
formulate O
gθ O
( O
xz O
< O
t O
, O
zt O
) O
remains O
a O
non O
- O
trivial O
problem O
. O

Two O
- O
Stream O
Self O
- O
Attention O

To O
avoid O
this O
problem O
, O
we O
propose O
to O
re O
- O
parameterize O
the O
next O
- O
token O
distribution O
to O
be O
target O
position O
aware O
: O
 O
exp O
e(x O
) O
> O

While O
the O
permutation O
language O
modeling O
objective O
has O
desired O
properties O
, O
naive O
implementation O
with O
standard O
Transformer O
parameterization O
may O
not O
work O
. O

( O
b O
): O
Query O
stream O
attention O
, O
which O
does O
not O
have O
access O
information O
about O
the O
content O
xzt O
. O

( O
, O
) O
g$ O
( O
, O
) O
h O
' O
Masked O
Two O
- O
stream O
Attention O
K O
, O
V O
( O
, O
) O
( O
, O
) O

Masked O
Two O
- O
stream O
Attention O
( O
, O
) O
g O
) O
Content O
stream O
: O
can O
see O
self O
( O
a O
) O
( O
$ O
) O
h$ O
( O
$ O
) O
( O
$ O
) O
h$ O
g$ O
( O
$ O
) O
g$ O
( O
$ O
) O
h O
' O
( O
$ O
) O
g O
' O
( O
$ O
) O
h O
( O
( O
$ O
) O
g O
( O
( O
$ O
) O
h O
) O
( O
$ O
) O
g O
) O
Query O
stream O
: O
can O
not O
see O
self O
Attention O
Q O
( O
, O
) O
h$ O

2.3 O
Architecture O
: O
Two O
- O
Stream O
Self O
- O
Attention O
for O
Target O
- O
Aware O
Representations O
( O
$ O
) O
h$ O
( O
$ O
) O
x$ O

Since O
the O
same O
model O
parameter O
θ O
is O
shared O
across O
all O
factorization O
orders O
during O
training O
, O
in O
expectation O
, O
xt O
has O
seen O
every O
possible O
element O
xi O
6= O
xt O
in O
the O
sequence O
, O
hence O
being O
able O
to O
capture O
the O
bidirectional O
context O
. O

Then O
, O
our O
proposed O
permutation O
language O
modeling O
objective O
can O
be O
expressed O
as O
follows O
: O
" O
T O
# O
X O
max O

[ O
32 O
] O
, O
we O
propose O
the O
permutation O
language O
modeling O
objective O
that O
not O
only O
retains O
the O
benefits O
of O
AR O
models O
but O
also O
allows O
models O
to O
capture O
bidirectional O
contexts O
. O

Borrowing O
ideas O
from O
orderless B-MethodName
NADE I-MethodName

According O
to O
the O
comparison O
above O
, O
AR O
language O
modeling O
and O
BERT B-MethodName
possess O
their O
unique O
advantages O
over O
the O
other O
. O

2.2 O
Objective O
: O
Permutation O
Language O
Modeling O

• O
Context O
dependency O
: O
The O
AR O
representation O
hθ O
( O
x1 O
: O
t−1 O
) O
is O
only O
conditioned O
on O
the O
tokens O
up O
to O
position O
t O
( O
i.e. O
tokens O
to O
the O
left O
) O
, O
while O
the O
BERT B-MethodName
representation O
Hθ O
( O
x)t O
has O
access O
to O
the O
contextual O
information O
on O
both O
sides O
. O

In O
comparison O
, O
AR O
language O
modeling O
does O
not O
rely O
on O
any O
input O
corruption O
and O
does O
not O
suffer O
from O
this O
issue O
. O

In O
comparison O
, O
the O
AR O
language O
modeling O
objective O
( O
1 O
) O
factorizes O
pθ O
( O
x O
) O
using O
the O
product O
rule O
that O
holds O
universally O
without O
such O
an O
independence O
assumption O
. O

θ O
x0 O
exp O
Hθ O
( O
x̂)t O
e(x O
) O
t=1 O
t=1 O
where O
mt O
= O
1 O
indicates O
xt O
is O
masked O
, O
and O
Hθ O
is O
a O
Transformer O
that O
maps O
a O
length O
- O
T O
text O
sequence O
x O
into O
a O
sequence O
of O
hidden O
vectors O

In O
comparison O
, O
BERT B-MethodName
is O
based O
on O
denoising O
auto O
- O
encoding O
. O

where O
hθ O
( O
x1 O
: O
t−1 O
) O
is O
a O
context O
representation O
produced O
by O
neural O
models O
, O
such O
as O
RNNs O
or O
Transformers O
, O
and O
e(x O
) O
denotes O
the O
embedding O
of O
x. O

Given O
a O
text O
sequence O
x O
= O
[ O
x1 O
, O
· O
· O
· O
, O
xT O
] O
, O
AR O
language O
modeling O
performs O
pretraining O
by O
maximizing O
the O
likelihood O
under O
the O
forward O
autoregressive O
factorization O
: O

2 O
Proposed O
Method O
2.1 O
Background O
In O
this O
section O
, O
we O
first O
review O
and O
compare O
the O
conventional O
AR O
language O
modeling O
and O
BERT B-MethodName
for O
language O
pretraining O
. O

Another O
related O
idea O
is O
to O
perform O
autoregressive O
denoising O
in O
the O
context O
of O
text B-TaskName
generation I-TaskName

Technically O
, O
to O
construct O
a O
valid O
target O
- O
aware O
prediction O
distribution O
, O
XLNet B-MethodName
incorporates O
the O
target O
position O
into O
the O
hidden O
state O
via O
two O
- O
stream O
attention O
while O
previous O
permutation O
- O
based O
AR O
models O
relied O
on O
implicit O
position O
awareness O
inherent O
to O
their O
MLP O
architectures O
. O

Firstly O
, O
previous O
models O
aim O
to O
improve O
density O
estimation O
by O
baking O
an O
“ O
orderless O
” O
inductive O
bias O
into O
the O
model O
while O
XLNet B-MethodName
is O
motivated O
by O
enabling O
AR O
language O
models O
to O
learn O
bidirectional O
contexts O
. O

Empirically O
, O
under O
comparable O
experiment O
setting O
, O
XLNet B-MethodName
consistently O
outperforms O
BERT B-MethodName
[ O
10 O
] O
on O
a O
wide O
spectrum O
of O
problems O
including O
GLUE B-DatasetName
language B-TaskName
understanding I-TaskName
tasks O
, O
reading B-TaskName
comprehension I-TaskName
tasks O
like O
SQuAD B-DatasetName
and O
RACE B-DatasetName
, O
text B-TaskName
classification I-TaskName
tasks O
such O
as O
Yelp B-DatasetName
and O
IMDB B-DatasetName
, O
and O
the O
ClueWeb09 B-DatasetName
- I-DatasetName
B I-DatasetName
document B-TaskName
ranking I-TaskName
task O
. O

The O
idea O
of O
permutation O
- O
based O
AR O
modeling O
has O
been O
explored O
in O
[ O
32 O
, O
12 O
] O
, O
but O
there O
are O
several O
key O
differences O
. O

• O
Naively O
applying O
a O
Transformer(-XL B-TaskName
) I-TaskName
architecture O
to O
permutation O
- O
based O
language O
modeling O
does O
not O
work O
because O
the O
factorization O
order O
is O
arbitrary O
and O
the O
target O
is O
ambiguous O
. O

• O
Inspired O
by O
the O
latest O
advancements O
in O
AR O
language O
modeling O
, O
XLNet B-MethodName
integrates O
the O
segment O
recurrence O
mechanism O
and O
relative O
encoding O
scheme O
of O
Transformer B-MethodName
- I-MethodName
XL I-MethodName
[ O
9 O
] O
into O
pretraining O
, O
which O
empirically O
improves O
the O
performance O
especially O
for O
tasks O
involving O
a O
longer O
text O
sequence O
. O

Faced O
with O
the O
pros O
and O
cons O
of O
existing O
language O
pretraining O
objectives O
, O
in O
this O
work O
, O
we O
propose O
XLNet B-MethodName
, O
a O
generalized O
autoregressive O
method O
that O
leverages O
the O
best O
of O
both O
AR O
language O
modeling O
and O
AE O
while O
avoiding O
their O
limitations O
. O

In O
other O
words O
, O
BERT B-MethodName
assumes O
the O
predicted O
tokens O
are O
independent O
of O
each O
other O
given O
the O
unmasked O
tokens O
, O
which O
is O
oversimplified O
as O
high O
- O
order O
, O
long O
- O
range O
dependency O
is O
prevalent O
in O
natural O
language O
[ O
9 O
] O
. O

Moreover O
, O
since O
the O
predicted O
tokens O
are O
masked O
in O
the O
input O
, O
BERT B-MethodName
is O
not O
able O
to O
model O
the O
joint O
probability O
using O
the O
product O
rule O
as O
in O
AR O
language O
modeling O
. O

As O
an O
immediate O
benefit O
, O
this O
closes O
the O
aforementioned O
bidirectional O
information O
gap O
in O
AR O
language O
modeling O
, O
leading O
to O
improved O
performance O
. O

bidirectional O
contexts O
for O
reconstruction O
. O

Since O
density O
estimation O
is O
not O
part O
of O
the O
objective O
, O
BERT B-MethodName
is O
allowed O
to O
utilize O
∗ O
1 O
Equal O
contribution O
. O

In O
comparison O
, O
AE O
based O
pretraining O
does O
not O
perform O
explicit O
density O
estimation O
but O
instead O
aims O
to O
reconstruct O
the O
original O
data O
from O
corrupted O
input O
. O

This O
results O
in O
a O
gap O
between O
AR O
language O
modeling O
and O
effective O
pretraining O
. O

On O
the O
contrary O
, O
downstream O
language B-TaskName
understanding I-TaskName
tasks O
often O
require O
bidirectional O
context O
information O
. O

Specifically O
, O
given O
a O
text O
sequence O
x O
= O
( O
x1 O
, O
· O
· O
· O
, O
xT O
) O
, O
AR O
language O
QT O
modeling O
factorizes O
the O
likelihood O
into O
a O
forward O
product O
p(x O
) O
= O
t=1 O
p(xt O
| O
x O
< O
t O
) O
or O
a O
backward O
Q1 O
one O
p(x O
) O
= O

AR O
language O
modeling O
seeks O
to O
estimate O
the O
probability O
distribution O
of O
a O
text O
corpus O
with O
an O
autoregressive O
model O
[ O
7 O
, O
27 O
, O
28 O
] O
. O

Among O
them O
, O
autoregressive O
( O
AR O
) O
language O
modeling O
and O
autoencoding O
( O
AE O
) O
have O
been O
the O
two O
most O
successful O
pretraining O
objectives O
. O

1 O
Introduction O
Unsupervised O
representation O
learning O
has O
been O
highly O
successful O
in O
the O
domain O
of O
natural B-TaskName
language I-TaskName
processing I-TaskName
[ O
7 O
, O
22 O
, O
27 O
, O
28 O
, O
10 O
] O
. O

Abstract O
With O
the O
capability O
of O
modeling O
bidirectional O
contexts O
, O
denoising O
autoencoding O
based O
pretraining O
like O
BERT B-MethodName
achieves O
better O
performance O
than O
pretraining O
approaches O
based O
on O
autoregressive O
language O
modeling O
. O

XLNet B-MethodName
: O
Generalized O
Autoregressive O
Pretraining O
for O
Language B-TaskName
Understanding I-TaskName
arXiv:1906.08237v2 O


18 O

The O
dash O
arrows O
indicate O
that O
the O
query O
stream O
can O
not O
access O
the O
token O
( O
content O
) O
at O
the O
same O
position O
, O
but O
only O
the O
location O
information O
. O

g O
( O
% O
) O
' O
' O
h(% O
) O
g O
( O
% O
) O
( O
( O
h#(% O
) O
g O
( O
% O
) O
# O
mem(+ O
) O
x% O
x O
' O
x O
( O
x O
# O
Position-4 O
View O
w O
w O
w O
w O
Position-1 O
View O
Split O
View O
of O
the O
Query O
Stream O
( O
Factorization O
order O
: O
3 O
à O
2 O
à O
4 O
à O
1 O
) O
Figure O
6 O
: O
A O
detailed O
illustration O
of O
the O
query O
stream O
of O
the O
proposed O
objective O
with O
both O
the O
joint O
view O
and O
split O
views O
based O
on O
a O
length-4 B-HyperparameterName
sequence O
under O
the O
factorization B-HyperparameterName
order I-HyperparameterName
[ B-HyperparameterValue
3 I-HyperparameterValue
, I-HyperparameterValue
2 I-HyperparameterValue
, I-HyperparameterValue
4 I-HyperparameterValue
, I-HyperparameterValue
1 I-HyperparameterValue
] I-HyperparameterValue
. O

g O
( O
% O
) O
% O
% O
h(% O
) O

( O
( O
h O
# O
( O
' O
) O
g O
( O
' O
) O
# O
mem(% O
) O
h(% O
) O

% O
% O
h O
( O
' O
) O
g O
( O
' O
) O
' O
' O
h O
( O
' O
) O
g O
( O
' O
) O

% O
% O
w O
w O
w O
w O
h O
( O
' O
) O
g O
( O
' O
) O

g O
( O
% O
) O
' O
' O
h(% O
) O
g O
( O
% O
) O
( O
( O
h#(% O
) O
g O
( O
% O
) O
# O
mem(+ O
) O
x% O
x O
' O
x O
( O
x O
# O
w O
w O
w O
Position-2 O
View O
h O
( O
' O
) O
g O
( O
' O
) O

g O
( O
% O
) O
% O
% O
h(% O
) O

h(% O
) O
h(% O
) O
% O
% O
h(% O
) O
g O
( O
% O
) O
' O
' O
h(% O
) O
g O
( O
% O
) O
( O
( O
h#(% O
) O
g O
( O
% O
) O
# O
mem(+ O
) O
x% O
x O
' O
x O
( O
x O
# O
w O
Position-3 O
View O
h O
( O
' O
) O
g O
( O
' O
) O
' O
' O
h O
( O
' O
) O
g O
( O
' O
) O
( O
( O
h O
# O
( O
' O
) O
g O
( O
' O
) O
# O
mem(% O
) O
h(% O
) O

g O
( O
% O
) O
' O
' O
h(% O
) O
g O
( O
% O
) O
( O
( O
h#(% O
) O
g O
( O
% O
) O
# O
mem(+ O
) O
x% O
x O
' O
x O
( O
x O
# O
w O
w O
w O
w O
h O
( O
' O
) O
g O
( O
' O
) O
% O
% O
h O
( O
' O
) O
g O
( O
' O
) O
' O
' O
h O
( O
' O
) O
g O
( O
' O
) O
( O
( O
h O
# O
( O
' O
) O
g O
( O
' O
) O
# O
mem(% O
) O

g O
( O
% O
) O
% O
% O
h(% O
) O

( O
( O
h O
# O
( O
' O
) O
g O
( O
' O
) O
# O
mem(% O
) O
h(% O
) O

% O
% O
h O
( O
' O
) O
g O
( O
' O
) O
' O
' O
h O
( O
' O
) O
g O
( O
' O
) O

g O
( O
% O
) O
' O
' O
h(% O
) O
g O
( O
% O
) O
( O
( O
h#(% O
) O
g O
( O
% O
) O
# O
mem(+ O
) O
x% O
x O
' O
x O
( O
x O
# O
w O
w O
w O
w O
Joint O
View O
of O
the O
Query O
Stream O
( O
Factorization O
order O
: O
3 O
à O
2 O
à O
4 O
à O
1 O
) O
Split O
View O
h O
( O
' O
) O
g O
( O
' O
) O

g O
( O
% O
) O
% O
% O
h(% O
) O

h O
( O
' O
) O
g O
( O
' O
) O
% O
% O
h O
( O
' O
) O
g O
( O
' O
) O
' O
' O
h O
( O
' O
) O
g O
( O
' O
) O
( O
( O
h O
# O
( O
' O
) O
g O
( O
' O
) O
# O
mem(% O
) O
h(% O
) O

17 O

Note O
that O
if O
we O
ignore O
the O
query O
representation O
, O
the O
computation O
in O
this O
figure O
is O
simply O
the O
standard O
self O
- O
attention O
, O
though O
with O
a O
particular O
attention O
mask O
. O

x O
' O
x O
( O
x O
# O
w O
w O
w O
w O
Position-4 O
View O
w O
w O
w O
w O
Position-1 O
View O
Split O
View O
of O
the O
Content O
Stream O
( O
Factorization O
order O
: O
3 O
à O
2 O
à O
4 O
à O
1 O
) O
Figure O
5 O
: O
A O
detailed O
illustration O
of O
the O
content O
stream O
of O
the O
proposed O
objective O
with O
both O
the O
joint O
view O
and O
split O
views O
based O
on O
a O
length-4 B-HyperparameterName
sequence O
under O
the O
factorization B-HyperparameterName
order I-HyperparameterName
[ B-HyperparameterValue
3 I-HyperparameterValue
, I-HyperparameterValue
2 I-HyperparameterValue
, I-HyperparameterValue
4 I-HyperparameterValue
, I-HyperparameterValue
1 I-HyperparameterValue
] I-HyperparameterValue
. O

x% O

h(% O
) O
g O
( O
% O
) O
% O
% O
h(% O
) O
g O
( O
% O
) O
' O
' O
h(% O
) O
g O
( O
% O
) O
( O
( O
h#(% O
) O
g O
( O
% O
) O
# O
mem(+ O
) O
x% O
x O
' O
x O
( O
x O
# O
mem(+ O
) O

g O
( O
% O
) O
' O
' O
h(% O
) O
g O
( O
% O
) O
( O
( O
h#(% O
) O
g O
( O
% O
) O
# O
mem(% O
) O

h(% O
) O
g O
( O
% O
) O
% O
% O
h(% O
) O

g O
' O
h O
( O
( O
' O
) O
mem(% O
) O

g O
' O
w O
Position-2 O
View O
( O
' O
) O
h O
( O
( O
' O
) O
g O
( O
( O
' O
) O
h O
# O
( O
' O
) O
( O
' O
) O
g O
# O
h% O
( O
' O
) O
g% O
( O
' O
) O
h O
' O
( O
' O
) O

h(% O
) O
h(% O
) O
% O
% O
h(% O
) O
g O
( O
% O
) O
' O
' O
h(% O
) O
g O
( O
% O
) O
( O
( O
h#(% O
) O
g O
( O
% O
) O
# O
mem(+ O
) O
x% O
x O
' O
x O
( O
w O
x O
# O
w O
( O
' O
) O
g O
( O
( O
' O
) O
h O
# O
( O
' O
) O
g O
# O
w O
Position-3 O
View O
( O
' O
) O
h% O
( O
' O
) O
g% O
( O
' O
) O
h O
' O
( O
' O
) O

g O
( O
% O
) O
' O
' O
h(% O
) O
g O
( O
% O
) O
( O
( O
h#(% O
) O
g O
( O
% O
) O
# O
mem(+ O
) O
x% O
x O
' O
x O
( O
x O
# O
w O
w O
w O
w O
h O
( O
' O
) O
g O
( O
' O
) O
% O
% O
h O
( O
' O
) O
g O
( O
' O
) O
' O
' O
h O
( O
' O
) O
g O
( O
' O
) O
( O
( O
h O
# O
( O
' O
) O
g O
( O
' O
) O
# O
mem(% O
) O

g O
( O
% O
) O
% O
% O
h(% O
) O

g O
( O
' O
) O
% O
% O
h O
( O
' O
) O
g O
( O
' O
) O
' O
' O
h O
( O
' O
) O
g O
( O
' O
) O
( O
( O
h O
# O
( O
' O
) O
g O
( O
' O
) O
# O
mem(% O
) O
h(% O
) O

g O
( O
% O
) O
' O
' O
h(% O
) O
g O
( O
% O
) O
( O
( O
h#(% O
) O
g O
( O
% O
) O
# O
mem(+ O
) O
x% O
x O
' O
x O
( O
x O
# O
w O
w O
w O
w O
Joint O
View O
of O
the O
Content O
Stream O
( O
Factorization O
order O
: O
3 O
à O
2 O
à O
4 O
à O
1 O
) O
Split O
View O
h O
( O
' O
) O

g O
( O
% O
) O
% O
% O
h(% O
) O

h O
( O
' O
) O
g O
( O
' O
) O
% O
% O
h O
( O
' O
) O
g O
( O
' O
) O
' O
' O
h O
( O
' O
) O
g O
( O
' O
) O
( O
( O
h O
# O
( O
' O
) O
g O
( O
' O
) O
# O
mem(% O
) O
h(% O
) O

https://openreview.net/forum?id=HJePno0cYm O
16 O

The O
main O
difference O
is O
that O
the O
query O
stream O
can O
not O
do O
self O
- O
attention O
and O
does O
not O
have O
access O
to O
the O
token O
at O
the O
position O
, O
while O
the O
content O
stream O
performs O
normal O
self O
- O
attention O
. O

Moreover O
, O
comparing O
Figure O
5 O
and O
6 O
, O
we O
can O
see O
how O
the O
query O
stream O
and O
the O
content O
stream O
work O
differently O
with O
a O
specific O
permutation O
through O
attention O
masks O
. O

As O
shown O
in O
Figure O
5 O
and O
6 O
, O
given O
the O
current O
position O
zt O
, O
the O
attention O
mask O
is O
decided O
by O
the O
permutation O
( O
or O
factorization O
order O
) O
z O
such O
that O
only O
tokens O
the O
occur O
before O
zt O
in O
the O
permutation O
can O
be O
attended O
; O
i.e. O
, O
positions O
zi O
with O
i O
< O
t. O

A.7 O

but O
with O
different O
factorization O
orders O
. O

x% O
( O
$ O
) O
h$ O
h O
# O
( O
$ O
) O
h% O
x% O
( O
$ O
) O
h O
" O
( O
$ O
) O
( O
$ O
) O
h$ O
h O
# O
( O
$ O
) O
h% O
( O
$ O
) O
h O
" O
( O
$ O
) O
mem O
( O
# O
) O
h O
# O
( O
# O
) O
h$ O
( O
# O
) O
h% O
( O
# O
) O
h O
" O
( O
# O
) O
mem O
( O
# O
) O
h O
# O
( O
# O
) O
h$ O
( O
# O
) O
h% O
( O
# O
) O
h O
" O
mem O
( O
+ O
) O
x O
# O
x$ O
x% O
x O
" O
mem O
( O
+ O
) O
x O
# O
x$ O
x% O
x O
" O
Factorization O
order O
: O
3 O
à O
2 O
à O
4 O
à O
1 O
Factorization O
order O
: O
2 O
à O
4 O
à O
3 O
à O
1 O
x% O
( O
$ O
) O
h$ O
h O
# O
( O
$ O
) O
h% O
( O
# O
) O
x% O
( O
$ O
) O
h O
" O
( O
$ O
) O
( O
$ O
) O
h$ O
h O
# O
( O
$ O
) O
h% O
( O
$ O
) O
h O
" O
( O
$ O
) O
mem O
( O
# O
) O
h O
# O
( O
# O
) O
h$ O
( O
# O
) O
h% O
( O
# O
) O
h O
" O
( O
# O
) O
mem O
( O
+ O
) O
h O
# O
( O
# O
) O
h$ O
( O
# O
) O
h% O
( O
# O
) O
h O
" O
mem O
( O
+ O
) O
x O
# O
x$ O
x% O
x O
" O
mem O
( O
+ O
) O
x O
# O
x$ O
x% O
x O
" O
Factorization O
order O
: O
1 O
à O
4 O
à O
2 O
à O
3 O
( O
# O
) O
Factorization O
order O
: O
4 O
à O
3 O
à O
1 O
à O
2 O
Figure O
4 O
: O
Illustration O
of O
the O
permutation O
language O
modeling O
objective O
for O
predicting O
x3 O
given O
the O
same O
input O
sequence O
x O

15 O

Rows O
and O
columns O
represent O
query O
and O
key O
respectively O
. O

( O
a O
) O
Self O
exclusion O
( O
b O
) O
Relative O
stride O
( O
c O
) O
One O
- O
side O
masked O
Figure O
3 O
: O
Attention O
patterns O
that O
appear O
only O
in O
XLNet B-MethodName
. O

We O
conjecture O
these O
unique O
patterns O
contribute O
to O
the O
performance O
advantage O
of O
XLNet B-MethodName
. O

Note O
that O
all O
these O
three O
unique O
patterns O
involve O
the O
relative O
positions O
rather O
than O
absolute O
ones O
, O
and O
hence O
are O
likely O
enabled O
by O
the O
“ O
relative O
attention O
” O
mechanism O
in O
XLNet B-MethodName
. O

It O
seems O
that O
the O
model O
learns O
not O
to O
attend O
the O
relative O
right O
half O
. O

More O
interestingly O
, O
in O
Fig O
. O
3 O
, O
we O
present O
3 O
patterns O
that O
only O
appear O
in O
XLNet B-MethodName
but O
not O
BERT B-MethodName
: O
( O
a O
) O
The O
self O
- O
exclusion O
pattern O
attends O
to O
all O
other O
tokens O
but O
itself O
, O
probably O
offering O
a O
fast O
way O
to O
gather O
global O
information O
; O
( O
b O
) O
The O
relative O
- O
stride O
pattern O
attends O
to O
positions O
every O
a O
few O
stride O
apart O
relative O
to O
the O
query O
position O
; O
( O
c O
) O
The O
one O
- O
side O
masked O
pattern O
is O
very O
similar O
to O
the O
lower O
- O
left O
part O
of O
Fig O
. O
1-(d O
) O
, O
with O
the O
upper O
- O
right O
triangle O
masked O
out O
. O

Rows O
and O
columns O
represent O
query O
and O
key O
respectively O
. O

( O
a O
) O
Content O
stripes O
( O
b O
) O
Local O
/ O
Self O
focus O
( O
c O
) O
Two O
segments O
( O
d O
) O
Content O
- O
based O
symmetry O
Figure O
2 O
: O
Attention O
patterns O
shared O
by O
XLNet B-MethodName
and O
BERT B-MethodName
. O

Firstly O
, O
we O
found O
4 O
typical O
patterns O
shared O
by O
both O
, O
as O
shown O
in O
Fig O
. O
2 O
. O

A.6 O
Qualitative O
Analysis O
of O
Attention O
Patterns O
We O
compare O
the O
attention O
pattern O
of O
BERT B-MethodName
and O
XLNet B-MethodName
without O
finetuning O
. O

As O
an O
example O
, O
we O
integrate O
Transformer B-MethodName
- I-MethodName
XL I-MethodName
into O
XLNet B-MethodName
to O
demonstrate O
the O
usefulness O
of O
the O
latest O
language O
modeling O
progress O
. O

[ O
4 O
, O
32 O
, O
24 O
] O
, O
language O
modeling O
has O
been O
a O
rapidly O
- O
developing O
research O
area O
[ O
9 O
, O
1 O
, O
3 O
] O
. O

A.5.3 O
Bridging O
the O
Gap O
Between O
Language O
Modeling O
and O
Pretraining O
With O
a O
deep O
root O
in O
density O
estimation4 O

Approaches O
like O
ELMo B-MethodName
[ O
27 O
] O
concatenate O
forward O
and O
backward O
language O
models O
in O
a O
shallow O
manner O
, O
which O
is O
not O
sufficient O
for O
modeling O
deep O
interactions O
between O
the O
two O
directions O
. O

In O
comparison O
, O
XLNet B-MethodName
is O
able O
to O
cover O
all O
dependencies O
in O
expectation O
. O

• O

14 O

More O
formally O
, O
consider O
a O
context O
- O
target O
pair O
( O
x O
, O
U O
): O
• O
If O
U O
6⊆ O
T O
< O
x O
, O
where O
T O
< O
x O
denotes O
the O
tokens O
prior O
to O
x O
in O
the O
original O
sequence O
, O
AR O
language O
modeling O
is O
not O
able O
to O
cover O
the O
dependency O
. O

For O
example O
, O
consider O
a O
span B-TaskName
extraction I-TaskName
question I-TaskName
answering I-TaskName
task O
with O
the O
context O
“ O
Thom O
Yorke O
is O
the O
singer O
of O
Radiohead O
” O
and O
the O
question O
“ O
Who O
is O
the O
singer O
of O
Radiohead O
” O
. O

XLNet B-MethodName
, O
on O
the O
other O
hand O
, O
is O
able O
to O
cover O
both O
in O
expectation O
over O
all O
factorization O
orders O
. O

A.5.2 O
Comparison O
with O
Language O
Modeling O
Borrowing O
examples O
and O
notations O
from O
Section O
A.5.1 O
, O
a O
standard O
AR O
language O
model O
like O
GPT B-MethodName
[ O
28 O
] O
is O
only O
able O
to O
cover O
the O
dependency O
( O
x O
= O
York O
, O
U O
= O
{ O
New O
} O
) O
but O
not O
( O
x O
= O
New O
, O
U O
= O
{ O
York O
} O
) O
. O

In O
other O
words O
, O
the O
XLNet B-MethodName
objective O
contains O
more O
effective O
training O
signals O
, O
which O
empirically O
leads O
to O
better O
performance O
in O
Section O
3 O
. O

As O
a O
result O
, O
XLNet B-MethodName
is O
able O
to O
cover O
more O
dependencies O
than O
BERT B-MethodName
. O

• O
If O
U O
⊆ O
N O
∪ O
T O
< O
x O
and O
U O
∩ O
T O
< O
x O
6= O
∅ O
, O
the O
dependency O
can O
only O
be O
covered O
by O
XLNet B-MethodName
but O
not O
BERT B-MethodName
. O

Given O
the O
definition O
, O
let O
’s O
consider O
two O
cases O
: O
• O
If O
U O
⊆ O
N O
, O
the O
dependency O
( O
x O
, O
U O
) O
is O
covered O
by O
both O
BERT B-MethodName
and O
XLNet B-MethodName
. O

I O
is O
covered O
by O
a O
model O
( O
objective O
) O
if O
U O
⊆ O
Vx O
. O

For O
convenience O
, O
we O
say O
a O
target O
- O
context O
pair O
( O
x O
, O
U O
) O
∈ O

I O
such O
that O
U O
⊆ O
Vx O
, O
then O
the O
loss O
term O
log O
p(x O
| O
Vx O
) O
provides O
a O
training O
signal O
to O
the O
dependency O
between O
x O
and O
U. O

Intuitively O
, O
if O
there O
exists O
a O
target O
- O
context O
pair O
( O
x O
, O
U O
) O
∈ O

Both O
objectives O
consist O
of O
multiple O
loss O
terms O
in O
the O
form O
of O
log O
p(x O
| O
Vx O
) O
. O

x∈T O
x∈T O
where O
T O
< O
x O
denote O
tokens O
in O
T O
that O
have O
a O
factorization O
order O
prior O
to O
x. O

Given O
a O
set O
of O
target O
tokens O
T O
and O
a O
set O
of O
non O
- O
target O
tokens O
N O
= O
x\T O
, O
BERT B-MethodName
and O
XLNet B-MethodName
both O
maximize O
log O
p(T O
| O
N O
) O
but O
with O
different O
formulations O
: O
X O
X O
JBERT O
= O
log O
p(x O
| O
N O
) O
; O
JXLNet O
= O
log O
p(x O
| O
N O
∪ O
T O
< O
x O
) O

Note O
that O
I O
is O
merely O
a O
virtual O
notion O
without O
unique O
ground O
truth O
, O
and O
our O
analysis O
will O
hold O
regardless O
of O
how O
I O
is O
instantiated O
. O

I O
= O
x O
= O
York O
, O
U O
= O
{ O
New O
} O
, O
x O
= O
York O
, O
U O
= O
{ O
city O
} O
, O
x O
= O
York O
, O
U O
= O
{ O
New O
, O
city O
} O
, O
· O
· O
· O
. O

 O
 O

For O
example O
, O
given O
the O
above O
sentence O
, O
the O
pairs O
of O
interest O
I O
could O
be O
instantiated O
as O
: O
n O
o O
 O

Inspired O
by O
previous O
work O
[ O
37 O
] O
, O
given O
a O
sequence O
x O
= O
[ O
x1 O
, O
· O
· O
· O
, O
xT O
] O
, O
we O
define O
a O
set O
of O
target O
- O
context O
pairs O
of O
interest O
, O
I O
= O
{ O
( O
x O
, O
U O
) O
} O
, O
where O
U O
is O
a O
set O
of O
tokens O
in O
x O
that O
form O
a O
context O
of O
x. O
Intuitively O
, O
we O
want O
the O
model O
to O
learn O
the O
dependency O
of O
x O
on O
U O
through O
a O
pretraining O
loss O
term O
log O
p(x O
| O
U O
) O
. O

K O
1e-6 O
1.0 O
Discussion O
and O
Analysis O
Comparison O
with O
BERT B-MethodName
To O
prove O
a O
general O
point O
beyond O
one O
example O
, O
we O
now O
turn O
to O
more O
formal O
expressions O
. O

13 O

A.4.2 O
Hyperparameters O
for O
Finetuning O
The O
hyperparameters O
used O
for O
finetuning O
XLNet B-MethodName
on O
various O
tasks O
are O
shown O
in O
Table O
8 O
. O

The O
hyperparameters O
used O
for O
pretraining O
XLNet B-MethodName
are O
shown O
in O
Table O
7 O
. O

We O
use O
a O
pretrained O
XLNet B-MethodName
to O
extract O
word O
embeddings O
for O
the O
documents O
and O
queries O
without O
finetuning O
, O
and O
employ O
a O
kernel O
pooling O
network O
[ O
36 O
] O
to O
rank O
the O
documents O
. O

Since O
document B-TaskName
ranking I-TaskName
, O
or O
ad B-TaskName
- I-TaskName
hoc I-TaskName
retrieval I-TaskName
, O
mainly O
concerns O
the O
low O
- O
level O
representations O
instead O
of O
high O
- O
level O
semantics O
, O
this O
dataset O
serves O
as O
a O
testbed O
for O
evaluating O
the O
quality O
of O
word O
embeddings O
. O

The O
queries O
were O
created O
by O
the O
TREC O
2009 O
- O
2012 O
Web O
Tracks O
based O
on O
50 O
M O
documents O
and O
the O
task O
is O
to O
rerank O
the O
top O
100 O
documents O
retrieved O
using O
a O
standard O
retrieval O
method O
. O

Following O
the O
setting O
in O
previous O
work O
[ O
8 O
] O
, O
we O
use O
the O
ClueWeb09 B-DatasetName
- I-DatasetName
B I-DatasetName
dataset O
to O
evaluate O
the O
performance O
on O
document B-TaskName
ranking I-TaskName
. O

A.3.5 O
ClueWeb09 B-DatasetName
- I-DatasetName
B I-DatasetName
Dataset O

For O
WNLI B-DatasetName
, O
we O
use O
the O
loss O
described O
in O
[ O
16 O
] O
. O

However O
, O
for O
fair O
comparison O
with O
BERT B-MethodName
, O
our O
result O
on O
the O
QNLI B-DatasetName
dev O
set O
is O
based O
on O
a O
standard O
classification O
paradigm O
. O

For O
QNLI B-DatasetName
, O
we O
employed O
a O
pairwise B-TaskName
relevance I-TaskName
ranking I-TaskName
scheme O
as O
in O
[ O
20 O
] O
for O
our O
test O
set O
submission O
. O

Only O
single O
- O
task O
training O
is O
employed O
for O
the O
four O
large O
datasets O
. O

In O
the O
multi O
- O
task O
setting O
, O
we O
jointly O
train O
an O
XLNet B-MethodName
on O
the O
four O
largest O
datasets O
— O
MNLI B-DatasetName
, O
SST-2 B-DatasetName
, O
QNLI B-DatasetName
, O
and O
QQP B-DatasetName
— O
and O
finetune O
the O
network O
on O
the O
other O
datasets O
. O

In O
Table O
5 O
, O
we O
present O
results O
of O
multiple O
settings O
, O
including O
single O
- O
task O
and O
multi O
- O
task O
, O
as O
well O
as O
single O
models O
and O
ensembles O
. O

The O
test O
set O
labels O
are O
removed O
from O
the O
publicly O
released O
version O
, O
and O
all O
the O
practitioners O
must O
submit O
their O
predictions O
on O
the O
evaluation O
server O
to O
obtain O
test O
set O
results O
. O

The O
GLUE B-DatasetName
dataset O
[ O
34 O
] O
is O
a O
collection O
of O
9 O
natural B-TaskName
language I-TaskName
understanding I-TaskName
tasks O
. O

A.3.4 O
GLUE B-DatasetName
Dataset O

A.3.3 O
Text B-TaskName
classification I-TaskName
Datasets O
Following O
previous O
work O
on O
text B-TaskName
classification I-TaskName
[ O
39 O
, O
23 O
] O
, O
we O
evaluate O
XLNet B-MethodName
on O
the O
following O
benchmarks O
: O
IMDB B-DatasetName
, O
Yelp-2 B-DatasetName
, O
Yelp-5 B-DatasetName
, O
DBpedia B-DatasetName
, O
AG B-DatasetName
, O
Amazon-2 B-DatasetName
, O
and O
Amazon-5 B-DatasetName
. O

12 O

To O
finetune O
an O
XLNet B-MethodName
on O
SQuAD2.0 B-DatasetName
, O
we O
jointly O
apply O
a O
logistic O
regression O
loss O
for O
answerability B-TaskName
prediction I-TaskName
similar O
to O
classification B-TaskName
tasks O
and O
a O
standard O
span O
extraction O
loss O
for O
question B-TaskName
answering I-TaskName
[ O
10 O
] O
. O

SQuAD1.1 B-DatasetName
[ O
30 O
] O
contains O
questions O
that O
always O
have O
a O
corresponding O
answer O
in O
the O
given O
passages O
, O
while O
SQuAD2.0 B-DatasetName
[ O
29 O
] O
introduces O
unanswerable O
questions O
. O

A.3.2 O
SQuAD B-DatasetName
SQuAD B-DatasetName
is O
a O
large O
- O
scale O
reading B-TaskName
comprehension I-TaskName
dataset O
with O
two O
tasks O
. O

We O
use O
a O
sequence B-HyperparameterName
length I-HyperparameterName
of O
512 B-HyperparameterValue
during O
finetuning O
. O

As O
a O
result O
, O
this O
dataset O
serves O
as O
a O
challenging O
benchmark O
for O
long B-TaskName
text I-TaskName
understanding I-TaskName
. O

Moreover O
, O
the O
average O
length O
of O
the O
passages O
in O
RACE B-DatasetName
are O
longer O
than O
300 O
, O
which O
is O
significantly O
longer O
than O
other O
popular O
reading O
comprehension O
datasets O
such O
as O
SQuAD B-DatasetName
[ O
29 O
] O
. O

This O
is O
one O
of O
the O
most O
difficult O
reading B-TaskName
comprehension I-TaskName
datasets O
that O
involve O
challenging O
reasoning O
questions O
. O

The O
RACE B-DatasetName
dataset O
[ O
18 O
] O
contains O
near O
100 O
K O
questions O
taken O
from O
the O
English O
exams O
for O
middle O
and O
high O
school O
Chinese O
students O
in O
the O
age O
range O
between O
12 O
to O
18 O
, O
with O
the O
answers O
generated O
by O
human O
experts O
. O

P O
0 O
> O
( O
M O
) O
x0 O
exp O
e(x O
) O
gzt O
A.3 O
A.3.1 O
Datasets O
RACE B-DatasetName
Dataset O

| O
xz O
< O
t O
) O
= O

t O
t O
t O
Target O
- O
aware O
prediction O
distribution O
: O
 O
 O
( O
M O
) O
exp O
e(x O
) O
> O
gzt O
 O
 O
, O
pθ O
( O
Xzt O
= O
x O

zt O
zt O
z O
< O
t O
t O
 O
 O
 O
gz(m O
) O
= O
LayerNorm O
ĝz(m O
) O
+ O
PosFF O
ĝz(m O
) O

LayerNorm O
g O
+ O
RelAttn O
g O
, O
h̃ O
, O
h O

LayerNorm O
h O
+ O
RelAttn O
h O
, O
h̃ O
, O
h O
zt O
zt O
zt O
z≤t O
 O
 O
 O
( O
m O
) O
( O
m O
) O
h(m O
) O
zt O
= O
LayerNorm O
ĥzt O
+ O
PosFF O
ĥzt O
 O
 O
h O
i O
( O
m−1 O
) O
( O
m−1 O
) O
( O
m−1 O
) O
( O
m−1 O
) O
ĝz(m O
) O
= O

, O
T O
: O
ĥ(m O
) O
= O

For O
the O
Transformer B-MethodName
- I-MethodName
XL I-MethodName
layer O
m O
= O
1 O
, O
· O
· O
· O
, O
M O
, O
attention O
with O
relative O
positional O
encoding O
and O
position O
- O
wise O
feed O
- O
forward O
are O
consecutively O
employed O
to O
update O
the O
represetntations O
: O
 O
 O
h O
i O
( O
m−1 O
) O
( O
m−1 O
) O
( O
m−1 O
) O
( O
m−1 O
) O
∀t O
= O
1 O
, O
. O
. O
. O

, O
T O
: O
ht O
= O
e(xt O
) O
and O
gt O
= O
w O
Cached O
layer O
- O
m O
content O
represetation O
( O
memory O
) O
from O
previous O
segment O
: O
h̃(m O
) O

Initial O
represetation O
: O
∀t O
= O
1 O
, O
. O
. O
. O

References O

ZD O
and O
YY O
were O
supported O
in O
part O
by O
NSF O
under O
the O
grant O
IIS-1546329 O
and O
by O
the O
DOE O
- O
Office O
of O
Science O
under O
the O
grant O
ASCR O
# O
KJ040201 O
. O

ZY O
and O
RS O
were O
supported O
by O
the O
Office O
of O
Naval O
Research O
grant O
N000141812861 O
, O
the O
National O
Science O
Foundation O
( O
NSF O
) O
grant O
IIS1763562 O
, O
the O
Nvidia O
fellowship O
, O
and O
the O
Siebel O
scholarship O
. O

Acknowledgments O
The O
authors O
would O
like O
to O
thank O
Qizhe O
Xie O
and O
Adams O
Wei O
Yu O
for O
providing O
useful O
feedback O
on O
the O
project O
, O
Jamie O
Callan O
for O
providing O
the O
ClueWeb O
dataset O
, O
Youlong O
Cheng O
, O
Yanping O
Huang O
and O
Shibo O
Wang O
for O
providing O
ideas O
to O
improve O
our O
TPU O
implementation O
, O
Chenyan O
Xiong O
and O
Zhuyun O
Dai O
for O
clarifying O
the O
setting O
of O
the O
document B-TaskName
ranking I-TaskName
task O
. O

XLNet B-MethodName
achieves O
substantial O
improvement O
over O
previous O
pretraining O
objectives O
on O
various O
tasks O
. O

Finally O
, O
we O
also O
perform O
a O
qualitative O
study O
of O
the O
attention O
patterns O
, O
which O
is O
included O
in O
Appendix O
A.6 O
due O
to O
page O
limit O
. O

In O
addition O
, O
rows O
6 O
- O
7 O
show O
that O
both O
span O
- O
based O
prediction O
and O
the O
bidirectional O
input O
pipeline O
play O
important O
roles O
in O
XLNet B-MethodName
. O

Moreover O
, O
if O
we O
remove O
the O
memory O
caching O
mechanism O
( O
row O
5 O
) O
, O
the O
performance O
clearly O
drops O
, O
especially O
for O
RACE B-DatasetName
which O
involves O
the O
longest O
context O
among O
the O
4 O
tasks O
. O

K B-HyperparameterName
is O
a O
hyperparameter O
to O
control O
the O
optimization O
difficulty O
( O
see O
Section O
2.3 O
) O
. O

We O
run O
BERT B-MethodName
on O
the O
other O
datasets O
using O
the O
official O
implementation O
and O
the O
same O
hyperparameter O
search O
space O
as O
XLNet B-MethodName
. O

For O
fair O
comparison O
, O
all O
models O
are O
based O
on O
a O
12 B-HyperparameterValue
- O
layer B-HyperparameterName
architecture O
with O
the O
same O
model O
hyper O
- O
parameters O
as O
BERT B-MethodName
- I-MethodName
Base I-MethodName
and O
are O
trained O
on O
only O
Wikipedia B-DatasetName
and O
the O
BooksCorpus B-DatasetName
. O

8 O

• O
The O
importance O
of O
using O
Transformer B-MethodName
- I-MethodName
XL I-MethodName
as O
the O
backbone O
neural O
architecture O
. O

3.4 O
Ablation O
Study O
We O
perform O
an O
ablation O
study O
to O
understand O
the O
importance O
of O
each O
design O
choice O
based O
on O
four O
datasets O
with O
diverse O
characteristics O
. O

• O
For O
classification O
tasks O
that O
already O
have O
abundant O
supervised O
examples O
such O
as O
MNLI B-DatasetName
( O
> O
390 O
K O
) O
, O
Yelp B-DatasetName
( O
> O
560 O
K O
) O
and O
Amazon B-DatasetName
( O
> O
3 O
M O
) O
, O
XLNet B-MethodName
still O
lead O
to O
substantial O
gains O
. O

This O
superiority O
at O
dealing O
with O
longer O
context O
could O
come O
from O
the O
Transformer B-MethodName
- I-MethodName
XL I-MethodName
backbone O
in O
XLNet B-MethodName
. O

• O
For O
explicit O
reasoning O
tasks O
like O
SQuAD B-DatasetName
and O
RACE B-DatasetName
that O
involve O
longer O
context O
, O
the O
performance O
gain O
of O
XLNet B-MethodName
is O
usually O
larger O
. O

The O
upper O
section O
shows O
direct O
comparison O
on O
dev O
data O
and O
the O
lower O
section O
shows O
comparison O
with O
state O
- O
of O
- O
the O
- O
art O
results O
on O
the O
public O
leaderboard O
. O

All O
dev O
results O
are O
the O
median O
of O
10 O
runs O
. O

∗ O
indicates O
using O
ensembles O
, O
and O
† O
denotes O
single O
- O
task O
results O
in O
a O
multi O
- O
task O
row O
. O

‡ O
: O
We O
are O
not O
able O
to O
obtain O
the O
test O
results O
of O
our O
latest O
model O
on O
SQuAD1.1 B-DatasetName
from O
the O
organizers O
after O
submitting O
our O
result O
for O
more O
than O
one O
month O
, O
and O
thus O
report O
the O
results O
of O
an O
older O
version O
for O
the O
SQuAD1.1 B-DatasetName
test O
set O
. O

∗ O
indicates O
ensembles O
. O

† O
marks O
our O
runs O
with O
the O
official O
code O
. O

In O
addition O
, O
we O
make O
two O
more O
interesting O
observations O
: O
3 O
Hyperparameters O
for O
pretraining O
and O
finetuning O
are O
in O
Appendix O
A.4 O
. O
7 O

The O
results O
are O
presented O
in O
Tables O
2 O
( O
reading B-TaskName
comprehension I-TaskName
& O
document B-TaskName
ranking I-TaskName
) O
, O
3 O
( O
question B-TaskName
answering I-TaskName
) O
, O
4 O
( O
text B-TaskName
classification I-TaskName
) O
and O
5 O
( O
natural B-TaskName
language I-TaskName
understanding I-TaskName
) O
, O
where O
XLNet B-MethodName
generally O
outperforms O
BERT B-MethodName
and O
RoBERTa B-MethodName
. O

To O
obtain O
relatively O
fair O
comparison O
with O
RoBERTa B-MethodName
, O
the O
experiment O
in O
this O
section O
is O
based O
on O
full O
data O
and O
reuses O
the O
hyper O
- O
parameters O
of O
RoBERTa B-MethodName
, O
as O
described O
in O
section O
3.1 O
. O

After O
the O
initial O
publication O
of O
our O
manuscript O
, O
a O
few O
other O
pretrained O
models O
were O
released O
such O
as O
RoBERTa B-MethodName
[ O
21 O
] O
and O
ALBERT B-MethodName
[ O
19 O
] O
. O

“ O
Middle O
” O
and O
“ O
High O
” O
in O
RACE O
are O
two O
subsets O
representing O
middle O
and O
high O
school O
difficulty O
levels O
. O

† O
indicates O
our O
implementations O
. O

∗ O
indicates O
using O
ensembles O
. O

As O
we O
can O
see O
, O
trained O
on O
the O
same O
data O
with O
an O
almost O
identical O
training O
recipe O
, O
XLNet B-MethodName
outperforms O
BERT B-MethodName
by O
a O
sizable O
margin O
on O
all O
the O
considered O
datasets O
. O

In O
Table O
1 O
, O
we O
compare O
( O
1 O
) O
best O
performance O
of O
three O
different O
variants O
of O
BERT B-MethodName
and O
( O
2 O
) O
XLNet B-MethodName
trained O
with O
the O
same O
data O
and O
hyperparameters O
. O

Here O
, O
we O
first O
compare O
the O
performance O
of O
BERT B-MethodName
and O
XLNet B-MethodName
in O
a O
fair O
setting O
to O
decouple O
the O
effects O
of O
using O
more O
data O
and O
the O
improvement O
from O
BERT B-MethodName
to O
XLNet B-MethodName
. O

All O
models O
are O
trained O
using O
the O
same O
data O
and O
hyperparameters O
as O
in O
BERT B-MethodName
. O

Detailed O
descriptions O
of O
the O
settings O
for O
all O
the O
datasets O
can O
be O
found O
in O
Appendix O
A.3 O
. O

We O
use O
a O
variety O
of O
natural B-TaskName
language I-TaskName
understanding I-TaskName
datasets O
to O
evaluate O
the O
performance O
of O
our O
method O
. O

[ O
1 O
, O
· O
· O
· O
, O
5 O
] O
, O
and O
then O
randomly O
select O
a O
consecutive O
span O
of O
L O
tokens O
as O
prediction O
targets O
within O
a O
context O
of O
( O
KL O
) O
tokens O
. O

Our O
finetuning O
procedure O
follows O
BERT B-MethodName
[ O
10 O
] O
except O
otherwise O
specified3 O
. O

For O
training O
XLNet B-MethodName
- I-MethodName
Large I-MethodName
, O
we O
set O
the O
partial B-HyperparameterName
prediction I-HyperparameterName
constant I-HyperparameterName
K B-HyperparameterName
as O
6 B-HyperparameterValue
( O
see O
Section O
2.3 O
) O
. O

Finally O
, O
we O
perform O
ablation O
study O
( O
section O
3.4 O
) O
based O
on O
the O
XLNet B-MethodName
- I-MethodName
Base I-MethodName
- I-MethodName
wikibooks I-MethodName
. O

observed O
that O
the O
model O
still O
underfits O
the O
data O
at O
the O
end O
of O
training O
. O

It O
was O
6 O

Then O
, O
we O
scale O
up O
the O
training O
of O
XLNet B-MethodName
- I-MethodName
Large I-MethodName
by O
using O
all O
the O
datasets O
described O
above O
. O

Our O
largest O
model O
XLNet B-MethodName
- O
Large O
has O
the O
same O
architecture O
hyperparameters O
as O
BERT B-MethodName
- I-MethodName
Large I-MethodName
, O
which O
results O
in O
a O
similar O
model O
size O
. O

We O
use O
heuristics O
to O
aggressively O
filter O
out O
short O
or O
low O
- O
quality O
articles O
for O
ClueWeb B-DatasetName
2012 I-DatasetName
- I-DatasetName
B I-DatasetName
and O
Common B-DatasetName
Crawl I-DatasetName
, O
which O
results O
in O
19 O
GB O
and O
110 O
GB O
text O
respectively O
. O

[ O
26 O
] O
, O
ClueWeb B-DatasetName
2012 I-DatasetName
- I-DatasetName
B I-DatasetName
( O
extended O
from O
[ O
5 O
] O
) O
, O
and O
Common B-DatasetName
Crawl I-DatasetName
[ O
6 O
] O
for O
pretraining O
. O

In O
addition O
, O
we O
include O
Giga5 B-DatasetName
( O
16 O
GB O
text O
) O

3 O
Experiments O
3.1 O
Pretraining O
and O
Implementation O
Following O
BERT B-MethodName
[ O
10 O
] O
, O
we O
use O
the O
BooksCorpus B-DatasetName
[ O
40 O
] O
and O
English O
Wikipedia B-DatasetName
as O
part O
of O
our O
pretraining O
data O
, O
which O
have O
13 O
GB O
plain O
text O
combined O
. O

For O
more O
formal O
analysis O
and O
further O
discussion O
, O
please O
refer O
to O
Appendix O
A.5 O
. O

Although O
in O
this O
example O
, O
BERT B-MethodName
learns O
some O
dependency O
pairs O
such O
as O
( O
New O
, O
city O
) O
and O
( O
York O
, O
city O
) O
, O
it O
is O
obvious O
that O
XLNet B-MethodName
always O
learns O
more O
dependency O
pairs O
given O
the O
same O
target O
and O
contains O
“ O
denser O
” O
effective O
training O
signals O
. O

Notice O
that O
XLNet B-MethodName
is O
able O
to O
capture O
the O
dependency O
between O
the O
pair O
( O
New O
, O
York O
) O
, O
which O
is O
omitted O
by O
BERT B-MethodName
. O

| O
New O
, O
is O
a O
city O
) O
. O

In O
this O
case O
, O
BERT B-MethodName
and O
XLNet B-MethodName
respectively O
reduce O
to O
the O
following O
objectives O
: O
JBERT O
= O
log O
p(New O
| O
is O
a O
city O
) O
+ O
log O
p(York O
| O
is O
a O
city O
) O
, O
JXLNet O
= O
log O
p(New O
| O
is O
a O
city O
) O
+ O
log O
p(York O

Also O
suppose O
that O
XLNet B-MethodName
samples O
the O
factorization O
order O
[ O
is O
, O
a O
, O
city O
, O
New O
, O
York O
] O
. O

Suppose O
both O
BERT B-MethodName
and O
XLNet B-MethodName
select O
the O
two O
tokens O
[ O
New O
, O
York O
] O
as O
the O
prediction O
targets O
and O
maximize O
log O
p(New O
York O
| O
is O
a O
city O
) O
. O

To O
better O
understand O
the O
difference O
, O
let O
’s O
consider O
a O
concrete O
example O
[ O
New O
, O
York O
, O
is O
, O
a O
, O
city O
] O
. O

However O
, O
the O
independence O
assumption O
discussed O
in O
Section O
2.1 O
disables O
BERT B-MethodName
to O
model O
dependency O
between O
targets O
. O

This O
is O
a O
necessary O
choice O
for O
BERT B-MethodName
because O
if O
all O
tokens O
are O
masked O
, O
it O
is O
impossible O
to O
make O
any O
meaningful O
predictions O
. O

Second O
, O
it O
opens O
the O
possibility O
of O
finetuning O
on O
tasks O
that O
have O
more O
than O
two O
input O
segments O
, O
which O
is O
not O
possible O
using O
absolute O
segment O
encodings O
. O

First O
, O
the O
inductive O
bias O
of O
relative O
encodings O
improves O
generalization O
[ O
9 O
] O
. O

There O
are O
two O
benefits O
of O
using O
relative O
segment O
encodings O
. O

Finally O
, O
the O
value O
aij O
is O
added O
to O
the O
normal O
attention O
weight O
. O

When O
i O
attends O
to O
j O
, O
the O
segment O
encoding O
sij O
is O
used O
to O
compute O
an O
attention O
weight O
aij O
= O
( O
qi O
+ O
b O
) O
> O
sij O
, O
where O
qi O
is O
the O
query O
vector O
as O
in O
a O
standard O
attention O
operation O
and O
b O
is O
a O
learnable O
head O
- O
specific O
bias O
vector O
. O

This O
is O
consistent O
with O
the O
core O
idea O
of O
relative O
encodings O
; O
i.e. O
, O
only O
modeling O
the O
relationships O
between O
positions O
. O

In O
other O
words O
, O
we O
only O
consider O
whether O
the O
two O
positions O
are O
within O
the O
same O
segment O
, O
as O
opposed O
to O
considering O
which O
specific O
segments O
they O
are O
from O
. O

Given O
a O
pair O
of O
positions O
i O
and O
j O
in O
the O
sequence O
, O
if O
i O
and O
j O
are O
from O
the O
same O
segment O
, O
we O
use O
a O
segment O
encoding O
sij O
= O
s+ O
or O
otherwise O
sij O
= O
s− O
, O
where O
s+ O
and O
s− O
are O
learnable O
model O
parameters O
for O
each O
attention O
head O
. O

Relative O
Segment O
Encodings O
Architecturally O
, O
different O
from O
BERT B-MethodName
that O
adds O
an O
absolute O
segment O
embedding O
to O
the O
word O
embedding O
at O
each O
position O
, O
we O
extend O
the O
idea O
of O
relative O
encodings O
from O
Transformer B-MethodName
- I-MethodName
XL I-MethodName
to O
also O
encode O
the O
segments O
. O

Although O
5 O

[ O
CLS O
, O
A O
, O
SEP O
, O
B O
, O
SEP O
] O
, O
where O
“ O
SEP O
” O
and O
“ O
CLS O
” O
are O
two O
special O
symbols O
and O
“ O
A O
” O
and O
“ O
B O
” O
are O
the O
two O
segments O
. O

Specifically O
, O
the O
input O
to O
our O
model O
is O
the O
same O
as O
BERT B-MethodName
: O

We O
only O
reuse O
the O
memory O
that O
belongs O
to O
the O
same O
context O
. O

The O
query O
stream O
can O
be O
computed O
in O
the O
same O
way O
. O

In O
expectation O
, O
the O
model O
learns O
to O
utilize O
the O
memory O
over O
all O
factorization O
orders O
of O
the O
last O
segment O
. O

This O
allows O
caching O
and O
reusing O
the O
memory O
without O
knowing O
the O
factorization O
order O
of O
the O
previous O
segment O
. O

Thus O
, O
the O
above O
attention O
update O
is O
independent O
of O
z̃ O
once O
the O
representations O
h̃(m O
) O
are O
obtained O
. O

Notice O
that O
positional O
encodings O
only O
depend O
on O
the O
actual O
positions O
in O
the O
original O
sequence O
. O

denotes O
concatenation O
along O
the O
sequence O
dimension O
. O

Then O
, O
for O
the O
next O
segment O
x O
, O
the O
attention O
update O
with O
memory O
can O
be O
written O
as O
h O
i O
( O
m−1 O
) O
h(m O
) O
, O
KV O
= O
h̃(m−1 O
) O
, O
hz(m−1 O
) O
; O
θ O
) O
zt O
← O
Attention(Q O
= O
hzt O
≤t O
where O
[ O
. O
, O
. O
] O

Then O
, O
based O
on O
the O
permutation O
z̃ O
, O
we O
process O
the O
first O
segment O
, O
and O
then O
cache O
the O
obtained O
content O
representations O
h̃(m O
) O
for O
each O
layer O
m. O

+ O
1 O
· O
· O
· O
2 O
T O
] O
respectively O
. O

Let O
z̃ O
and O
z O
be O
permutations O
of O
[ O
1 O
· O
· O
· O
T O
] O
and O
[ O
T O

Without O
loss O
of O
generality O
, O
suppose O
we O
have O
two O
segments O
taken O
from O
a O
long O
sequence O
s O
; O
i.e. O
, O
x̃ O
= O
s1 O
: O
T O
and O
x O
= O
sT O
+1:2 O
T O
. O

Now O
we O
discuss O
how O
to O
integrate O
the O
recurrence O
mechanism O
into O
the O
proposed O
permutation O
setting O
and O
enable O
the O
model O
to O
reuse O
hidden O
states O
from O
previous O
segments O
. O

We O
apply O
relative O
positional O
encodings O
based O
on O
the O
original O
sequence O
as O
discussed O
earlier O
, O
which O
is O
straightforward O
. O

We O
integrate O
two O
important O
techniques O
in O
Transformer B-MethodName
- I-MethodName
XL I-MethodName
, O
namely O
the O
relative O
positional O
encoding O
scheme O
and O
the O
segment O
recurrence O
mechanism O
. O

2.4 O
Incorporating O
Ideas O
from O
Transformer B-MethodName
- I-MethodName
XL I-MethodName
Since O
our O
objective O
function O
fits O
in O
the O
AR O
framework O
, O
we O
incorporate O
the O
state O
- O
of O
- O
the O
- O
art O
AR O
language O
model O
, O
Transformer B-MethodName
- I-MethodName
XL I-MethodName
[ O
9 O
] O
, O
into O
our O
pretraining O
framework O
, O
and O
name O
our O
method O
after O
it O
. O

For O
unselected O
tokens O
, O
their O
query O
representations O
need O
not O
be O
computed O
, O
which O
saves O
speed O
and O
memory O
. O

A O
hyperparameter O
K B-HyperparameterName
is O
used O
such O
that O
about O
1 O
/ O
K O
tokens O
are O
selected O
for O
predictions O
; O
i.e. O
, O
|z| O
/(|z| O
− O
c O
) O
≈ O
K. O

Ez∼ZT O
 O
log O
pθ O
( O
xzt O
| O
xz O
< O
t O
) O
. O
( O
5 O
) O
θ O
t O
= O
c+1 O
Note O
that O
z O
> O
c O
is O
chosen O
as O
the O
target O
because O
it O
possesses O
the O
longest O
context O
in O
the O
sequence O
given O
the O
current O
factorization O
order O
z. O

Ez∼ZT O
log O
pθ O
( O
xz O
> O
c O
| O
xz≤c O
) O
= O

The O
objective O
is O
to O
maximize O
the O
log O
- O
likelihood O
of O
the O
target O
subsequence O
conditioned O
on O
the O
non O
- O
target O
subsequence O
, O
i.e. O
, O
 O
 O
|z| O
h O
i O
X O
max O

Formally O
, O
we O
split O
z O
into O
a O
non O
- O
target O
subsequence O
z≤c O
and O
a O
target O
subsequence O
z O
> O
c O
, O
where O
c O
is O
the O
cutting O
point O
. O

To O
reduce O
the O
optimization O
difficulty O
, O
we O
choose O
to O
only O
predict O
the O
last O
tokens O
in O
a O
factorization O
order O
. O

Partial O
Prediction O

Finally O
, O
( O
M O
) O
we O
can O
use O
the O
last O
- O
layer O
query O
representation O
gzt O
to O
compute O
Eq O
. O
( O
4 O
) O
. O

The O
update O
rule O
of O
the O
content O
representations O
is O
exactly O
the O
same O
as O
the O
standard O
self O
- O
attention O
, O
so O
during O
finetuning O
, O
we O
can O
simply O
drop O
the O
query O
stream O
and O
use O
the O
content O
stream O
as O
a O
normal O
Transformer(-XL B-MethodName
) B-MethodName
. O

where O
Q O
, O
K O
, O
V O
denote O
the O
query O
, O
key O
, O
and O
value O
in O
an O
attention O
operation O
[ O
33 O
] O
. O

← O
Attention(Q O
= O
gz(m−1 O
) O
, O
KV O
= O
hz(m−1 O
) O
; O
θ O
) O
, O
gz(m O
) O
t O
t O
< O
t O
( O
query O
stream O
: O
use O
zt O
but O
can O
not O
see O
xzt O
) O
( O
m−1 O
) O
h(m O
) O
, O
KV O
= O
h(m−1 O
) O
; O
θ O
) O
, O
zt O
← O
Attention(Q O
= O
hzt O
z≤t O
( O
content O
stream O
: O
use O
both O
zt O
and O
xzt O
) O
. O

with O
a O
shared O
set O
of O
parameters O
as O
follows O
( O
illustrated O
in O
Figures O
1 O
( O
a O
) O
and O
( O
b O
) O
): O

4 O

The O
details O
are O
included O
in O
Appendix O
A.2 O
for O
reference O
. O

For O
each O
self O
- O
attention O
layer O
m O
= O
1 O
, O
. O
. O
. O

( O
0 O
) O
Computationally O
, O
the O
first O
layer O
query O
stream O
is O
initialized O
with O
a O
trainable O
vector O
, O
i.e. O
gi O
= O
w O
, O
( O
0 O
) O
while O
the O
content O
stream O
is O
set O
to O
the O
corresponding O
word O
embedding O
, O
i.e. O
hi O
= O
e(xi O
) O
. O

zt O
, O
but O
not O
the O
content O
xzt O
, O
as O
discussed O
above O
. O

This O
representation O
encodes O
both O
the O
context O
and O
xzt O
itself O
. O

To O
resolve O
such O
a O
contradiction O
, O
we O
propose O
to O
use O
two O
sets O
of O
hidden O
representations O
instead O
of O
one O
: O
• O
The O
content O
representation O
hθ O
( O
xz≤t O
) O
, O
or O
abbreviated O
as O
hzt O
, O
which O
serves O
a O
similar O
role O
to O
the O
standard O
hidden O
states O
in O
Transformer O
. O

For O
this O
parameterization O
to O
work O
, O
there O
are O
two O
requirements O
that O
are O
contradictory O
in O
a O
standard O
Transformer O
architecture O
: O
( O
1 O
) O
to O
predict O
the O
token O
xzt O
, O
gθ O
( O
xz O
< O
t O
, O
zt O
) O
should O
only O
use O
the O
position O
zt O
and O
not O
the O
content O
xzt O
, O
otherwise O
the O
objective O
becomes O
trivial O
; O
( O
2 O
) O
to O
predict O
the O
other O
tokens O
xzj O
with O
j O
> O
t O
, O
gθ O
( O
xz O
< O
t O
, O
zt O
) O
should O
also O
encode O
the O
content O
xzt O
to O
provide O
full O
contextual O
information O
. O

Among O
other O
possibilities O
, O
we O
propose O
to O
“ O
stand O
” O
at O
the O
target O
position O
zt O
and O
rely O
on O
the O
position O
zt O
to O
gather O
information O
from O
the O
context O
xz O
< O
t O
through O
attention O
. O

where O
gθ O
( O
xz O
< O
t O
, O
zt O
) O
denotes O
a O
new O
type O
of O
representations O
which O
additionally O
take O
the O
target O
position O
zt O
as O
input O
. O

P O
0 O
> O
x0 O
exp O
( O
e(x O
) O
gθ O
( O
xz O
< O
t O
, O
zt O
) O
) O

| O
xz O
< O
t O
) O
= O

gθ O
( O
xz O
< O
t O
, O
zt O
) O
, O
( O
4 O
) O
pθ O
( O
Xzt O
= O
x O

Consequently O
, O
the O
same O
distribution O
is O
predicted O
regardless O
of O
the O
target O
position O
, O
which O
is O
not O
able O
to O
learn O
useful O
representations O
( O
see O
Appendix O
A.1 O
for O
a O
concrete O
example O
) O
. O

Now O
notice O
that O
the O
representation O
hθ O
( O
xz O
< O
t O
) O
does O
not O
depend O
on O
which O
position O
it O
will O
predict O
, O
i.e. O
, O
the O
value O
of O
zt O
. O

θ O
x0 O
produced O
by O
the O
shared O
Transformer O
network O
after O
proper O
masking O
. O

P O
exp O
e(x0 O
) O
> O
h O
( O
x O
, O
where O
hθ O
( O
xz O
< O
t O
) O
denotes O
the O
hidden O
representation O
of O
xz O
< O
t O
( O
z O
< O
t O
) O
) O

| O
xz O
< O
t O
) O
= O

( O
xz O
< O
t O
) O
) O
x O

hθ O

To O
see O
the O
problem O
, O
assume O
we O
parameterize O
the O
next O
- O
token O
distribution O
pθ O
( O
Xzt O
| O
xz O
< O
t O
) O
using O
the O
standard O
Softmax O
formulation O
, O
i.e. O
, O
pθ O
( O
Xzt O
= O
exp(e(x O
) O
> O

( O
c O
): O
Overview O
of O
the O
permutation B-TaskName
language I-TaskName
modeling I-TaskName
training O
with O
two B-MethodName
- I-MethodName
stream I-MethodName
attention I-MethodName
. O

w O
Sample O
a O
factorization O
order O
: O
3à2à4à1 O
( O
c O
) O
Figure O
1 O
: O
( O
a O
): O
Content B-MethodName
stream I-MethodName
attention I-MethodName
, O
which O
is O
the O
same O
as O
the O
standard O
self B-MethodName
- I-MethodName
attention I-MethodName
. O

g O
' O
h O
( O
( O
, O
) O
g O
( O
( O
, O
) O
h O
) O
( O
, O
) O
g O
) O
e(x$ O
) O
w O
e(x O
' O
) O
( O
b O
) O
w O
e(x O
( O
) O
w O
e(x O
) O
) O

( O
, O
) O
h O
' O
( O
, O
) O
( O
, O
) O
g O
' O
h O
( O
( O
, O
) O
g O
( O
( O
, O
) O
h O
) O

g O
' O
x O
( O
( O
' O
) O
h O
( O
( O
' O
) O
g O
( O
x O
) O
( O
' O
) O
h O
) O
( O
' O
) O
g O
) O
K O
, O
V O
Attention O
Masks O
( O
, O
) O
h$ O
( O
, O
) O
g$ O

g$ O
Attention O
Q O
( O
' O
) O
h$ O
x O
' O
( O
' O
) O
g$ O
( O
' O
) O
h O
' O
( O
' O
) O

To O
provide O
an O
overall O
picture O
, O
we O
show O
an O
example O
of O
predicting O
the O
token O
x3 O
given O
the O
same O
input O
sequence O
x O
but O
under O
different O
factorization O
orders O
in O
the O
Appendix O
A.7 O
with O
Figure O
4 O
. O
3 O

Note O
that O
this O
choice O
is O
necessary O
, O
since O
the O
model O
will O
only O
encounter O
text O
sequences O
with O
the O
natural O
order O
during O
finetuning O
. O

In O
other O
words O
, O
we O
keep O
the O
original O
sequence O
order O
, O
use O
the O
positional O
encodings O
corresponding O
to O
the O
original O
sequence O
, O
and O
rely O
on O
a O
proper O
attention O
mask O
in O
Transformers O
to O
achieve O
permutation O
of O
the O
factorization O
order O
. O

Remark O
on O
Permutation O
The O
proposed O
objective O
only O
permutes O
the O
factorization O
order O
, O
not O
the O
sequence O
order O
. O

Moreover O
, O
as O
this O
objective O
fits O
into O
the O
AR O
framework O
, O
it O
naturally O
avoids O
the O
independence O
assumption O
and O
the O
pretrain O
- O
finetune O
discrepancy O
discussed O
in O
Section O
2.1 O
. O

( O
3 O
) O
θ O
t=1 O
Essentially O
, O
for O
a O
text O
sequence O
x O
, O
we O
sample O
a O
factorization O
order O
z O
at O
a O
time O
and O
decompose O
the O
likelihood O
pθ O
( O
x O
) O
according O
to O
factorization O
order O
. O

Ez∼ZT O
log O
pθ O
( O
xzt O
| O
xz O
< O
t O
) O
. O

We O
use O
zt O
and O
z O
< O
t O
to O
denote O
the O
t O
- O
th O
element O
and O
the O
first O
t−1 O
elements O
of O
a O
permutation O
z O
∈ O
ZT O
. O

, O
T O
] O
. O

To O
formalize O
the O
idea O
, O
let O
ZT O
be O
the O
set O
of O
all O
possible O
permutations O
of O
the O
length O
- O
T O
index O
sequence O
[ O
1 O
, O
2 O
, O
. O
. O
. O

Intuitively O
, O
if O
model O
parameters O
are O
shared O
across O
all O
factorization O
orders O
, O
in O
expectation O
, O
the O
model O
will O
learn O
to O
gather O
information O
from O
all O
positions O
on O
both O
sides O
. O

different O
orders O
to O
perform O
a O
valid O
autoregressive O
factorization O
. O

Specifically O
, O
for O
a O
sequence O
x O
of O
length O
T O
, O
there O
are O
T O
! O

A O
natural O
question O
to O
ask O
is O
whether O
there O
exists O
a O
pretraining O
objective O
that O
brings O
the O
advantages O
of O
both O
while O
avoiding O
their O
weaknesses O
. O

As O
a O
result O
, O
the O
BERT B-MethodName
objective O
allows O
the O
model O
to O
be O
pretrained O
to O
better O
capture O
bidirectional O
context O
. O

Replacing O
[ O
MASK O
] O
with O
original O
tokens O
as O
in O
[ O
10 O
] O
does O
not O
solve O
the O
problem O
because O
original O
tokens O
can O
be O
only O
used O
with O
a O
small O
probability O
— O
otherwise O
Eq O
. O
( O
2 O
) O
will O
be O
trivial O
to O
optimize O
. O

[ O
MASK O
] O
that O
never O
occur O
in O
downstream O
tasks O
, O
which O
creates O
a O
pretrain O
- O
finetune O
discrepancy O
. O

• O
Input O
noise O
: O
The O
input O
to O
BERT B-MethodName
contains O
artificial O
symbols O
like O

The O
pros O
and O
cons O
of O
the O
two O
pretraining O
objectives O
are O
compared O
in O
the O
following O
aspects O
: O
• O
Independence O
Assumption O
: O
As O
emphasized O
by O
the O
≈ O
sign O
in O
Eq O
. O
( O
2 O
) O
, O
BERT B-MethodName
factorizes O
the O
joint O
conditional O
probability O
p(x̄ O
| O
x̂ O
) O
based O
on O
an O
independence O
assumption O
that O
all O
masked O
tokens O
x̄ O
are O
separately O
reconstructed O
. O

[ O
Hθ O
( O
x)1 O
, O
Hθ O
( O
x)2 O
, O
· O
· O
· O
, O
Hθ O
( O
x)T O
] O
. O

Hθ O
( O
x O
) O
= O

Specifically O
, O
for O
a O
text O
sequence O
x O
, O
BERT B-MethodName
first O
constructs O
a O
corrupted O
version O
x̂ O
by O
randomly O
setting O
a O
portion O
( O
e.g. O
15 O
% O
) O
of O
tokens O
in O
x O
to O
a O
special O
symbol O

log O
, O
> O
0 O
x0 O
exp O
( O
hθ O
( O
x1 O
: O
t−1 O
) O
e(x O
) O
) O
t=1 O
t=1 O
2 O
( O
1 O
) O

max O
θ O
T O
X O
T O
X O
 O
exp O
hθ O
( O
x1 O
: O
t−1 O
) O
> O
e(xt O
) O
P O
log O
pθ O
( O
x O
) O
= O
log O
pθ O
( O
xt O
| O
x O
< O
t O
) O
= O

[ O
11 O
] O
, O
which O
only O
considers O
a O
fixed O
order O
though O
. O

Finally O
, O
for O
both O
orderless O
NADE B-MethodName
and O
XLNet B-MethodName
, O
we O
would O
like O
to O
emphasize O
that O
“ O
orderless O
” O
does O
not O
mean O
that O
the O
input O
sequence O
can O
be O
randomly O
permuted O
but O
that O
the O
model O
allows O
for O
different O
factorization O
orders O
of O
the O
distribution O
. O

Related O
Work O

As O
a O
solution O
, O
we O
propose O
to O
reparameterize O
the O
Transformer(-XL B-MethodName
) I-MethodName
network O
to O
remove O
the O
ambiguity O
. O

In O
addition O
to O
a O
novel O
pretraining O
objective O
, O
XLNet B-MethodName
improves O
architectural O
designs O
for O
pretraining O
. O

Meanwhile O
, O
the O
autoregressive O
objective O
also O
provides O
a O
natural O
way O
to O
use O
the O
product O
rule O
for O
factorizing O
the O
joint O
probability O
of O
the O
predicted O
tokens O
, O
eliminating O
the O
independence O
assumption O
made O
in O
BERT B-MethodName
. O

Hence O
, O
XLNet B-MethodName
does O
not O
suffer O
from O
the O
pretrain O
- O
finetune O
discrepancy O
that O
BERT B-MethodName
is O
subject O
to O
. O

• O
Secondly O
, O
as O
a O
generalized O
AR O
language O
model O
, O
XLNet B-MethodName
does O
not O
rely O
on O
data O
corruption O
. O

In O
expectation O
, O
each O
position O
learns O
to O
utilize O
contextual O
information O
from O
all O
positions O
, O
i.e. O
, O
capturing O
bidirectional O
context O
. O

Thanks O
to O
the O
permutation O
operation O
, O
the O
context O
for O
each O
position O
can O
consist O
of O
tokens O
from O
both O
left O
and O
right O
. O

all O
possible O
permutations O
of O
the O
factorization O
order O
. O

Firstly O
, O
instead O
of O
using O
a O
fixed O
forward O
or O
backward O
factorization O
order O
as O
in O
conventional O
AR O
models O
, O
XLNet B-MethodName
maximizes O
the O
expected O
log O
likelihood O
of O
a O
sequence O
w.r.t O
. O

• O

However O
, O
the O
artificial O
symbols O
like O
[ O
MASK O
] O
used O
by O
BERT B-MethodName
during O
pretraining O
are O
absent O
from O
real O
data O
at O
finetuning O
time O
, O
resulting O
in O
a O
pretrain O
- O
finetune O
discrepancy O
. O

Pretrained O
models O
and O
code O
are O
available O
at O
https://github.com/zihangdai/xlnet O
33rd O
Conference O
on O
Neural O
Information O
Processing O
Systems O
( O
NeurIPS O
2019 O
) O
, O
Vancouver O
, O
Canada O
. O

Order O
determined O
by O
swapping O
the O
one O
in O
[ O
9 O
] O
. O

Given O
the O
input O
token O
sequence O
, O
a O
certain O
portion O
of O
tokens O
are O
replaced O
by O
a O
special O
symbol O
[ O
MASK O
] O
, O
and O
the O
model O
is O
trained O
to O
recover O
the O
original O
tokens O
from O
the O
corrupted O
version O
. O

[ O
10 O
] O
, O
which O
has O
been O
the O
state O
- O
of O
- O
the O
- O
art O
pretraining O
approach O
. O

A O
notable O
example O
is O
BERT B-MethodName

A O
parametric O
model O
( O
e.g. O
a O
neural O
network O
) O
is O
trained O
to O
model O
each O
conditional O
distribution O
. O

t O
= O
T O
p(xt O
| O
x O
> O
t O
) O
. O

Under O
this O
shared O
high O
- O
level O
idea O
, O
different O
unsupervised O
pretraining O
objectives O
have O
been O
explored O
in O
literature O
. O

Typically O
, O
these O
methods O
first O
pretrain O
neural O
networks O
on O
large O
- O
scale O
unlabeled O
text O
corpora O
, O
and O
then O
finetune O
the O
models O
or O
representations O
on O
downstream O
tasks O
. O

Empirically O
, O
under O
comparable O
experiment O
settings O
, O
XLNet B-MethodName
outperforms O
BERT B-TaskName
on O
20 O
tasks O
, O
often O
by O
a O
large O
margin O
, O
including O
question B-TaskName
answering I-TaskName
, O
natural B-TaskName
language I-TaskName
inference I-TaskName
, O
sentiment B-TaskName
analysis I-TaskName
, O
and O
document B-TaskName
ranking.1 I-TaskName
. O

Furthermore O
, O
XLNet B-MethodName
integrates O
ideas O
from O
Transformer B-MethodName
- I-MethodName
XL I-MethodName
, O
the O
state O
- O
of O
- O
the O
- O
art O
autoregressive O
model O
, O
into O
pretraining O
. O

In O
light O
of O
these O
pros O
and O
cons O
, O
we O
propose O
XLNet B-MethodName
, O
a O
generalized O
autoregressive O
pretraining O
method O
that O
( O
1 O
) O
enables O
learning O
bidirectional O
contexts O
by O
maximizing O
the O
expected O
likelihood O
over O
all O
permutations O
of O
the O
factorization O
order O
and O
( O
2 O
) O
overcomes O
the O
limitations O
of O
BERT B-MethodName
thanks O
to O
its O
autoregressive O
formulation O
. O

However O
, O
relying O
on O
corrupting O
the O
input O
with O
masks O
, O
BERT B-MethodName
neglects O
dependency O
between O
the O
masked O
positions O
and O
suffers O
from O
a O
pretrain O
- O
finetune O
discrepancy O
. O

Yiming O
Yang1 O
, O
Jaime O
Carbonell1 O
, O
Ruslan O
Salakhutdinov1 O
, O
Quoc O
V. O
Le2 O
1 O
Carnegie O
Mellon O
University O
, O
2 O
Google O
AI O
Brain O
Team O
{ O
zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu O
, O
qvl@google.com O

[ O
cs O
. O
CL O
] O
2 O
Jan O
2020 O
Zhilin O
Yang∗1 O
, O
Zihang O
Dai∗12 O
, O

