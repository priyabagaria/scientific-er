-DOCSTART- -X- O
With -X- _ O
such -X- _ O
a -X- _ O
novel -X- _ O
design -X- _ O
, -X- _ O
our -X- _ O
model -X- _ O
is -X- _ O
able -X- _ O
to -X- _ O
deal -X- _ O
with -X- _ O
the -X- _ O
problems -X- _ O
of -X- _ O
saliency -X- _ O
and -X- _ O
redundancy -X- _ O
explicitly -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
introduce -X- _ O
a -X- _ O
decoder -X- _ O
with -X- _ O
a -X- _ O
two -X- _ O
- -X- _ O
level -X- _ O
attention -X- _ O
mechanism -X- _ O
, -X- _ O
which -X- _ O
firstly -X- _ O
attends -X- _ O
to -X- _ O
the -X- _ O
entity -X- _ O
nodes -X- _ O
, -X- _ O
where -X- _ O
the -X- _ O
attention -X- _ O
weights -X- _ O
are -X- _ O
then -X- _ O
subsequently -X- _ O
utilized -X- _ O
to -X- _ O
guide -X- _ O
the -X- _ O
attention -X- _ O
to -X- _ O
the -X- _ O
text -X- _ O
units -X- _ O
. -X- _ O

We -X- _ O
introduce -X- _ O
entity -X- _ O
nodes -X- _ O
in -X- _ O
addition -X- _ O
to -X- _ O
text -X- _ O
unit -X- _ O
nodes -X- _ O
to -X- _ O
construct -X- _ O
a -X- _ O
heterogeneous -X- _ O
graph -X- _ O
, -X- _ O
helping -X- _ O
our -X- _ O
model -X- _ O
capture -X- _ O
complicated -X- _ O
relations -X- _ O
between -X- _ O
text -X- _ O
units -X- _ O
. -X- _ O

Conclusion -X- _ O
In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
an -X- _ O
entity -X- _ O
- -X- _ O
aware -X- _ O
multidocument -X- _ B-TaskName
summarization -X- _ I-TaskName
model -X- _ O
. -X- _ O

Part -X- _ O
of -X- _ O
this -X- _ O
work -X- _ O
was -X- _ O
done -X- _ O
when -X- _ O
Wei -X- _ O
Lu -X- _ O
was -X- _ O
a -X- _ O
visiting -X- _ O
Professor -X- _ O
at -X- _ O
SJTU -X- _ O
. -X- _ O

This -X- _ O
research -X- _ O
work -X- _ O
was -X- _ O
supported -X- _ O
by -X- _ O
the -X- _ O
National -X- _ O
Natural -X- _ O
Science -X- _ O
Foundation -X- _ O
of -X- _ O
China -X- _ O
( -X- _ O
Grant -X- _ O
No.61772337 -X- _ O
, -X- _ O
U1736207 -X- _ O
) -X- _ O
. -X- _ O

5 -X- _ O
Acknowledgments -X- _ O
We -X- _ O
thank -X- _ O
all -X- _ O
the -X- _ O
anonymous -X- _ O
reviewers -X- _ O
for -X- _ O
their -X- _ O
valuable -X- _ O
suggestions -X- _ O
. -X- _ O

These -X- _ O
results -X- _ O
show -X- _ O
that -X- _ O
our -X- _ O
EMSum -X- _ B-MethodName
model -X- _ O
is -X- _ O
able -X- _ O
to -X- _ O
generate -X- _ O
summaries -X- _ O
of -X- _ O
higher -X- _ O
quality -X- _ O
than -X- _ O
other -X- _ O
models -X- _ O
and -X- _ O
further -X- _ O
show -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
our -X- _ O
proposed -X- _ O
approach -X- _ O
. -X- _ O

The -X- _ O
results -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
5 -X- _ O
. -X- _ O

On -X- _ O
the -X- _ O
MultiNews -X- _ B-DatasetName
dataset -X- _ O
, -X- _ O
we -X- _ O
choose -X- _ O
FT -X- _ B-MethodName
, -X- _ O
T -X- _ B-MethodName
- -X- _ I-MethodName
DMCA -X- _ I-MethodName
, -X- _ O
HS -X- _ B-MethodName
, -X- _ O
together -X- _ O
with -X- _ O
EMSum -X- _ B-MethodName
. -X- _ O

On -X- _ O
the -X- _ O
WikiSum -X- _ B-DatasetName
dataset -X- _ O
, -X- _ O
we -X- _ O
choose -X- _ O
FT -X- _ B-MethodName
, -X- _ O
TDMCA -X- _ B-MethodName
, -X- _ O
HT -X- _ B-MethodName
, -X- _ O
EMSum -X- _ B-MethodName
and -X- _ O
conduct -X- _ O
human -X- _ B-MetricName
evaluation -X- _ I-MetricName
to -X- _ O
compare -X- _ O
their -X- _ O
performance -X- _ O
. -X- _ O

Ratings -X- _ O
range -X- _ O
from -X- _ O
-1 -X- _ B-MetricValue
( -X- _ O
worst -X- _ B-MetricValue
) -X- _ O
to -X- _ O
1 -X- _ B-MetricValue
( -X- _ O
best -X- _ B-MetricValue
) -X- _ O
. -X- _ O

The -X- _ O
rating -X- _ O
of -X- _ O
each -X- _ O
system -X- _ O
was -X- _ O
computed -X- _ O
as -X- _ O
the -X- _ O
percentage -X- _ O
of -X- _ O
times -X- _ O
it -X- _ O
was -X- _ O
chosen -X- _ O
as -X- _ O
best -X- _ O
minus -X- _ O
the -X- _ O
times -X- _ O
it -X- _ O
was -X- _ O
selected -X- _ O
as -X- _ O
worst -X- _ O
. -X- _ O

Annotators -X- _ O
are -X- _ O
presented -X- _ O
with -X- _ O
the -X- _ O
gold -X- _ O
summary -X- _ O
and -X- _ O
summaries -X- _ O
generated -X- _ O
from -X- _ O
3 -X- _ O
out -X- _ O
of -X- _ O
4 -X- _ O
systems -X- _ O
and -X- _ O
decide -X- _ O
which -X- _ O
summary -X- _ O
is -X- _ O
the -X- _ O
best -X- _ O
and -X- _ O
which -X- _ O
is -X- _ O
the -X- _ O
worst -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
criteria -X- _ O
mentioned -X- _ O
above -X- _ O
. -X- _ O

We -X- _ O
used -X- _ O
BestWorst -X- _ B-MetricName
Scaling -X- _ I-MetricName
( -X- _ O
Louviere -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
because -X- _ O
it -X- _ O
has -X- _ O
been -X- _ O
shown -X- _ O
to -X- _ O
produce -X- _ O
more -X- _ O
reliable -X- _ O
results -X- _ O
than -X- _ O
rating -X- _ O
scales -X- _ O
( -X- _ O
Kiritchenko -X- _ O
and -X- _ O
Mohammad -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

( -X- _ O
3 -X- _ O
) -X- _ O
Succinctness -X- _ O
: -X- _ O
does -X- _ O
redundancy -X- _ O
occur -X- _ O
in -X- _ O
the -X- _ O
summary -X- _ O
? -X- _ O

( -X- _ O
2 -X- _ O
) -X- _ O
Fluency -X- _ O
: -X- _ O
Is -X- _ O
the -X- _ O
summary -X- _ O
fluent -X- _ O
and -X- _ O
grammatical -X- _ O
? -X- _ O

parts -X- _ O
of -X- _ O
the -X- _ O
input -X- _ O
? -X- _ O

We -X- _ O
would -X- _ O
also -X- _ O
like -X- _ O
to -X- _ O
apply -X- _ O
our -X- _ O
method -X- _ O
to -X- _ O
other -X- _ O
tasks -X- _ O
such -X- _ O
as -X- _ O
multidocument -X- _ O
question -X- _ O
answering -X- _ O
( -X- _ O
Joshi -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
future -X- _ O
, -X- _ O
we -X- _ O
would -X- _ O
like -X- _ O
to -X- _ O
explore -X- _ O
other -X- _ O
approaches -X- _ O
such -X- _ O
as -X- _ O
reinforcement -X- _ O
learning -X- _ O
based -X- _ O
methods -X- _ O
( -X- _ O
Sharma -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
to -X- _ O
further -X- _ O
improve -X- _ O
the -X- _ O
summary -X- _ O
quality -X- _ O
in -X- _ O
the -X- _ O
context -X- _ O
of -X- _ O
multidocument -X- _ B-TaskName
summarization -X- _ I-TaskName
. -X- _ O

Experiments -X- _ O
on -X- _ O
standard -X- _ O
datasets -X- _ O
show -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
. -X- _ O

358 -X- _ O

Following -X- _ O
criteria -X- _ O
used -X- _ O
by -X- _ O
previous -X- _ O
work -X- _ O
( -X- _ O
Liu -X- _ O
and -X- _ O
Lapata -X- _ O
, -X- _ O
2019a -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
evaluation -X- _ O
score -X- _ O
takes -X- _ O
three -X- _ O
aspects -X- _ O
into -X- _ O
account -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
Informativeness -X- _ O
: -X- _ O
does -X- _ O
the -X- _ O
summary -X- _ O
include -X- _ O
salient -X- _ O

We -X- _ O
randomly -X- _ O
sampled -X- _ O
20 -X- _ O
documents -X- _ O
- -X- _ O
summary -X- _ O
pairs -X- _ O
from -X- _ O
the -X- _ O
WikiSum -X- _ B-DatasetName
test -X- _ O
set -X- _ O
and -X- _ O
20 -X- _ O
from -X- _ O
the -X- _ O
MultiNews -X- _ B-DatasetName
test -X- _ O
set -X- _ O
, -X- _ O
and -X- _ O
invited -X- _ O
3 -X- _ O
participants -X- _ O
to -X- _ O
assess -X- _ O
the -X- _ O
outputs -X- _ O
of -X- _ O
different -X- _ O
models -X- _ O
independently -X- _ O
. -X- _ O

4.5 -X- _ O
Human -X- _ B-MetricName
Evaluation -X- _ I-MetricName
We -X- _ O
further -X- _ O
employ -X- _ O
human -X- _ B-MetricName
evaluation -X- _ I-MetricName
to -X- _ O
assess -X- _ O
model -X- _ O
performance -X- _ O
. -X- _ O

Incorporating -X- _ O
entity -X- _ O
information -X- _ O
to -X- _ O
construct -X- _ O
a -X- _ O
heterogeneous -X- _ O
graph -X- _ O
network -X- _ O
enables -X- _ O
better -X- _ O
information -X- _ O
flowing -X- _ O
between -X- _ O
text -X- _ O
nodes -X- _ O
, -X- _ O
and -X- _ O
our -X- _ O
design -X- _ O
of -X- _ O
the -X- _ O
novel -X- _ O
two -X- _ O
- -X- _ O
level -X- _ O
attention -X- _ O
mechanism -X- _ O
in -X- _ O
this -X- _ O
task -X- _ O
is -X- _ O
indeed -X- _ O
playing -X- _ O
an -X- _ O
important -X- _ O
role -X- _ O
towards -X- _ O
the -X- _ O
overall -X- _ O
effectiveness -X- _ O
of -X- _ O
our -X- _ O
approach -X- _ O
. -X- _ O

The -X- _ O
results -X- _ O
show -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
our -X- _ O
new -X- _ O
introduced -X- _ O
module -X- _ O
. -X- _ O

Table -X- _ O
4 -X- _ O
shows -X- _ O
the -X- _ O
results -X- _ O
. -X- _ O

For -X- _ O
experiments -X- _ O
without -X- _ O
two -X- _ O
- -X- _ O
level -X- _ O
attention -X- _ O
, -X- _ O
we -X- _ O
apply -X- _ O
token -X- _ O
- -X- _ O
level -X- _ O
attention -X- _ O
directly -X- _ O
, -X- _ O
but -X- _ O
attend -X- _ O
to -X- _ O
the -X- _ O
entity -X- _ O
cluster -X- _ O
representation -X- _ O
additionally -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
a -X- _ O
naive -X- _ O
way -X- _ O
to -X- _ O
incorporate -X- _ O
entity -X- _ O
information -X- _ O
. -X- _ O

For -X- _ O
experiments -X- _ O
without -X- _ O
graph -X- _ O
encoder -X- _ O
module -X- _ O
, -X- _ O
we -X- _ O
simply -X- _ O
fix -X- _ O
the -X- _ O
entity -X- _ O
cluster -X- _ O
representation -X- _ O
and -X- _ O
paragraph -X- _ O
representation -X- _ O
after -X- _ O
the -X- _ O
multi -X- _ O
- -X- _ O
head -X- _ O
pooling -X- _ O
layer -X- _ O
. -X- _ O

Ablation -X- _ O
Study -X- _ O
To -X- _ O
validate -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
individual -X- _ O
components -X- _ O
such -X- _ O
as -X- _ O
graph -X- _ O
encoder -X- _ O
module -X- _ O
and -X- _ O
two -X- _ O
- -X- _ O
level -X- _ O
attention -X- _ O
module -X- _ O
, -X- _ O
we -X- _ O
conduct -X- _ O
experiments -X- _ O
of -X- _ O
ablation -X- _ O
studies -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
we -X- _ O
choose -X- _ O
k -X- _ B-HyperparameterName
= -X- _ O
10 -X- _ B-HyperparameterValue
because -X- _ O
it -X- _ O
performs -X- _ O
the -X- _ O
best -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
conduct -X- _ O
ablation -X- _ O
studies -X- _ O
to -X- _ O
validate -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
different -X- _ O
components -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
. -X- _ O

We -X- _ O
further -X- _ O
conduct -X- _ O
experiments -X- _ O
to -X- _ O
analyze -X- _ O
the -X- _ O
effects -X- _ O
of -X- _ O
the -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
iterations -X- _ I-HyperparameterName
and -X- _ O
the -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
paragraphs -X- _ I-HyperparameterName
selected -X- _ O
for -X- _ O
attention -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
the -X- _ O
results -X- _ O
show -X- _ O
us -X- _ O
that -X- _ O
t -X- _ B-HyperparameterName
= -X- _ O
3 -X- _ B-HyperparameterValue
, -X- _ O
4 -X- _ B-HyperparameterValue
outperforms -X- _ O
t -X- _ B-HyperparameterName
= -X- _ O
2 -X- _ B-HyperparameterValue
on -X- _ O
ROUGE -X- _ B-MetricName
- -X- _ I-MetricName
L -X- _ I-MetricName
and -X- _ O
the -X- _ O
overall -X- _ O
performance -X- _ O
R̃ -X- _ B-MetricName
fluctuates -X- _ O
very -X- _ O
little -X- _ O
. -X- _ O

R̃ -X- _ B-MetricName
is -X- _ O
the -X- _ O
mean -X- _ O
of -X- _ O
R-1 -X- _ B-MetricName
, -X- _ O
R-2 -X- _ B-MetricName
and -X- _ O
R -X- _ B-MetricName
- -X- _ I-MetricName
L. -X- _ I-MetricName
Model -X- _ O
EMSum -X- _ B-MethodName
w/o -X- _ O
graph -X- _ O
enc -X- _ O
w/o -X- _ O
two -X- _ O
- -X- _ O
level -X- _ O
attn -X- _ O
Analysis -X- _ O

When -X- _ O
k -X- _ B-HyperparameterName
= -X- _ O
20 -X- _ B-HyperparameterValue
, -X- _ O
that -X- _ O
means -X- _ O
we -X- _ O
do -X- _ O
not -X- _ O
perform -X- _ O
any -X- _ O
cut -X- _ O
- -X- _ O
off -X- _ O
but -X- _ O
only -X- _ O
modify -X- _ O
the -X- _ O
paragraph -X- _ O
attention -X- _ O
weights -X- _ O
with -X- _ O
the -X- _ O
entity -X- _ O
attention -X- _ O
weights -X- _ O
, -X- _ O
so -X- _ O
the -X- _ O
performance -X- _ O
is -X- _ O
also -X- _ O
reduced -X- _ O
. -X- _ O

As -X- _ O
the -X- _ O
results -X- _ O
in -X- _ O
the -X- _ O
second -X- _ O
block -X- _ O
of -X- _ O
Table -X- _ O
3 -X- _ O
show -X- _ O
, -X- _ O
when -X- _ O
k -X- _ B-HyperparameterName
= -X- _ O
5 -X- _ B-HyperparameterValue
, -X- _ O
the -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
attended -X- _ I-HyperparameterName
paragraphs -X- _ I-HyperparameterName
is -X- _ O
relatively -X- _ O
small -X- _ O
thereby -X- _ O
degrading -X- _ O
performance -X- _ O
heavily -X- _ O
. -X- _ O

We -X- _ O
conduct -X- _ O
experiments -X- _ O
on -X- _ O
WikiSum -X- _ B-DatasetName
dataset -X- _ O
when -X- _ O
k -X- _ B-HyperparameterName
= -X- _ O
5 -X- _ B-HyperparameterValue
, -X- _ O
10 -X- _ B-HyperparameterValue
, -X- _ O
15 -X- _ B-HyperparameterValue
, -X- _ O
20 -X- _ B-HyperparameterValue
. -X- _ O

So -X- _ O
we -X- _ O
need -X- _ O
to -X- _ O
figure -X- _ O
out -X- _ O
how -X- _ O
much -X- _ O
salient -X- _ O
information -X- _ O
is -X- _ O
enough -X- _ O
for -X- _ O
our -X- _ O
model -X- _ O
, -X- _ O
namely -X- _ O
the -X- _ O
proper -X- _ O
value -X- _ O
of -X- _ O
k. -X- _ B-HyperparameterName

The -X- _ O
attention -X- _ O
weights -X- _ O
over -X- _ O
the -X- _ O
entire -X- _ O
long -X- _ O
token -X- _ O
sequence -X- _ O
may -X- _ O
be -X- _ O
sparse -X- _ O
. -X- _ O

The -X- _ O
Number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
Paragraphs -X- _ I-HyperparameterName
Selected -X- _ O
for -X- _ O
Attention -X- _ O
At -X- _ O
each -X- _ O
decoding -X- _ O
step -X- _ O
, -X- _ O
our -X- _ O
two -X- _ O
- -X- _ O
level -X- _ O
attention -X- _ O
mechanism -X- _ O
firstly -X- _ O
computes -X- _ O
weights -X- _ O
over -X- _ O
entity -X- _ O
nodes -X- _ O
to -X- _ O
identify -X- _ O
the -X- _ O
most -X- _ O
salient -X- _ O
parts -X- _ O
of -X- _ O
source -X- _ O
documents -X- _ O
. -X- _ O

Therefore -X- _ O
we -X- _ O
choose -X- _ O
t -X- _ B-HyperparameterName
= -X- _ O
2 -X- _ B-HyperparameterValue
finally -X- _ O
. -X- _ O

We -X- _ O
argue -X- _ O
the -X- _ O
performance -X- _ O
is -X- _ O
limited -X- _ O
by -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
introduced -X- _ O
parameters -X- _ O
. -X- _ O

Intuitively -X- _ O
, -X- _ O
the -X- _ O
more -X- _ O
iterations -X- _ O
the -X- _ O
graph -X- _ O
is -X- _ O
updated -X- _ O
, -X- _ O
the -X- _ O
more -X- _ O
information -X- _ O
is -X- _ O
flowed -X- _ O
across -X- _ O
the -X- _ O
nodes -X- _ O
. -X- _ O

The -X- _ O
first -X- _ O
block -X- _ O
in -X- _ O
Table -X- _ O
3 -X- _ O
shows -X- _ O
the -X- _ O
results -X- _ O
. -X- _ O

To -X- _ O
this -X- _ O
end -X- _ O
, -X- _ O
we -X- _ O
conduct -X- _ O
experiments -X- _ O
on -X- _ O
WikiSum -X- _ B-DatasetName
dataset -X- _ O
when -X- _ O
t -X- _ B-HyperparameterName
= -X- _ O
1 -X- _ B-HyperparameterValue
, -X- _ O
2 -X- _ B-HyperparameterValue
, -X- _ O
3 -X- _ B-HyperparameterValue
, -X- _ O
4 -X- _ B-HyperparameterValue
. -X- _ O

The -X- _ O
Number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
Iterations -X- _ I-HyperparameterName
We -X- _ O
investigate -X- _ O
how -X- _ O
the -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
iterations -X- _ I-HyperparameterName
t -X- _ B-HyperparameterName
influences -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
. -X- _ O

Overall -X- _ O
, -X- _ O
the -X- _ O
results -X- _ O
demonstrate -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
on -X- _ O
different -X- _ O
types -X- _ O
of -X- _ O
corpora -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
our -X- _ O
model -X- _ O
still -X- _ O
achieves -X- _ O
higher -X- _ O
ROUGE-1 -X- _ B-MetricName
and -X- _ O
ROUGE-2 -X- _ B-MetricName
scores -X- _ O
than -X- _ O
HeterSumGraph -X- _ B-MethodName
. -X- _ O

HeterSumGraph -X- _ B-MethodName
is -X- _ O
a -X- _ O
extractive -X- _ O
method -X- _ O
so -X- _ O
it -X- _ O
achieves -X- _ O
better -X- _ O
ROUGE -X- _ B-MetricName
- -X- _ I-MetricName
L -X- _ I-MetricName
score -X- _ O
. -X- _ O

We -X- _ O
can -X- _ O
see -X- _ O
that -X- _ O
EMSum -X- _ B-MethodName
outperforms -X- _ O
GraphSum -X- _ B-MethodName
and -X- _ O
EMSum+RoBERTa -X- _ B-MethodName
outperforms -X- _ O
GraphSum+RoBERTa -X- _ B-MethodName
. -X- _ O

The -X- _ O
last -X- _ O
block -X- _ O
shows -X- _ O
the -X- _ O
results -X- _ O
of -X- _ O
our -X- _ O
models -X- _ O
. -X- _ O

We -X- _ O
report -X- _ O
the -X- _ O
results -X- _ O
of -X- _ O
FT -X- _ B-MethodName
, -X- _ O
HT -X- _ B-MethodName
and -X- _ O
GraphSum -X- _ B-MethodName
following -X- _ O
Li -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
second -X- _ O
block -X- _ O
shows -X- _ O
the -X- _ O
abstractive -X- _ O
methods -X- _ O
. -X- _ O

Similarly -X- _ O
, -X- _ O
the -X- _ O
first -X- _ O
block -X- _ O
shows -X- _ O
two -X- _ O
extractive -X- _ O
baselines -X- _ O
LexRank -X- _ B-MethodName
, -X- _ O
and -X- _ O
HeterSumGraph -X- _ B-MethodName
. -X- _ O

Results -X- _ O
on -X- _ O
MultiNews -X- _ B-DatasetName
Table -X- _ O
2 -X- _ O
summarizes -X- _ O
the -X- _ O
evaluation -X- _ O
results -X- _ O
on -X- _ O
the -X- _ O
MultiNews -X- _ B-DatasetName
dataset -X- _ O
. -X- _ O


The -X- _ O
improvements -X- _ O
over -X- _ O
GraphSum+RoBERTa -X- _ B-MethodName
are -X- _ O
0.28 -X- _ B-MetricValue
on -X- _ O
ROUGE-2 -X- _ B-MetricName
and -X- _ O
0.83 -X- _ B-MetricValue
on -X- _ O
ROUGE -X- _ B-MetricName
- -X- _ I-MetricName
L -X- _ I-MetricName
, -X- _ O
also -X- _ O
showing -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
even -X- _ O
in -X- _ O
the -X- _ O
presence -X- _ O
of -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
LMs -X- _ O
. -X- _ O

For -X- _ O
models -X- _ O
combined -X- _ O
with -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
LMs -X- _ O
, -X- _ O
the -X- _ O
results -X- _ O
show -X- _ O
that -X- _ O
EMSum+RoBERTa -X- _ B-MethodName
further -X- _ O
improves -X- _ O
the -X- _ O
summarization -X- _ B-TaskName
performance -X- _ O
on -X- _ O
all -X- _ O
metrics -X- _ O
over -X- _ O
EMSum -X- _ B-MethodName
. -X- _ O

357 -X- _ O

Considering -X- _ O
all -X- _ O
these -X- _ O
three -X- _ O
metrics -X- _ O
together -X- _ O
, -X- _ O
the -X- _ O
results -X- _ O
show -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
. -X- _ O

The -X- _ O
gap -X- _ O
between -X- _ O
EMSum -X- _ B-MethodName
and -X- _ O
GraphSum -X- _ B-MethodName
on -X- _ O
ROUGE-1 -X- _ B-MetricName
score -X- _ O
is -X- _ O
0.23 -X- _ B-MetricValue
( -X- _ O
42.40 -X- _ B-MetricValue
vs -X- _ O
42.63 -X- _ B-MetricValue
) -X- _ O
. -X- _ O

Compared -X- _ O
to -X- _ O
the -X- _ O
reported -X- _ O
results -X- _ O
of -X- _ O
GraphSum -X- _ B-MethodName
( -X- _ O
which -X- _ O
used -X- _ O
top -X- _ O
40 -X- _ O
documents -X- _ O
) -X- _ O
, -X- _ O
EMSum -X- _ B-MethodName
achieves -X- _ O
improvements -X- _ O
on -X- _ O
ROUGE-2 -X- _ B-MetricName
and -X- _ O
ROUGE -X- _ B-MetricName
- -X- _ I-MetricName
L -X- _ I-MetricName
, -X- _ O
even -X- _ O
though -X- _ O
EMSum -X- _ B-MethodName
takes -X- _ O
shorter -X- _ O
source -X- _ O
documents -X- _ O
as -X- _ O
input -X- _ O
. -X- _ O

Our -X- _ O
model -X- _ O
EMSum -X- _ B-MethodName
performs -X- _ O
the -X- _ O
best -X- _ O
under -X- _ O
the -X- _ O
top-20 -X- _ O
setting -X- _ O
. -X- _ O

We -X- _ O
believe -X- _ O
this -X- _ O
is -X- _ O
because -X- _ O
the -X- _ O
lower -X- _ O
- -X- _ O
ranked -X- _ O
paragraphs -X- _ O
can -X- _ O
still -X- _ O
provide -X- _ O
information -X- _ O
anyway -X- _ O
. -X- _ O

The -X- _ O
results -X- _ O
show -X- _ O
that -X- _ O
if -X- _ O
we -X- _ O
limit -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
input -X- _ O
paragraphs -X- _ O
to -X- _ O
20 -X- _ O
, -X- _ O
ROUGE -X- _ B-MetricName
score -X- _ O
of -X- _ O
all -X- _ O
models -X- _ O
will -X- _ O
drop -X- _ O
by -X- _ O
about -X- _ O
2 -X- _ B-MetricValue
points -X- _ I-MetricValue
. -X- _ O

The -X- _ O
last -X- _ O
block -X- _ O
shows -X- _ O
the -X- _ O
results -X- _ O
of -X- _ O
some -X- _ O
abstractive -X- _ O
models -X- _ O
and -X- _ O
our -X- _ O
model -X- _ O
, -X- _ O
but -X- _ O
such -X- _ O
models -X- _ O
are -X- _ O
fed -X- _ O
with -X- _ O
20 -X- _ O
top -X- _ O
- -X- _ O
ranked -X- _ O
paragraphs -X- _ O
as -X- _ O
input -X- _ O
. -X- _ O

We -X- _ O
report -X- _ O
their -X- _ O
results -X- _ O
following -X- _ O
Li -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
second -X- _ O
block -X- _ O
shows -X- _ O
the -X- _ O
results -X- _ O
of -X- _ O
abstractive -X- _ O
models -X- _ O
introduced -X- _ O
in -X- _ O
Section -X- _ O
4.2 -X- _ O
. -X- _ O

The -X- _ O
first -X- _ O
block -X- _ O
shows -X- _ O
the -X- _ O
baseline -X- _ O
model -X- _ O
Lead -X- _ B-MethodName
and -X- _ O
LexRank -X- _ B-MethodName
( -X- _ O
Erkan -X- _ O
and -X- _ O
Radev -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
extractive -X- _ O
methods -X- _ O
. -X- _ O

Results -X- _ O
Results -X- _ O
on -X- _ O
WikiSum -X- _ B-DatasetName
Table -X- _ O
1 -X- _ O
summarizes -X- _ O
the -X- _ O
evaluation -X- _ O
results -X- _ O
on -X- _ O
the -X- _ O
WikiSum -X- _ B-DatasetName
dataset -X- _ O
. -X- _ O

Moreover -X- _ O
, -X- _ O
we -X- _ O
choose -X- _ O
Hierarchical -X- _ B-MethodName
Transformer -X- _ I-MethodName
( -X- _ O
HT -X- _ B-MethodName
) -X- _ O
proposed -X- _ O
by -X- _ O
Liu -X- _ O
and -X- _ O
Lapata -X- _ O
( -X- _ O
2019a -X- _ O
) -X- _ O
, -X- _ O
GraphSum -X- _ B-MethodName
proposed -X- _ O
by -X- _ O
Li -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
HeterSumGraph -X- _ B-MethodName
proposed -X- _ O
by -X- _ O
Wang -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020a -X- _ O
) -X- _ O
for -X- _ O
comparisons -X- _ O
. -X- _ O

They -X- _ O
use -X- _ O
a -X- _ O
Transformer -X- _ O
decoder -X- _ O
but -X- _ O
apply -X- _ O
a -X- _ O
convolutional -X- _ O
layer -X- _ O
to -X- _ O
compress -X- _ O
the -X- _ O
key -X- _ O
and -X- _ O
value -X- _ O
in -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
. -X- _ O

Transformer -X- _ B-MethodName
Decoder -X- _ I-MethodName
with -X- _ I-MethodName
Memory -X- _ I-MethodName
Compressed -X- _ I-MethodName
Attention -X- _ I-MethodName
model -X- _ O
( -X- _ O
T -X- _ B-MethodName
- -X- _ I-MethodName
DMCA -X- _ I-MethodName
) -X- _ O
is -X- _ O
proposed -X- _ O
by -X- _ O
Liu -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
with -X- _ O
the -X- _ O
WikiSum -X- _ B-DatasetName
dataset -X- _ O
. -X- _ O

Flat -X- _ B-MethodName
Transformer -X- _ I-MethodName
( -X- _ O
FT -X- _ B-MethodName
) -X- _ O
is -X- _ O
a -X- _ O
6 -X- _ B-HyperparameterValue
- -X- _ O
layer -X- _ B-HyperparameterName
encoder -X- _ O
- -X- _ O
decoder -X- _ O
model -X- _ O
. -X- _ O

The -X- _ O
title -X- _ O
and -X- _ O
ranked -X- _ O
paragraphs -X- _ O
were -X- _ O
concatenated -X- _ O
and -X- _ O
truncated -X- _ O
to -X- _ O
800 -X- _ B-HyperparameterValue
tokens -X- _ O
. -X- _ O

We -X- _ O
choose -X- _ O
a -X- _ O
series -X- _ O
of -X- _ O
Transformer -X- _ O
- -X- _ O
based -X- _ O
models -X- _ O
for -X- _ O
comparison -X- _ O
due -X- _ O
to -X- _ O
their -X- _ O
excellent -X- _ O
performance -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
part -X- _ O
, -X- _ O
the -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
and -X- _ O
warmup -X- _ B-HyperparameterName
steps -X- _ I-HyperparameterName
are -X- _ O
set -X- _ O
as -X- _ O
0.002 -X- _ B-HyperparameterValue
and -X- _ O
20,000 -X- _ B-HyperparameterValue
, -X- _ O
while -X- _ O
for -X- _ O
other -X- _ O
parts -X- _ O
are -X- _ O
0.2 -X- _ B-HyperparameterValue
and -X- _ O
8,000 -X- _ B-HyperparameterValue
, -X- _ O
respectively -X- _ O
. -X- _ O

We -X- _ O
follow -X- _ O
Liu -X- _ O
and -X- _ O
Lapata -X- _ O
( -X- _ O
2019b -X- _ O
) -X- _ O
, -X- _ O
employing -X- _ O
two -X- _ O
Adam -X- _ B-HyperparameterValue
optimizers -X- _ B-HyperparameterName
( -X- _ O
Kingma -X- _ O
and -X- _ O
Ba -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
for -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
part -X- _ O
and -X- _ O
other -X- _ O
parts -X- _ O
, -X- _ O
with -X- _ O
β1 -X- _ B-HyperparameterName
= -X- _ O
0.9 -X- _ B-HyperparameterValue
, -X- _ O
β2 -X- _ B-HyperparameterName
= -X- _ O
0.998 -X- _ B-HyperparameterValue
. -X- _ O

For -X- _ O
models -X- _ O
with -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
LMs -X- _ O
, -X- _ O
we -X- _ O
choose -X- _ O
the -X- _ O
base -X- _ O
version -X- _ O
of -X- _ O
RoBERTa -X- _ O
. -X- _ O

During -X- _ O
decoding -X- _ O
we -X- _ O
apply -X- _ O
beam -X- _ O
search -X- _ O
with -X- _ O
beam -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
5 -X- _ B-HyperparameterValue
and -X- _ O
length -X- _ B-HyperparameterName
penalty -X- _ I-HyperparameterName
( -X- _ O
Wu -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
with -X- _ O
factor -X- _ O
0.4 -X- _ B-HyperparameterValue
. -X- _ O

We -X- _ O
train -X- _ O
our -X- _ O
model -X- _ O
for -X- _ O
200,000 -X- _ B-HyperparameterValue
steps -X- _ B-HyperparameterName
with -X- _ O
gradient -X- _ O
accumulation -X- _ O
every -X- _ O
four -X- _ O
steps -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
dropout -X- _ B-HyperparameterName
with -X- _ I-HyperparameterName
probability -X- _ I-HyperparameterName
0.1 -X- _ B-HyperparameterValue
before -X- _ O
all -X- _ O
linear -X- _ O
layers -X- _ O
and -X- _ O
label -X- _ O
smoothing -X- _ O
( -X- _ O
Szegedy -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
with -X- _ O
smoothing -X- _ B-HyperparameterName
factor -X- _ I-HyperparameterName
0.1 -X- _ B-HyperparameterValue
. -X- _ O

4.3 -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
iterations -X- _ I-HyperparameterName
t -X- _ B-HyperparameterName
= -X- _ O
2 -X- _ B-HyperparameterValue
based -X- _ O
on -X- _ O
the -X- _ O
performance -X- _ O
. -X- _ O

We -X- _ O
have -X- _ O
introduced -X- _ O
them -X- _ O
in -X- _ O
Section -X- _ O
2 -X- _ O
. -X- _ O

We -X- _ O
select -X- _ O
the -X- _ O

In -X- _ O
the -X- _ O
graph -X- _ O
encoding -X- _ O
process -X- _ O
, -X- _ O
each -X- _ O
layer -X- _ O
has -X- _ O
8 -X- _ B-HyperparameterValue
heads -X- _ B-HyperparameterName
and -X- _ O
the -X- _ O
hidden -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
is -X- _ O
256 -X- _ B-HyperparameterValue
. -X- _ O

In -X- _ O
the -X- _ O
multi -X- _ O
- -X- _ O
head -X- _ O
pooling -X- _ O
layer -X- _ O
, -X- _ O
the -X- _ O
number -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
heads -X- _ I-HyperparameterName
is -X- _ O
8 -X- _ B-HyperparameterValue
. -X- _ O

We -X- _ O
truncate -X- _ O
the -X- _ O
length -X- _ B-HyperparameterName
of -X- _ I-HyperparameterName
input -X- _ I-HyperparameterName
paragraphs -X- _ I-HyperparameterName
and -X- _ O
entity -X- _ O
clusters -X- _ O
to -X- _ O
100 -X- _ B-HyperparameterValue
and -X- _ O
50 -X- _ B-HyperparameterValue
tokens -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

We -X- _ O
apply -X- _ O
the -X- _ O
graph -X- _ O
attention -X- _ O
network -X- _ O
( -X- _ O
GAT -X- _ O
) -X- _ O
( -X- _ O
Veličković -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
to -X- _ O
enable -X- _ O
information -X- _ O
flow -X- _ O
between -X- _ O
nodes -X- _ O
and -X- _ O
iteratively -X- _ O
update -X- _ O
the -X- _ O
node -X- _ O
representations -X- _ O
. -X- _ O

Our -X- _ O
model -X- _ O
augments -X- _ O
the -X- _ O
classical -X- _ O
Transformer -X- _ B-MethodName
- -X- _ O
based -X- _ O
encoder -X- _ O
- -X- _ O
decoder -X- _ O
framework -X- _ O
with -X- _ O
a -X- _ O
heterogeneous -X- _ O
graph -X- _ O
consisting -X- _ O
of -X- _ O
text -X- _ O
units -X- _ O
and -X- _ O
entities -X- _ O
as -X- _ O
nodes -X- _ O
, -X- _ O
which -X- _ O
allows -X- _ O
rich -X- _ O
cross -X- _ O
- -X- _ O
document -X- _ O
information -X- _ O
to -X- _ O
be -X- _ O
captured -X- _ O
. -X- _ O

Entity -X- _ O
- -X- _ O
Aware -X- _ O
Abstractive -X- _ O
Multi -X- _ B-TaskName
- -X- _ I-TaskName
Document -X- _ I-TaskName
Summarization -X- _ I-TaskName
Hao -X- _ O
Zhou1 -X- _ O
, -X- _ O
Weidong -X- _ O
Ren1 -X- _ O
, -X- _ O
Gongshen -X- _ O
Liu1∗ -X- _ O
, -X- _ O
Bo -X- _ O
Su1 -X- _ O
, -X- _ O
Wei -X- _ O
Lu2 -X- _ O
1 -X- _ O
School -X- _ O
of -X- _ O
Electronic -X- _ O
Information -X- _ O
and -X- _ O
Electrical -X- _ O
Engineering -X- _ O
, -X- _ O
Shanghai -X- _ O
Jiao -X- _ O
Tong -X- _ O
University -X- _ O
2 -X- _ O
StatNLP -X- _ O
Research -X- _ O
Group -X- _ O
, -X- _ O
Singapore -X- _ O
University -X- _ O
of -X- _ O
Technology -X- _ O
and -X- _ O
Design -X- _ O
{ -X- _ O
zhou1998,renweidong1997,lgshen,subo}@sjtu.edu.cn -X- _ O
luwei@sutd.edu.sg -X- _ O
Abstract -X- _ O
Entities -X- _ O
and -X- _ O
their -X- _ O
mentions -X- _ O
convey -X- _ O
significant -X- _ O
semantic -X- _ O
information -X- _ O
in -X- _ O
documents -X- _ O
. -X- _ O

Hyperparameters -X- _ O
We -X- _ O
set -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
our -X- _ O
vanilla -X- _ O
Transformer -X- _ O
encoding -X- _ O
layers -X- _ O
as -X- _ O
6 -X- _ B-HyperparameterValue
, -X- _ O
the -X- _ O
hidden -X- _ O
size -X- _ O
as -X- _ O
256 -X- _ B-HyperparameterValue
and -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
heads -X- _ O
as -X- _ O
8 -X- _ B-HyperparameterValue
, -X- _ O
while -X- _ O
the -X- _ O
hidden -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ I-HyperparameterName
feed -X- _ I-HyperparameterName
- -X- _ I-HyperparameterName
forward -X- _ I-HyperparameterName
layers -X- _ I-HyperparameterName
is -X- _ O
1,024 -X- _ B-HyperparameterValue
. -X- _ O

356 -X- _ O

We -X- _ O
report -X- _ O
different -X- _ O
versions -X- _ O
of -X- _ O
the -X- _ O
metric -X- _ O
, -X- _ O
based -X- _ O
on -X- _ O
overlaps -X- _ O
of -X- _ O
unigrams -X- _ O
( -X- _ O
ROUGE-1 -X- _ B-MetricName
, -X- _ O
R-1 -X- _ B-MetricName
) -X- _ O
, -X- _ O
bigrams -X- _ O
( -X- _ O
ROUGE-2 -X- _ B-MetricName
, -X- _ O
R-2 -X- _ B-MetricName
) -X- _ O
and -X- _ O
the -X- _ O
longest -X- _ O
common -X- _ O
subsequences -X- _ O
( -X- _ O
ROUGE -X- _ B-MetricName
- -X- _ I-MetricName
L -X- _ I-MetricName
, -X- _ O
R -X- _ B-MetricName
- -X- _ I-MetricName
L -X- _ I-MetricName
) -X- _ O
. -X- _ O

Experiments -X- _ O
We -X- _ O
use -X- _ O
ROUGE -X- _ B-MetricName
scores -X- _ O
to -X- _ O
evaluate -X- _ O
summarization -X- _ B-TaskName
quality -X- _ O
automatically -X- _ O
( -X- _ O
Lin -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
. -X- _ O

For -X- _ O
each -X- _ O
instance -X- _ O
, -X- _ O
we -X- _ O
get -X- _ O
13.3 -X- _ O
clusters -X- _ O
on -X- _ O
average -X- _ O
and -X- _ O
each -X- _ O
cluster -X- _ O
has -X- _ O
9.9 -X- _ O
tokens -X- _ O
on -X- _ O
average -X- _ O
. -X- _ O

Different -X- _ O
from -X- _ O
the -X- _ O
WikiSum -X- _ B-DatasetName
dataset -X- _ O
, -X- _ O
each -X- _ O
source -X- _ O
article -X- _ O
only -X- _ O
contains -X- _ O
2.8 -X- _ O
paragraphs -X- _ O
and -X- _ O
21.6 -X- _ O
sentences -X- _ O
on -X- _ O
average -X- _ O
, -X- _ O
thus -X- _ O
we -X- _ O
choose -X- _ O
to -X- _ O
build -X- _ O
graph -X- _ O
on -X- _ O
sentence -X- _ O
level -X- _ O
rather -X- _ O
than -X- _ O
paragraph -X- _ O
level -X- _ O
for -X- _ O
this -X- _ O
dataset -X- _ O
. -X- _ O

Following -X- _ O
their -X- _ O
experimental -X- _ O
settings -X- _ O
, -X- _ O
we -X- _ O
get -X- _ O
44,972 -X- _ O
instances -X- _ O
for -X- _ O
training -X- _ O
, -X- _ O
5,622 -X- _ O
for -X- _ O
validation -X- _ O
and -X- _ O
5,622 -X- _ O
for -X- _ O
test -X- _ O
. -X- _ O

The -X- _ O
source -X- _ O
articles -X- _ O
come -X- _ O
from -X- _ O
a -X- _ O
diverse -X- _ O
set -X- _ O
of -X- _ O
news -X- _ O
sources -X- _ O
, -X- _ O
over -X- _ O
1,500 -X- _ O
sites -X- _ O
. -X- _ O

MultiNews -X- _ B-DatasetName
Dataset -X- _ O
Introduced -X- _ O
by -X- _ O
Fabbri -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
MultiNews -X- _ B-DatasetName
consists -X- _ O
of -X- _ O
news -X- _ O
articles -X- _ O
and -X- _ O
hand -X- _ O
- -X- _ O
written -X- _ O
summaries -X- _ O
. -X- _ O

For -X- _ O
each -X- _ O
instance -X- _ O
, -X- _ O
we -X- _ O
get -X- _ O
23.7 -X- _ O
clusters -X- _ O
on -X- _ O
average -X- _ O
and -X- _ O
each -X- _ O
cluster -X- _ O
has -X- _ O
10.2 -X- _ O
tokens -X- _ O
on -X- _ O
average -X- _ O
. -X- _ O

We -X- _ O
then -X- _ O
perform -X- _ O
entity -X- _ O
cluster -X- _ O
extraction -X- _ O
on -X- _ O
the -X- _ O
top-20 -X- _ O
WikiSum -X- _ B-DatasetName
dataset -X- _ O
. -X- _ O

On -X- _ O
average -X- _ O
, -X- _ O
each -X- _ O
paragraph -X- _ O
has -X- _ O
70.1 -X- _ O
tokens -X- _ O
, -X- _ O
and -X- _ O
target -X- _ O
sumamry -X- _ O
has -X- _ O
139.4 -X- _ O
tokens -X- _ O
. -X- _ O

We -X- _ O
get -X- _ O
300,000 -X- _ O
instances -X- _ O
for -X- _ O
training -X- _ O
, -X- _ O
38,144 -X- _ O
for -X- _ O
validation -X- _ O
and -X- _ O
38,205 -X- _ O
for -X- _ O
test -X- _ O
. -X- _ O

So -X- _ O
we -X- _ O
choose -X- _ O
to -X- _ O
use -X- _ O
the -X- _ O
top-20 -X- _ O
version -X- _ O
of -X- _ O
WikiSum -X- _ B-DatasetName
dataset -X- _ O
in -X- _ O
order -X- _ O
to -X- _ O
find -X- _ O
a -X- _ O
balance -X- _ O
between -X- _ O
computational -X- _ O
cost -X- _ O
and -X- _ O
the -X- _ O
coverage -X- _ O
of -X- _ O
input -X- _ O
content -X- _ O
. -X- _ O

Experiment -X- _ O
shows -X- _ O
that -X- _ O
the -X- _ O
ROUGE -X- _ O
- -X- _ O
L -X- _ O
recall -X- _ O
of -X- _ O
top-20 -X- _ O
paragraphs -X- _ O
against -X- _ O
the -X- _ O
gold -X- _ O
target -X- _ O
text -X- _ O
is -X- _ O
53.84 -X- _ O
, -X- _ O
and -X- _ O
top-40 -X- _ O
is -X- _ O
60.42 -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
the -X- _ O
top-40 -X- _ O
dataset -X- _ O
is -X- _ O
quite -X- _ O
heavy -X- _ O
for -X- _ O
entity -X- _ O
extraction -X- _ O
and -X- _ O
co -X- _ O
- -X- _ O
reference -X- _ O
resolution -X- _ O
. -X- _ O

They -X- _ O
further -X- _ O
split -X- _ O
the -X- _ O
long -X- _ O
and -X- _ O
messy -X- _ O
source -X- _ O
documents -X- _ O
into -X- _ O
multiple -X- _ O
paragraphs -X- _ O
by -X- _ O
line -X- _ O
- -X- _ O
breaks -X- _ O
and -X- _ O
select -X- _ O
the -X- _ O
top-40 -X- _ O
paragraphs -X- _ O
as -X- _ O
input -X- _ O
for -X- _ O
summarization -X- _ O
systems -X- _ O
. -X- _ O

Liu -X- _ O
and -X- _ O
Lapata -X- _ O
( -X- _ O
2019a -X- _ O
) -X- _ O
crawled -X- _ O
Wikipedia -X- _ O
articles -X- _ O
and -X- _ O
source -X- _ O
reference -X- _ O
documents -X- _ O
through -X- _ O
the -X- _ O
provided -X- _ O
urls -X- _ O
. -X- _ O

WikiSum -X- _ B-DatasetName
Dataset -X- _ O
Liu -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
treat -X- _ O
the -X- _ O
generation -X- _ O
of -X- _ O
Wikipedia -X- _ O
section -X- _ O
titles -X- _ O
as -X- _ O
a -X- _ O
supervised -X- _ O
multi -X- _ B-TaskName
- -X- _ I-TaskName
document -X- _ I-TaskName
summarization -X- _ I-TaskName
task -X- _ O
. -X- _ O

4 -X- _ O
4.1 -X- _ O
Experimental -X- _ O
Setup -X- _ O
We -X- _ O
conduct -X- _ O
experiments -X- _ O
on -X- _ O
two -X- _ O
major -X- _ O
datasets -X- _ O
used -X- _ O
in -X- _ O
the -X- _ O
literature -X- _ O
of -X- _ O
multi -X- _ B-TaskName
- -X- _ I-TaskName
document -X- _ I-TaskName
summarization -X- _ I-TaskName
, -X- _ O
namely -X- _ O
WikiSum -X- _ B-DatasetName
( -X- _ O
Liu -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
and -X- _ O
MultiNews -X- _ B-DatasetName
( -X- _ O
Fabbri -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
we -X- _ O
perform -X- _ O
the -X- _ O
same -X- _ O
multi -X- _ O
- -X- _ O
head -X- _ O
pooling -X- _ O
strategy -X- _ O
to -X- _ O
obtain -X- _ O
paragraph -X- _ O
representations -X- _ O
. -X- _ O

Then -X- _ O
a -X- _ O
single -X- _ O
- -X- _ O
layer -X- _ O
bidirectional -X- _ O
LSTM -X- _ O
is -X- _ O
employed -X- _ O
over -X- _ O
token -X- _ O
embeddings -X- _ O
, -X- _ O
producing -X- _ O
token -X- _ O
features -X- _ O
. -X- _ O

We -X- _ O
feed -X- _ O
input -X- _ O
tokens -X- _ O
to -X- _ O
a -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
language -X- _ O
model -X- _ O
and -X- _ O
take -X- _ O
the -X- _ O
last -X- _ O
layer -X- _ O
output -X- _ O
as -X- _ O
token -X- _ O
embeddings -X- _ O
. -X- _ O

Pre -X- _ O
- -X- _ O
trained -X- _ O
language -X- _ O
models -X- _ O
can -X- _ O
be -X- _ O
more -X- _ O
effective -X- _ O
on -X- _ O
short -X- _ O
inputs -X- _ O
than -X- _ O
training -X- _ O
stacked -X- _ O
transformer -X- _ O
layers -X- _ O
from -X- _ O
scratch -X- _ O
. -X- _ O

3.7 -X- _ O
Pre -X- _ O
- -X- _ O
trained -X- _ O
LMs -X- _ O
as -X- _ O
Document -X- _ O
Encoder -X- _ O
Our -X- _ O
document -X- _ O
encoder -X- _ O
illustrated -X- _ O
in -X- _ O
section -X- _ O
3.3 -X- _ O
can -X- _ O
be -X- _ O
replaced -X- _ O
by -X- _ O
a -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
language -X- _ O
model -X- _ O
such -X- _ O
as -X- _ O
BERT -X- _ O
( -X- _ O
Devlin -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
and -X- _ O
RoBERTa -X- _ O
( -X- _ O
Liu -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O

3.6 -X- _ O
Training -X- _ O
Our -X- _ O
training -X- _ O
process -X- _ O
follows -X- _ O
that -X- _ O
of -X- _ O
the -X- _ O
traditional -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
modeling -X- _ O
, -X- _ O
with -X- _ O
maximum -X- _ O
likelihood -X- _ O
estimation -X- _ O
that -X- _ O
minimizes -X- _ O
: -X- _ O
1 -X- _ O
X -X- _ O
Lseq -X- _ O
= -X- _ O
− -X- _ O
log -X- _ O
p(y|x -X- _ O
; -X- _ O
θ -X- _ O
) -X- _ O
( -X- _ O
17 -X- _ O
) -X- _ O
|D| -X- _ O
( -X- _ O
y -X- _ O
, -X- _ O
x)∈D -X- _ O
where -X- _ O
x -X- _ O
and -X- _ O
y -X- _ O
are -X- _ O
document -X- _ O
- -X- _ O
summary -X- _ O
pairs -X- _ O
from -X- _ O
training -X- _ O
set -X- _ O
D -X- _ O
, -X- _ O
and -X- _ O
θ -X- _ O
are -X- _ O
parameters -X- _ O
to -X- _ O
be -X- _ O
learned -X- _ O
. -X- _ O

We -X- _ O
further -X- _ O
add -X- _ O
a -X- _ O
copy -X- _ O
mechanism -X- _ O
as -X- _ O
proposed -X- _ O
by -X- _ O
See -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
the -X- _ O
weight -X- _ O
- -X- _ O
sharing -X- _ O
strategy -X- _ O
between -X- _ O
the -X- _ O
input -X- _ O
embedding -X- _ O
matrix -X- _ O
and -X- _ O
the -X- _ O
matrix -X- _ O
Wo -X- _ O
to -X- _ O
reuse -X- _ O
linguistic -X- _ O
knowledge -X- _ O
( -X- _ O
Paulus -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

Then -X- _ O
the -X- _ O
context -X- _ O
vector -X- _ O
vt -X- _ O
can -X- _ O
be -X- _ O
computed -X- _ O
by -X- _ O
: -X- _ O
X -X- _ O
vt -X- _ O
= -X- _ O
γ̂wi -X- _ O
h̃wi -X- _ O
( -X- _ O
15 -X- _ O
) -X- _ O
i -X- _ O
Token -X- _ O
Prediction -X- _ O
Context -X- _ O
vectors -X- _ O
, -X- _ O
treated -X- _ O
as -X- _ O
salient -X- _ O
contents -X- _ O
summarized -X- _ O
from -X- _ O
sources -X- _ O
, -X- _ O
are -X- _ O
concatenated -X- _ O
with -X- _ O
the -X- _ O
decoder -X- _ O
hidden -X- _ O
state -X- _ O
st -X- _ O
to -X- _ O
produce -X- _ O
the -X- _ O
vocabulary -X- _ O
distribution -X- _ O
: -X- _ O

For -X- _ O
token -X- _ O
wi -X- _ O
in -X- _ O
paragraph -X- _ O
Pj -X- _ O
, -X- _ O
we -X- _ O
further -X- _ O
modify -X- _ O
γwi -X- _ O
by -X- _ O
γ̂wi -X- _ O
= -X- _ O
βj -X- _ O
× -X- _ O
γwi -X- _ O
( -X- _ O
14 -X- _ O
) -X- _ O

Then -X- _ O
we -X- _ O
apply -X- _ O
the -X- _ O
attention -X- _ O
mechanism -X- _ O
over -X- _ O
the -X- _ O
Tw -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
selected -X- _ O
paragraphs -X- _ O
. -X- _ O

Attending -X- _ O
the -X- _ O
Paragraph -X- _ O
Tokens -X- _ O
We -X- _ O
select -X- _ O
the -X- _ O
top -X- _ O
- -X- _ O
k -X- _ O
paragraph -X- _ O
nodes -X- _ O
with -X- _ O
the -X- _ O
highest -X- _ O
attention -X- _ O
score -X- _ O
βj -X- _ O
. -X- _ O

We -X- _ O
incorporate -X- _ O
zi -X- _ O
with -X- _ O
edge -X- _ O
weights -X- _ O
ẽij -X- _ O
to -X- _ O
enable -X- _ O
the -X- _ O
information -X- _ O
flow -X- _ O
between -X- _ O
entity -X- _ O
nodes -X- _ O
and -X- _ O
paragraph -X- _ O
nodes -X- _ O
by -X- _ O
: -X- _ O
( -X- _ O
8) -X- _ O
m -X- _ O
X -X- _ O
zi -X- _ O
× -X- _ O

The -X- _ O
entity -X- _ O
nodes -X- _ O
act -X- _ O
as -X- _ O
the -X- _ O
intermediary -X- _ O
between -X- _ O
paragraph -X- _ O
nodes -X- _ O
. -X- _ O

We -X- _ O
also -X- _ O
add -X- _ O
a -X- _ O
residual -X- _ O
connection -X- _ O
to -X- _ O
avoid -X- _ O
gradient -X- _ O
vanishing -X- _ O
after -X- _ O
several -X- _ O
iterations -X- _ O
: -X- _ O
h̃i -X- _ O
= -X- _ O
hi -X- _ O
+ -X- _ O
ui -X- _ O
Entity -X- _ O
- -X- _ O
Aware -X- _ O
Decoder -X- _ O
with -X- _ O
Two -X- _ O
- -X- _ O
level -X- _ O
Attention -X- _ O
( -X- _ O
9 -X- _ O
) -X- _ O

We -X- _ O
combine -X- _ O
GAT -X- _ O
with -X- _ O
multi -X- _ O
- -X- _ O
head -X- _ O
operation -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
we -X- _ O
directly -X- _ O
incorporate -X- _ O
the -X- _ O
raw -X- _ O
TF -X- _ O
- -X- _ O
IDF -X- _ O
information -X- _ O
into -X- _ O
the -X- _ O
GAT -X- _ O
mechanism -X- _ O
by -X- _ O
modifying -X- _ O
the -X- _ O
attention -X- _ O
weights -X- _ O
using -X- _ O
Equation -X- _ O
5 -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
we -X- _ O
argue -X- _ O
that -X- _ O
TF -X- _ O
- -X- _ O
IDF -X- _ O
values -X- _ O
themselves -X- _ O
indicate -X- _ O
the -X- _ O
closeness -X- _ O
between -X- _ O
an -X- _ O
entity -X- _ O
cluster -X- _ O
and -X- _ O
a -X- _ O
paragraph -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
way -X- _ O
, -X- _ O
the -X- _ O
information -X- _ O
contained -X- _ O
in -X- _ O
the -X- _ O
values -X- _ O
needs -X- _ O
to -X- _ O
be -X- _ O
learned -X- _ O
by -X- _ O
an -X- _ O
additional -X- _ O
embedding -X- _ O
matrix -X- _ O
. -X- _ O

That -X- _ O
is -X- _ O
how -X- _ O
they -X- _ O
map -X- _ O
the -X- _ O
weights -X- _ O
to -X- _ O
the -X- _ O
multi -X- _ O
- -X- _ O
dimensional -X- _ O
embedding -X- _ O
space -X- _ O
eij -X- _ O
∈ -X- _ O
Rde -X- _ O
. -X- _ O

They -X- _ O
infuse -X- _ O
the -X- _ O
scalar -X- _ O
edge -X- _ O
weight -X- _ O
ẽij -X- _ O
by -X- _ O
simply -X- _ O
discretizing -X- _ O
the -X- _ O
real -X- _ O
values -X- _ O
into -X- _ O
integers -X- _ O
, -X- _ O
and -X- _ O
then -X- _ O
learn -X- _ O
embeddings -X- _ O
for -X- _ O
such -X- _ O
integers -X- _ O
. -X- _ O

j∈Ni -X- _ O
where -X- _ O
Wa -X- _ O
, -X- _ O
Wq -X- _ O
, -X- _ O
Wk -X- _ O
, -X- _ O
Wv -X- _ O
are -X- _ O
trainable -X- _ O
weights -X- _ O
, -X- _ O
σ -X- _ O
is -X- _ O
the -X- _ O
sigmoid -X- _ O
function -X- _ O
, -X- _ O
ẽij -X- _ O
is -X- _ O
the -X- _ O
edge -X- _ O
weight -X- _ O
derived -X- _ O
from -X- _ O
TF -X- _ O
- -X- _ O
IDF -X- _ O
value -X- _ O
matrix -X- _ O
Ẽ. -X- _ O
We -X- _ O
basically -X- _ O
follow -X- _ O
Wang -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020a -X- _ O
) -X- _ O
to -X- _ O
iteratively -X- _ O
update -X- _ O
node -X- _ O
representations -X- _ O
. -X- _ O

The -X- _ O
GAT -X- _ O
layer -X- _ O
is -X- _ O
designed -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O

Rdh -X- _ O
to -X- _ O
denote -X- _ O
the -X- _ O
node -X- _ O
representations -X- _ O
, -X- _ O
and -X- _ O
use -X- _ O
Ni -X- _ O
to -X- _ O
denote -X- _ O
the -X- _ O
set -X- _ O
of -X- _ O
neighboring -X- _ O
nodes -X- _ O
of -X- _ O
node -X- _ O
i. -X- _ O

We -X- _ O
use -X- _ O
i -X- _ O
, -X- _ O
j -X- _ O
∈ -X- _ O
{ -X- _ O
1 -X- _ O
, -X- _ O
... -X- _ O
, -X- _ O
( -X- _ O
m -X- _ O
+ -X- _ O
n -X- _ O
) -X- _ O
} -X- _ O
to -X- _ O
denote -X- _ O
an -X- _ O
arbitrary -X- _ O
node -X- _ O
in -X- _ O
graph -X- _ O
, -X- _ O
use -X- _ O
hi -X- _ O
, -X- _ O
hj -X- _ O
∈ -X- _ O

We -X- _ O
use -X- _ O
graph -X- _ O
attention -X- _ O
networks -X- _ O
( -X- _ O
GAT -X- _ O
) -X- _ O
( -X- _ O
Veličković -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
to -X- _ O
update -X- _ O
the -X- _ O
representations -X- _ O
of -X- _ O
semantic -X- _ O
nodes -X- _ O
. -X- _ O

Graph -X- _ O
Encoder -X- _ O

Different -X- _ O
from -X- _ O
Section -X- _ O
3.4 -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
i -X- _ O
and -X- _ O
j -X- _ O
to -X- _ O
denote -X- _ O
the -X- _ O
entity -X- _ O
node -X- _ O
and -X- _ O
paragraph -X- _ O
node -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

The -X- _ O
indicator -X- _ O
restricts -X- _ O
the -X- _ O
token -X- _ O
- -X- _ O
level -X- _ O
attention -X- _ O
only -X- _ O
to -X- _ O
some -X- _ O
of -X- _ O
the -X- _ O
paragraphs -X- _ O
, -X- _ O
which -X- _ O
can -X- _ O
further -X- _ O
reduce -X- _ O
redundancy -X- _ O
than -X- _ O
naively -X- _ O
attending -X- _ O
to -X- _ O
all -X- _ O
tokens -X- _ O
. -X- _ O

Our -X- _ O
two -X- _ O
- -X- _ O
level -X- _ O
decoding -X- _ O
process -X- _ O
firstly -X- _ O
focuses -X- _ O
on -X- _ O
several -X- _ O
centering -X- _ O
entity -X- _ O
cluster -X- _ O
nodes -X- _ O
, -X- _ O
which -X- _ O
can -X- _ O
be -X- _ O
regarded -X- _ O
as -X- _ O
indicators -X- _ O
of -X- _ O
saliency -X- _ O
. -X- _ O

If -X- _ O
the -X- _ O
decoder -X- _ O
needs -X- _ O
to -X- _ O
compute -X- _ O
attention -X- _ O
weights -X- _ O
over -X- _ O
all -X- _ O
tokens -X- _ O
, -X- _ O
the -X- _ O
cost -X- _ O
would -X- _ O
be -X- _ O
very -X- _ O
high -X- _ O
and -X- _ O
the -X- _ O
attention -X- _ O
could -X- _ O
be -X- _ O
dispersed -X- _ O
. -X- _ O

Under -X- _ O
the -X- _ O
setting -X- _ O
of -X- _ O
multi -X- _ B-TaskName
- -X- _ I-TaskName
document -X- _ I-TaskName
summarization -X- _ I-TaskName
, -X- _ O
the -X- _ O
input -X- _ O
source -X- _ O
documents -X- _ O
may -X- _ O
involve -X- _ O
an -X- _ O
extremely -X- _ O
large -X- _ O
number -X- _ O
of -X- _ O
word -X- _ O
tokens -X- _ O
. -X- _ O

3.5 -X- _ O

Rnw -X- _ O
×(dw -X- _ O
+ -X- _ O
dh -X- _ B-HyperparameterName
) -X- _ O
. -X- _ O

After -X- _ O
iterating -X- _ O
for -X- _ O
t -X- _ O
times -X- _ O
, -X- _ O
we -X- _ O
concatenate -X- _ O
H̃p -X- _ O
to -X- _ O
each -X- _ O
corresponding -X- _ O
input -X- _ O
token -X- _ O
vector -X- _ O
, -X- _ O
arriving -X- _ O
at -X- _ O
H̃pw -X- _ O
∈ -X- _ O

Each -X- _ O
iteration -X- _ O
contains -X- _ O
a -X- _ O
paragraph -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
entity -X- _ O
and -X- _ O
a -X- _ O
entity -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
paragraph -X- _ O
updating -X- _ O
process -X- _ O
. -X- _ O

3.4 -X- _ O
We -X- _ O
use -X- _ O
the -X- _ O
above -X- _ O
GAT -X- _ O
layer -X- _ O
and -X- _ O
positionwise -X- _ O
feed -X- _ O
- -X- _ O
forward -X- _ O
layer -X- _ O
to -X- _ O
iteratively -X- _ O
update -X- _ O
the -X- _ O
node -X- _ O
representations -X- _ O
. -X- _ O

Rmw -X- _ O
×dw -X- _ O
and -X- _ O
Hc -X- _ O
∈ -X- _ O
Rm×dh -X- _ O
to -X- _ O
denote -X- _ O
the -X- _ O
token -X- _ O
level -X- _ O
feature -X- _ O
matrix -X- _ O
and -X- _ O
cluster -X- _ O
level -X- _ O
feature -X- _ O
matrix -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
Hcw -X- _ O
∈ -X- _ O

Note -X- _ O
that -X- _ O
we -X- _ O
firstly -X- _ O
remove -X- _ O
pronouns -X- _ O
and -X- _ O
stopwords -X- _ O
in -X- _ O
entity -X- _ O
mention -X- _ O
clusters -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
common -X- _ O
in -X- _ O
co -X- _ O
- -X- _ O
reference -X- _ O
resolution -X- _ O
results -X- _ O
but -X- _ O
render -X- _ O
little -X- _ O
benefit -X- _ O
for -X- _ O
our -X- _ O
semantic -X- _ O
modeling -X- _ O
. -X- _ O

this -X- _ O
method -X- _ O
rather -X- _ O
than -X- _ O
additional -X- _ O
entity -X- _ O
embedding -X- _ O
methods -X- _ O
because -X- _ O
we -X- _ O
seek -X- _ O
to -X- _ O
model -X- _ O
the -X- _ O
relationship -X- _ O
between -X- _ O
paragraphs -X- _ O
and -X- _ O
entities -X- _ O
in -X- _ O
a -X- _ O
unified -X- _ O
semantic -X- _ O
space -X- _ O
. -X- _ O

We -X- _ O
choose -X- _ O

We -X- _ O
perform -X- _ O
the -X- _ O
same -X- _ O
encoding -X- _ O
process -X- _ O
as -X- _ O
the -X- _ O
paragraph -X- _ O
encoder -X- _ O
to -X- _ O
get -X- _ O
entity -X- _ O
clusters -X- _ O
’ -X- _ O
representation -X- _ O
, -X- _ O
but -X- _ O
without -X- _ O
sharing -X- _ O
parameters -X- _ O
between -X- _ O
the -X- _ O
two -X- _ O
encoders -X- _ O
. -X- _ O

Entity -X- _ O
Cluster -X- _ O
Encoder -X- _ O

We -X- _ O
use -X- _ O
Hp -X- _ O
∈ -X- _ O
Rn×dh -X- _ O
to -X- _ O
denote -X- _ O
the -X- _ O
paragraph -X- _ O
level -X- _ O
feature -X- _ O
matrix -X- _ O
, -X- _ O
where -X- _ O
n -X- _ O
is -X- _ O
the -X- _ O
number -X- _ O
of -X- _ O
paragraphs -X- _ O
, -X- _ O
dh -X- _ B-HyperparameterName
is -X- _ O
the -X- _ O
hidden -X- _ O
size -X- _ O
. -X- _ O

For -X- _ O
the -X- _ O
l -X- _ O
- -X- _ O
th -X- _ O
transformer -X- _ O
layer -X- _ O
, -X- _ O
l−1 -X- _ O
, -X- _ O
the -X- _ O
hidden -X- _ O
state -X- _ O
is -X- _ O
hl -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
the -X- _ O
input -X- _ O
is -X- _ O
xw -X- _ O
w -X- _ O
354 -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O

Let -X- _ O
x0w -X- _ O
be -X- _ O
the -X- _ O
input -X- _ O
token -X- _ O
vector -X- _ O
. -X- _ O

The -X- _ O
transformer -X- _ O
layer -X- _ O
is -X- _ O
the -X- _ O
same -X- _ O
as -X- _ O
the -X- _ O
vanilla -X- _ O
transformer -X- _ O
layer -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

Document -X- _ O
Encoder -X- _ O
Paragraph -X- _ O
Encoder -X- _ O
Several -X- _ O
token -X- _ O
- -X- _ O
level -X- _ O
transformer -X- _ O
encoding -X- _ O
layers -X- _ O
are -X- _ O
stacked -X- _ O
to -X- _ O
encode -X- _ O
contextual -X- _ O
information -X- _ O
within -X- _ O
each -X- _ O
paragraph -X- _ O
. -X- _ O

hp -X- _ O
= -X- _ O
MHPool(hw1 -X- _ O
, -X- _ O
hw2 -X- _ O
, -X- _ O
... -X- _ O
) -X- _ O

The -X- _ O
multi -X- _ O
- -X- _ O
head -X- _ O
pooling -X- _ O
mechanism -X- _ O
calculates -X- _ O
the -X- _ O
weight -X- _ O
distributions -X- _ O
over -X- _ O
tokens -X- _ O
, -X- _ O
allowing -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
flexibly -X- _ O
encode -X- _ O
paragraphs -X- _ O
in -X- _ O
different -X- _ O
representational -X- _ O
subspace -X- _ O
by -X- _ O
different -X- _ O
head -X- _ O
. -X- _ O

Multi -X- _ O
- -X- _ O
Head -X- _ O
Pooling -X- _ O
To -X- _ O
obtain -X- _ O
fixed -X- _ O
length -X- _ O
paragraph -X- _ O
representations -X- _ O
, -X- _ O
we -X- _ O
follow -X- _ O
Liu -X- _ O
and -X- _ O
Lapata -X- _ O
( -X- _ O
2019a -X- _ O
) -X- _ O
to -X- _ O
apply -X- _ O
a -X- _ O
weighted -X- _ O
- -X- _ O
pooling -X- _ O
operation -X- _ O
. -X- _ O

Rnw -X- _ O
×dw -X- _ O
to -X- _ O
denote -X- _ O
the -X- _ O
token -X- _ O
- -X- _ O
level -X- _ O
feature -X- _ O
matrix -X- _ O
, -X- _ O
where -X- _ O
nw -X- _ O
is -X- _ O
the -X- _ O
total -X- _ O
number -X- _ O
of -X- _ O
tokens -X- _ O
in -X- _ O
all -X- _ O
paragraphs -X- _ O
and -X- _ O
dw -X- _ B-HyperparameterName
is -X- _ O
the -X- _ O
dimension -X- _ O
of -X- _ O
token -X- _ O
embedding -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
Hpw -X- _ O
∈ -X- _ O

We -X- _ O
take -X- _ O
the -X- _ O
output -X- _ O
of -X- _ O
last -X- _ O
layer -X- _ O
as -X- _ O
token -X- _ O
- -X- _ O
level -X- _ O
features -X- _ O
. -X- _ O

FFN -X- _ O
is -X- _ O
a -X- _ O
feed -X- _ O
- -X- _ O
forward -X- _ O
network -X- _ O
with -X- _ O
ReLU -X- _ O
as -X- _ O
activation -X- _ O
function -X- _ O
. -X- _ O

MHAttn -X- _ O
is -X- _ O
multi -X- _ O
- -X- _ O
head -X- _ O
attention -X- _ O
from -X- _ O
Vaswani -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

3.3 -X- _ O
xlw -X- _ O
= -X- _ O
LayerNorm(hlw -X- _ O
+ -X- _ O
FFN(hlw -X- _ O
) -X- _ O
) -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
LayerNorm -X- _ O
is -X- _ O
the -X- _ O
layer -X- _ O
normalization -X- _ O
operation -X- _ O
( -X- _ O
Ba -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2016 -X- _ O
) -X- _ O
. -X- _ O

Rm×n -X- _ O
to -X- _ O
model -X- _ O
the -X- _ O
importance -X- _ O
of -X- _ O
relationships -X- _ O
between -X- _ O
entity -X- _ O
clusters -X- _ O
and -X- _ O
paragraphs -X- _ O
. -X- _ O

Based -X- _ O
on -X- _ O
E -X- _ O
, -X- _ O
we -X- _ O
further -X- _ O
calculate -X- _ O
the -X- _ O
TF -X- _ O
- -X- _ O
IDF -X- _ O
value -X- _ O
matrix -X- _ O
Ẽ -X- _ O
∈ -X- _ O

We -X- _ O
get -X- _ O
an -X- _ O
occurrence -X- _ O
matrix -X- _ O
E -X- _ O
∈ -X- _ O
Rm×n -X- _ O
after -X- _ O
extraction -X- _ O
, -X- _ O
where -X- _ O
eij -X- _ O
6= -X- _ O
0 -X- _ O
indicates -X- _ O
Pi -X- _ O
contains -X- _ O
entity -X- _ O
mentions -X- _ O
in -X- _ O
Cj -X- _ O
for -X- _ O
eij -X- _ O
times -X- _ O
. -X- _ O

We -X- _ O
would -X- _ O
like -X- _ O
to -X- _ O
include -X- _ O
more -X- _ O
information -X- _ O
in -X- _ O
the -X- _ O
graph -X- _ O
. -X- _ O

An -X- _ O
edge -X- _ O
which -X- _ O
connects -X- _ O
Pi -X- _ O
and -X- _ O
Cj -X- _ O
means -X- _ O
paragraph -X- _ O
Pi -X- _ O
contains -X- _ O
an -X- _ O
entity -X- _ O
mention -X- _ O
in -X- _ O
Cj -X- _ O
. -X- _ O

There -X- _ O
exists -X- _ O
no -X- _ O
edge -X- _ O
inside -X- _ O
paragraph -X- _ O
nodes -X- _ O
or -X- _ O
entity -X- _ O
cluster -X- _ O
nodes -X- _ O
, -X- _ O
but -X- _ O
only -X- _ O
between -X- _ O
them -X- _ O
. -X- _ O

E -X- _ O
represents -X- _ O
undirected -X- _ O
edges -X- _ O
between -X- _ O
nodes -X- _ O
. -X- _ O

V -X- _ O
includes -X- _ O
paragraph -X- _ O
nodes -X- _ O
Vp -X- _ O
and -X- _ O
entity -X- _ O
cluster -X- _ O
nodes -X- _ O
Vc -X- _ O
. -X- _ O

We -X- _ O
then -X- _ O
construct -X- _ O
a -X- _ O
heterogeneous -X- _ O
graph -X- _ O
G -X- _ O
= -X- _ O
( -X- _ O
V -X- _ O
, -X- _ O
E -X- _ O
) -X- _ O
. -X- _ O

, -X- _ O
Pn -X- _ O
} -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
paragraphs -X- _ O
and -X- _ O
sentences -X- _ O
, -X- _ O
depending -X- _ O
on -X- _ O
the -X- _ O
characteristics -X- _ O
of -X- _ O
datasets -X- _ O
. -X- _ O

Given -X- _ O
a -X- _ O
source -X- _ O
document -X- _ O
cluster -X- _ O
D -X- _ O
, -X- _ O
we -X- _ O
firstly -X- _ O
divide -X- _ O
them -X- _ O
into -X- _ O
smaller -X- _ O
semantic -X- _ O
units -X- _ O
P -X- _ O
= -X- _ O
{ -X- _ O
P1 -X- _ O
, -X- _ O
P2 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O

3.2 -X- _ O
l−1 -X- _ O
hlw -X- _ O
= -X- _ O
LayerNorm(xl−1 -X- _ O
w -X- _ O
+ -X- _ O
MHAttn(xw -X- _ O
) -X- _ O
) -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
Graph -X- _ O
Construction -X- _ O

the -X- _ O
number -X- _ O
of -X- _ O
entity -X- _ O
mentions -X- _ O
in -X- _ O
cluster -X- _ O
Ci -X- _ O
. -X- _ O

output -X- _ O
is -X- _ O
xlw -X- _ O
. -X- _ O

Overall -X- _ O
architecture -X- _ O
of -X- _ O
our -X- _ O
model -X- _ O
. -X- _ O

Figure -X- _ O
2 -X- _ O
: -X- _ O

, -X- _ O
mentionl -X- _ O
} -X- _ O
, -X- _ O
and -X- _ O
l -X- _ O
is -X- _ O
353 -X- _ O

, -X- _ O
Cm -X- _ O
} -X- _ O
, -X- _ O
where -X- _ O
Ci -X- _ O
= -X- _ O
{ -X- _ O
mention1 -X- _ O
, -X- _ O
mention2 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O

We -X- _ O
denote -X- _ O
the -X- _ O
extracted -X- _ O
entity -X- _ O
clusters -X- _ O
as -X- _ O
C -X- _ O
= -X- _ O
{ -X- _ O
C1 -X- _ O
, -X- _ O
C2 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O

Note -X- _ O
that -X- _ O
we -X- _ O
perform -X- _ O
extraction -X- _ O
globally -X- _ O
, -X- _ O
which -X- _ O
means -X- _ O
we -X- _ O
concatenate -X- _ O
all -X- _ O
the -X- _ O
documents -X- _ O
into -X- _ O
one -X- _ O
long -X- _ O
document -X- _ O
. -X- _ O

We -X- _ O
utilize -X- _ O
the -X- _ O
co -X- _ O
- -X- _ O
reference -X- _ O
resolution -X- _ O
tool -X- _ O
( -X- _ O
Lee -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
from -X- _ O
AllenNLP -X- _ O
( -X- _ O
Gardner -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
to -X- _ O
extract -X- _ O
entity -X- _ O
clusters -X- _ O
. -X- _ O

Therefore -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
entity -X- _ O
clusters -X- _ O
as -X- _ O
more -X- _ O
advanced -X- _ O
semantic -X- _ O
units -X- _ O
. -X- _ O

The -X- _ O
total -X- _ O
number -X- _ O
of -X- _ O
words -X- _ O
will -X- _ O
be -X- _ O
vast -X- _ O
, -X- _ O
which -X- _ O
further -X- _ O
causes -X- _ O
a -X- _ O
hindrance -X- _ O
for -X- _ O
the -X- _ O
graph -X- _ O
construction -X- _ O
and -X- _ O
message -X- _ O
passing -X- _ O
process -X- _ O
. -X- _ O

For -X- _ O
multi -X- _ B-TaskName
- -X- _ I-TaskName
document -X- _ I-TaskName
summarization -X- _ I-TaskName
, -X- _ O
models -X- _ O
are -X- _ O
usually -X- _ O
required -X- _ O
to -X- _ O
process -X- _ O
tens -X- _ O
of -X- _ O
documents -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
we -X- _ O
argue -X- _ O
that -X- _ O
word -X- _ O
- -X- _ O
level -X- _ O
semantic -X- _ O
units -X- _ O
are -X- _ O
too -X- _ O
fine -X- _ O
and -X- _ O
will -X- _ O
bring -X- _ O
huge -X- _ O
computational -X- _ O
costs -X- _ O
. -X- _ O

3.1 -X- _ O
Entity -X- _ O
Cluster -X- _ O
Extraction -X- _ O
Wang -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020a -X- _ O
) -X- _ O
use -X- _ O
words -X- _ O
as -X- _ O
semantic -X- _ O
units -X- _ O
in -X- _ O
addition -X- _ O
to -X- _ O
sentence -X- _ O
nodes -X- _ O
, -X- _ O
acting -X- _ O
as -X- _ O
the -X- _ O
intermediary -X- _ O
to -X- _ O
enrich -X- _ O
the -X- _ O
relationships -X- _ O
between -X- _ O
sentences -X- _ O
. -X- _ O

We -X- _ O
design -X- _ O
a -X- _ O
novel -X- _ O
two -X- _ O
- -X- _ O
level -X- _ O
decoding -X- _ O
process -X- _ O
to -X- _ O
explicitly -X- _ O
deal -X- _ O
with -X- _ O
the -X- _ O
problem -X- _ O
of -X- _ O
saliency -X- _ O
and -X- _ O
redundancy -X- _ O
. -X- _ O

We -X- _ O
modify -X- _ O
the -X- _ O
encoder -X- _ O
with -X- _ O
graph -X- _ O
neural -X- _ O
networks -X- _ O
, -X- _ O
so -X- _ O
we -X- _ O
can -X- _ O
incorporate -X- _ O
entity -X- _ O
information -X- _ O
and -X- _ O
graph -X- _ O
representations -X- _ O
at -X- _ O
the -X- _ O
same -X- _ O
time -X- _ O
. -X- _ O

3 -X- _ O
Model -X- _ O
Our -X- _ O
model -X- _ O
is -X- _ O
illustrated -X- _ O
in -X- _ O
Figure -X- _ O
2 -X- _ O
, -X- _ O
which -X- _ O
follows -X- _ O
the -X- _ O
transformer -X- _ O
- -X- _ O
based -X- _ O
encoder -X- _ O
- -X- _ O
decoder -X- _ O
architecture -X- _ O
( -X- _ O
Vaswani -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
) -X- _ O
. -X- _ O

Wang -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020b -X- _ O
) -X- _ O
rearrange -X- _ O
and -X- _ O
further -X- _ O
explore -X- _ O
the -X- _ O
semantics -X- _ O
of -X- _ O
the -X- _ O
topic -X- _ O
model -X- _ O
and -X- _ O
develope -X- _ O
a -X- _ O
friendly -X- _ O
topic -X- _ O
assistant -X- _ O
for -X- _ O
transfomer -X- _ O
- -X- _ O
based -X- _ O
abstractive -X- _ O
summarization -X- _ O
models -X- _ O
. -X- _ O

Perez -X- _ O
- -X- _ O
Beltrachini -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
explicitly -X- _ O
model -X- _ O
the -X- _ O
topic -X- _ O
structure -X- _ O
of -X- _ O
summaries -X- _ O
, -X- _ O
and -X- _ O
utilize -X- _ O
it -X- _ O
to -X- _ O
guide -X- _ O
a -X- _ O
structured -X- _ O
convolutional -X- _ O
decoder -X- _ O
. -X- _ O

In -X- _ O
their -X- _ O
work -X- _ O
, -X- _ O
sentence -X- _ O
salience -X- _ O
is -X- _ O
estimated -X- _ O
in -X- _ O
a -X- _ O
hierarchical -X- _ O
way -X- _ O
with -X- _ O
subtopic -X- _ O
salience -X- _ O
and -X- _ O
relative -X- _ O
sentence -X- _ O
salience -X- _ O
. -X- _ O

Zheng -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
propose -X- _ O
to -X- _ O
mine -X- _ O
cross -X- _ O
- -X- _ O
document -X- _ O
subtopics -X- _ O
. -X- _ O

Narayan -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
recommend -X- _ O
an -X- _ O
encoder -X- _ O
associating -X- _ O
each -X- _ O
word -X- _ O
with -X- _ O
a -X- _ O
topic -X- _ O
vector -X- _ O
capturing -X- _ O
whether -X- _ O
it -X- _ O
is -X- _ O
representative -X- _ O
of -X- _ O
the -X- _ O
document -X- _ O
’s -X- _ O
content -X- _ O
, -X- _ O
and -X- _ O
a -X- _ O
decoder -X- _ O
where -X- _ O
each -X- _ O
word -X- _ O
prediction -X- _ O
is -X- _ O
conditioned -X- _ O
on -X- _ O
a -X- _ O
document -X- _ O
topic -X- _ O
vector -X- _ O
. -X- _ O

Apart -X- _ O
from -X- _ O
entity -X- _ O
or -X- _ O
fact -X- _ O
information -X- _ O
, -X- _ O
there -X- _ O
are -X- _ O
several -X- _ O
works -X- _ O
that -X- _ O
incorporate -X- _ O
topic -X- _ O
information -X- _ O
into -X- _ O
summarization -X- _ O
model -X- _ O
. -X- _ O

Zhu -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
extract -X- _ O
factual -X- _ O
relations -X- _ O
from -X- _ O
the -X- _ O
source -X- _ O
texts -X- _ O
to -X- _ O
build -X- _ O
a -X- _ O
local -X- _ O
knowledge -X- _ O
graph -X- _ O
and -X- _ O
integrated -X- _ O
it -X- _ O
into -X- _ O
the -X- _ O
transformer -X- _ O
- -X- _ O
based -X- _ O
model -X- _ O
. -X- _ O

Gunel -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
inject -X- _ O
structural -X- _ O
world -X- _ O
knowledge -X- _ O
from -X- _ O
Wikidata -X- _ O
to -X- _ O
a -X- _ O
transformer -X- _ O
- -X- _ O
based -X- _ O
model -X- _ O
, -X- _ O
enabling -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
be -X- _ O
more -X- _ O
fact -X- _ O
- -X- _ O
aware -X- _ O
. -X- _ O

By -X- _ O
contrast -X- _ O
, -X- _ O
our -X- _ O
EMSum -X- _ B-MethodName
model -X- _ O
is -X- _ O
an -X- _ O
end -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
end -X- _ O
method -X- _ O
for -X- _ O
multi -X- _ B-TaskName
- -X- _ I-TaskName
document -X- _ I-TaskName
summarization -X- _ I-TaskName
. -X- _ O

Sharma -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
take -X- _ O
a -X- _ O
pipeline -X- _ O
method -X- _ O
for -X- _ O
single -X- _ O
- -X- _ O
document -X- _ O
summarization -X- _ O
which -X- _ O
is -X- _ O
composed -X- _ O
of -X- _ O
an -X- _ O
entity -X- _ O
- -X- _ O
aware -X- _ O
content -X- _ O
selection -X- _ O
module -X- _ O
and -X- _ O
a -X- _ O
summary -X- _ O
generation -X- _ O
module -X- _ O
. -X- _ O

Cao -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2018 -X- _ O
) -X- _ O
extract -X- _ O
actual -X- _ O
fact -X- _ O
descriptions -X- _ O
from -X- _ O
the -X- _ O
source -X- _ O
text -X- _ O
and -X- _ O
propose -X- _ O
a -X- _ O
dual -X- _ O
- -X- _ O
attention -X- _ O
mechanism -X- _ O
to -X- _ O
force -X- _ O
the -X- _ O
generation -X- _ O
conditioned -X- _ O
on -X- _ O
both -X- _ O
the -X- _ O
source -X- _ O
text -X- _ O
and -X- _ O
the -X- _ O
extracted -X- _ O
fact -X- _ O
descriptions -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
to -X- _ O
the -X- _ O
direct -X- _ O
application -X- _ O
of -X- _ O
the -X- _ O
general -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
framework -X- _ O
, -X- _ O
researchers -X- _ O
attempted -X- _ O
to -X- _ O
incorporate -X- _ O
various -X- _ O
features -X- _ O
into -X- _ O
summarization -X- _ O
. -X- _ O

2.3 -X- _ O
Summarization -X- _ O
with -X- _ O
Additional -X- _ O
Features -X- _ O

Our -X- _ O
work -X- _ O
is -X- _ O
partly -X- _ O
similar -X- _ O
to -X- _ O
theirs -X- _ O
, -X- _ O
but -X- _ O
we -X- _ O
construct -X- _ O
heterogeneous -X- _ O
graphs -X- _ O
composed -X- _ O
of -X- _ O
text -X- _ O
unit -X- _ O
nodes -X- _ O
and -X- _ O
entity -X- _ O
nodes -X- _ O
for -X- _ O
abstractive -X- _ O
multi -X- _ B-TaskName
- -X- _ I-TaskName
document -X- _ I-TaskName
summarization -X- _ I-TaskName
. -X- _ O

However -X- _ O
, -X- _ O
Wang -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020a -X- _ O
) -X- _ O
are -X- _ O
the -X- _ O
first -X- _ O
to -X- _ O
introduce -X- _ O
different -X- _ O
granularity -X- _ O
levels -X- _ O
of -X- _ O
text -X- _ O
nodes -X- _ O
to -X- _ O
construct -X- _ O
heterogeneous -X- _ O
graphs -X- _ O
for -X- _ O
extractive -X- _ O
summarization -X- _ O
. -X- _ O

Li -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
utilize -X- _ O
homogeneous -X- _ O
graphs -X- _ O
to -X- _ O
capture -X- _ O
cross -X- _ O
- -X- _ O
document -X- _ O
relations -X- _ O
and -X- _ O
guide -X- _ O
the -X- _ O
summary -X- _ O
generation -X- _ O
process -X- _ O
. -X- _ O

Huang -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
further -X- _ O
design -X- _ O
a -X- _ O
graph -X- _ O
encoder -X- _ O
, -X- _ O
which -X- _ O
improves -X- _ O
upon -X- _ O
graph -X- _ O
attention -X- _ O
networks -X- _ O
, -X- _ O
to -X- _ O
maintain -X- _ O
the -X- _ O
global -X- _ O
context -X- _ O
and -X- _ O
local -X- _ O
entities -X- _ O
complementing -X- _ O
each -X- _ O
other -X- _ O
. -X- _ O

Fan -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
construct -X- _ O
a -X- _ O
local -X- _ O
knowledge -X- _ O
graph -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
then -X- _ O
linearized -X- _ O
into -X- _ O
a -X- _ O
structured -X- _ O
input -X- _ O
sequence -X- _ O
so -X- _ O
that -X- _ O
models -X- _ O
can -X- _ O
encode -X- _ O
within -X- _ O
the -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
setting -X- _ O
. -X- _ O

Yasunaga -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
construct -X- _ O
an -X- _ O
approximate -X- _ O
discourse -X- _ O
graph -X- _ O
based -X- _ O
on -X- _ O
discourse -X- _ O
markers -X- _ O
and -X- _ O
entity -X- _ O
links -X- _ O
, -X- _ O
then -X- _ O
apply -X- _ O
graph -X- _ O
convolutional -X- _ O
networks -X- _ O
over -X- _ O
the -X- _ O
relation -X- _ O
graph -X- _ O
. -X- _ O

For -X- _ O
recent -X- _ O
methods -X- _ O
based -X- _ O
on -X- _ O
graph -X- _ O
neural -X- _ O
networks -X- _ O
, -X- _ O
Tan -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2017 -X- _ O
) -X- _ O
propose -X- _ O
a -X- _ O
graph -X- _ O
- -X- _ O
based -X- _ O
attention -X- _ O
mechanism -X- _ O
to -X- _ O
identify -X- _ O
salient -X- _ O
sentences -X- _ O
. -X- _ O

Christensen -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2013 -X- _ O
) -X- _ O
build -X- _ O
multi -X- _ O
- -X- _ O
document -X- _ O
graphs -X- _ O
to -X- _ O
approximate -X- _ O
the -X- _ O
discourse -X- _ O
relations -X- _ O
across -X- _ O
sentences -X- _ O
based -X- _ O
on -X- _ O
indicators -X- _ O
including -X- _ O
discourse -X- _ O
cues -X- _ O
, -X- _ O
deverbal -X- _ O
nouns -X- _ O
, -X- _ O
co -X- _ O
- -X- _ O
reference -X- _ O
and -X- _ O
more -X- _ O
. -X- _ O

Wan -X- _ O
( -X- _ O
2008 -X- _ O
) -X- _ O
further -X- _ O
incorporate -X- _ O
the -X- _ O
document -X- _ O
- -X- _ O
level -X- _ O
information -X- _ O
and -X- _ O
the -X- _ O
sentence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
document -X- _ O
relationship -X- _ O
into -X- _ O
the -X- _ O
graph -X- _ O
- -X- _ O
based -X- _ O
ranking -X- _ O
process -X- _ O
. -X- _ O

the -X- _ O
eigenvector -X- _ O
centrality -X- _ O
in -X- _ O
the -X- _ O
connectivity -X- _ O
graph -X- _ O
of -X- _ O
inter -X- _ O
- -X- _ O
sentence -X- _ O
cosine -X- _ O
similarity -X- _ O
. -X- _ O

LexRank -X- _ B-MethodName
( -X- _ O
Erkan -X- _ O
and -X- _ O
Radev -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
computes -X- _ O
sentence -X- _ O
salience -X- _ O
based -X- _ O
on -X- _ O

Text -X- _ O
units -X- _ O
on -X- _ O
graphs -X- _ O
are -X- _ O
ranked -X- _ O
and -X- _ O
selected -X- _ O
as -X- _ O
the -X- _ O
most -X- _ O
salient -X- _ O
ones -X- _ O
to -X- _ O
be -X- _ O
included -X- _ O
in -X- _ O
the -X- _ O
summary -X- _ O
. -X- _ O

Our -X- _ O
code -X- _ O
is -X- _ O
at -X- _ O
https://github.com/Oceandam/EMSum -X- _ O
352 -X- _ O
2.2 -X- _ O
Graph -X- _ O
- -X- _ O
based -X- _ O
Document -X- _ O
Summarization -X- _ O
Graph -X- _ O
- -X- _ O
based -X- _ O
methods -X- _ O
have -X- _ O
long -X- _ O
been -X- _ O
utilized -X- _ O
for -X- _ O
extractive -X- _ O
summarization -X- _ O
. -X- _ O

Zou -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
present -X- _ O
three -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
objectives -X- _ O
by -X- _ O
reinstating -X- _ O
source -X- _ O
text -X- _ O
for -X- _ O
abstractive -X- _ O
summarization -X- _ O
. -X- _ O

Moreover -X- _ O
, -X- _ O
several -X- _ O
general -X- _ O
purpose -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
models -X- _ O
are -X- _ O
proposed -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
T5 -X- _ O
( -X- _ O
Raffel -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
and -X- _ O
BART -X- _ O
( -X- _ O
Lewis -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
. -X- _ O

They -X- _ O
are -X- _ O
further -X- _ O
finetuned -X- _ O
for -X- _ O
the -X- _ O
summarization -X- _ O
task -X- _ O
. -X- _ O

Zhang -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020 -X- _ O
) -X- _ O
propose -X- _ O
PEGASUS -X- _ O
, -X- _ O
in -X- _ O
which -X- _ O
they -X- _ O
design -X- _ O
a -X- _ O
pre -X- _ O
- -X- _ O
training -X- _ O
objective -X- _ O
tailored -X- _ O
for -X- _ O
abstractive -X- _ O
text -X- _ O
summarization -X- _ O
. -X- _ O

Zhang -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2019 -X- _ O
) -X- _ O
build -X- _ O
low -X- _ O
- -X- _ O
level -X- _ O
and -X- _ O
high -X- _ O
- -X- _ O
level -X- _ O
Berts -X- _ O
for -X- _ O
sentence -X- _ O
and -X- _ O
document -X- _ O
understanding -X- _ O
, -X- _ O
respectively -X- _ O
. -X- _ O

Liu -X- _ O
and -X- _ O
Lapata -X- _ O
( -X- _ O
2019b -X- _ O
) -X- _ O
propose -X- _ O
BertSUM -X- _ B-MethodName
for -X- _ O
both -X- _ O
extractive -X- _ O
and -X- _ O
abstractive -X- _ O
summarization -X- _ O
. -X- _ O

More -X- _ O
recently -X- _ O
, -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
excellent -X- _ O
performance -X- _ O
on -X- _ O
various -X- _ O
text -X- _ O
generation -X- _ O
tasks -X- _ O
, -X- _ O
transformer -X- _ O
- -X- _ O
based -X- _ O
methods -X- _ O
become -X- _ O
the -X- _ O
mainstream -X- _ O
approach -X- _ O
for -X- _ O
abstractive -X- _ O
multi -X- _ B-TaskName
- -X- _ I-TaskName
document -X- _ I-TaskName
summarization -X- _ I-TaskName
, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
language -X- _ O
models -X- _ O
. -X- _ O

Traditional -X- _ O
approaches -X- _ O
to -X- _ O
abstractive -X- _ O
summarization -X- _ O
can -X- _ O
be -X- _ O
divided -X- _ O
into -X- _ O
sentence -X- _ O
fusion -X- _ O
- -X- _ O
based -X- _ O
( -X- _ O
Barzilay -X- _ O
and -X- _ O
McKeown -X- _ O
, -X- _ O
2005 -X- _ O
; -X- _ O
Filippova -X- _ O
and -X- _ O
Strube -X- _ O
, -X- _ O
2008 -X- _ O
; -X- _ O
Banerjee -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2015 -X- _ O
) -X- _ O
, -X- _ O
paraphrasing -X- _ O
- -X- _ O
based -X- _ O
( -X- _ O
Bing -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2015 -X- _ O
; -X- _ O
Cohn -X- _ O
and -X- _ O
Lapata -X- _ O
, -X- _ O
2009 -X- _ O
) -X- _ O
and -X- _ O
information -X- _ O
extraction -X- _ O
- -X- _ O
based -X- _ O
( -X- _ O
Li -X- _ O
, -X- _ O
2015 -X- _ O
; -X- _ O
Wang -X- _ O
and -X- _ O
Cardie -X- _ O
, -X- _ O
2013 -X- _ O
; -X- _ O
Pighin -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2014 -X- _ O
) -X- _ O
. -X- _ O

With -X- _ O
the -X- _ O
development -X- _ O
of -X- _ O
neural -X- _ O
- -X- _ O
based -X- _ O
methods -X- _ O
, -X- _ O
abstractive -X- _ O
methods -X- _ O
achieved -X- _ O
promising -X- _ O
results -X- _ O
on -X- _ O
single -X- _ O
document -X- _ O
summarization -X- _ O
( -X- _ O
See -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2017 -X- _ O
; -X- _ O
Paulus -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Gehrmann -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Li -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

By -X- _ O
contrast -X- _ O
, -X- _ O
the -X- _ O
process -X- _ O
of -X- _ O
abstractive -X- _ O
summarization -X- _ O
is -X- _ O
more -X- _ O
similar -X- _ O
to -X- _ O
the -X- _ O
human -X- _ O
summarization -X- _ O
process -X- _ O
and -X- _ O
requires -X- _ O
more -X- _ O
sophisticated -X- _ O
natural -X- _ O
language -X- _ O
understanding -X- _ O
and -X- _ O
generation -X- _ O
techniques -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
sentence -X- _ O
- -X- _ O
level -X- _ O
extraction -X- _ O
lacks -X- _ O
flexibility -X- _ O
and -X- _ O
tends -X- _ O
to -X- _ O
produce -X- _ O
redundant -X- _ O
information -X- _ O
. -X- _ O

• -X- _ O
We -X- _ O
propose -X- _ O
a -X- _ O
novel -X- _ O
two -X- _ O
- -X- _ O
level -X- _ O
attention -X- _ O
mechanism -X- _ O
during -X- _ O
the -X- _ O
decoding -X- _ O
process -X- _ O
, -X- _ O
solving -X- _ O
the -X- _ O
issues -X- _ O
of -X- _ O
saliency -X- _ O
and -X- _ O
redundancy -X- _ O
explicitly -X- _ O
. -X- _ O

Thus -X- _ O
, -X- _ O
they -X- _ O
may -X- _ O
be -X- _ O
able -X- _ O
to -X- _ O
achieve -X- _ O
relatively -X- _ O
high -X- _ O
ROUGE -X- _ B-MetricName
scores -X- _ O
( -X- _ O
Lin -X- _ O
, -X- _ O
2004 -X- _ O
) -X- _ O
. -X- _ O

• -X- _ O
Our -X- _ O
model -X- _ O
achieves -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
results -X- _ O
on -X- _ O
WikiSum -X- _ B-DatasetName
and -X- _ O
MultiNews -X- _ B-DatasetName
. -X- _ O

The -X- _ O
mechanism -X- _ O
can -X- _ O
also -X- _ O
reduce -X- _ O
the -X- _ O
computational -X- _ O
cost -X- _ O
, -X- _ O
making -X- _ O
it -X- _ O
easier -X- _ O
to -X- _ O
process -X- _ O
long -X- _ O
inputs -X- _ O
. -X- _ O

Experiments -X- _ O
show -X- _ O
that -X- _ O
exploiting -X- _ O
entity -X- _ O
nodes -X- _ O
as -X- _ O
the -X- _ O
intermediary -X- _ O
between -X- _ O
documents -X- _ O
can -X- _ O
be -X- _ O
more -X- _ O
effective -X- _ O
than -X- _ O
exploiting -X- _ O
other -X- _ O
semantic -X- _ O
units -X- _ O
( -X- _ O
e.g. -X- _ O
, -X- _ O
words -X- _ O
) -X- _ O
. -X- _ O

To -X- _ O
the -X- _ O
best -X- _ O
of -X- _ O
our -X- _ O
knowledge -X- _ O
, -X- _ O
we -X- _ O
are -X- _ O
the -X- _ O
first -X- _ O
to -X- _ O
model -X- _ O
the -X- _ O
relations -X- _ O
between -X- _ O
documents -X- _ O
and -X- _ O
entities -X- _ O
in -X- _ O
one -X- _ O
heterogeneous -X- _ O
graph -X- _ O
. -X- _ O

The -X- _ O
graph -X- _ O
consists -X- _ O
of -X- _ O
document -X- _ O
- -X- _ O
level -X- _ O
and -X- _ O
entitylevel -X- _ O
nodes -X- _ O
. -X- _ O

Our -X- _ O
contributions -X- _ O
are -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O
• -X- _ O
We -X- _ O
construct -X- _ O
a -X- _ O
heterogeneous -X- _ O
graph -X- _ O
network -X- _ O
for -X- _ O
multi -X- _ B-TaskName
- -X- _ I-TaskName
document -X- _ I-TaskName
summarization -X- _ I-TaskName
. -X- _ O

Further -X- _ O
improvements -X- _ O
can -X- _ O
be -X- _ O
made -X- _ O
when -X- _ O
our -X- _ O
model -X- _ O
is -X- _ O
used -X- _ O
together -X- _ O
with -X- _ O
the -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
language -X- _ O
models -X- _ O
. -X- _ O

Experiments -X- _ O
show -X- _ O
that -X- _ O
our -X- _ O
model -X- _ O
significantly -X- _ O
improves -X- _ O
the -X- _ O
performance -X- _ O
on -X- _ O
several -X- _ O
multi -X- _ O
- -X- _ O
document -X- _ O
datasets -X- _ O
. -X- _ O

By -X- _ O
considering -X- _ O
the -X- _ O
global -X- _ O
interactions -X- _ O
between -X- _ O
entities -X- _ O
and -X- _ O
documents -X- _ O
in -X- _ O
the -X- _ O
graph -X- _ O
, -X- _ O
the -X- _ O
second -X- _ O
stage -X- _ O
is -X- _ O
able -X- _ O
to -X- _ O
handle -X- _ O
the -X- _ O
redundancy -X- _ O
issue -X- _ O
. -X- _ O

Intuitively -X- _ O
, -X- _ O
the -X- _ O
first -X- _ O
stage -X- _ O
indentifies -X- _ O
the -X- _ O
salient -X- _ O
content -X- _ O
in -X- _ O
each -X- _ O
decoding -X- _ O
step -X- _ O
. -X- _ O

Next -X- _ O
, -X- _ O
the -X- _ O
attention -X- _ O
weights -X- _ O
of -X- _ O
entities -X- _ O
are -X- _ O
incorporated -X- _ O
with -X- _ O
graph -X- _ O
edge -X- _ O
weights -X- _ O
to -X- _ O
guide -X- _ O
the -X- _ O
attention -X- _ O
to -X- _ O
the -X- _ O
documents -X- _ O
. -X- _ O

The -X- _ O
decoder -X- _ O
first -X- _ O
attends -X- _ O
to -X- _ O
the -X- _ O
entities -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
decoding -X- _ O
process -X- _ O
, -X- _ O
we -X- _ O
design -X- _ O
a -X- _ O
novel -X- _ O
two -X- _ O
- -X- _ O
level -X- _ O
attention -X- _ O
mechanism -X- _ O
. -X- _ O

The -X- _ O
entity -X- _ O
nodes -X- _ O
can -X- _ O
serve -X- _ O
as -X- _ O
bridges -X- _ O
that -X- _ O
connect -X- _ O
different -X- _ O
documents -X- _ O
– -X- _ O
we -X- _ O
can -X- _ O
model -X- _ O
the -X- _ O
relations -X- _ O
across -X- _ O
documents -X- _ O
through -X- _ O
entity -X- _ O
clusters -X- _ O
. -X- _ O

Inspired -X- _ O
by -X- _ O
Wang -X- _ O
et -X- _ O
al. -X- _ O
( -X- _ O
2020a -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
build -X- _ O
a -X- _ O
heterogeneous -X- _ O
graph -X- _ O
that -X- _ O
consists -X- _ O
of -X- _ O
nodes -X- _ O
that -X- _ O
represent -X- _ O
documents -X- _ O
and -X- _ O
entities -X- _ O
. -X- _ O

This -X- _ O
motivates -X- _ O
us -X- _ O
to -X- _ O
propose -X- _ O
an -X- _ O
entity -X- _ O
- -X- _ O
aware -X- _ O
abstractive -X- _ O
multi -X- _ B-TaskName
- -X- _ I-TaskName
document -X- _ I-TaskName
summarization -X- _ I-TaskName
model -X- _ O
that -X- _ O
effectively -X- _ O
encodes -X- _ O
relations -X- _ O
across -X- _ O
documents -X- _ O
with -X- _ O
the -X- _ O
help -X- _ O
of -X- _ O
entities -X- _ O
, -X- _ O
and -X- _ O
explicitly -X- _ O
solve -X- _ O
the -X- _ O
issues -X- _ O
of -X- _ O
saliency -X- _ O
and -X- _ O
redundancy -X- _ O
. -X- _ O

as -X- _ O
the -X- _ O
indicator -X- _ O
of -X- _ O
saliency -X- _ O
and -X- _ O
can -X- _ O
be -X- _ O
used -X- _ O
to -X- _ O
reduce -X- _ O
redundancy -X- _ O
. -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
1 -X- _ O
, -X- _ O
entity -X- _ O
mentions -X- _ O
frequently -X- _ O
appear -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
article -X- _ O
, -X- _ O
and -X- _ O
are -X- _ O
playing -X- _ O
unique -X- _ O
roles -X- _ O
that -X- _ O
contribute -X- _ O
towards -X- _ O
the -X- _ O
coherence -X- _ O
and -X- _ O
conciseness -X- _ O
of -X- _ O
the -X- _ O
text -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
while -X- _ O
effective -X- _ O
empirically -X- _ O
, -X- _ O
such -X- _ O
approaches -X- _ O
do -X- _ O
not -X- _ O
focus -X- _ O
on -X- _ O
explicitly -X- _ O
modeling -X- _ O
the -X- _ O
underlying -X- _ O
semantic -X- _ O
information -X- _ O
across -X- _ O
documents -X- _ O
. -X- _ O

Several -X- _ O
previous -X- _ O
research -X- _ O
efforts -X- _ O
have -X- _ O
shown -X- _ O
that -X- _ O
modeling -X- _ O
cross -X- _ O
- -X- _ O
document -X- _ O
relations -X- _ O
is -X- _ O
essential -X- _ O
in -X- _ O
multi -X- _ B-TaskName
- -X- _ I-TaskName
document -X- _ I-TaskName
summarization -X- _ I-TaskName
( -X- _ O
Liu -X- _ O
and -X- _ O
Lapata -X- _ O
, -X- _ O
2019a -X- _ O
; -X- _ O
Li -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O

that -X- _ O
were -X- _ O
shown -X- _ O
effective -X- _ O
for -X- _ O
single -X- _ O
- -X- _ O
document -X- _ O
summarization -X- _ O
to -X- _ O
the -X- _ O
multi -X- _ O
- -X- _ O
document -X- _ O
setup -X- _ O
may -X- _ O
not -X- _ O
lead -X- _ O
to -X- _ O
ideal -X- _ O
results -X- _ O
( -X- _ O
Lebanoff -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Zhang -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018 -X- _ O
; -X- _ O
Baumel -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2018 -X- _ O
) -X- _ O
. -X- _ O

Thus -X- _ O
, -X- _ O
simply -X- _ O
adopting -X- _ O
models -X- _ O
∗ -X- _ O

While -X- _ O
significant -X- _ O
progress -X- _ O
has -X- _ O
been -X- _ O
made -X- _ O
in -X- _ O
single -X- _ O
- -X- _ O
document -X- _ O
summarization -X- _ O
, -X- _ O
the -X- _ O
mainstream -X- _ O
sequence -X- _ O
- -X- _ O
to -X- _ O
- -X- _ O
sequence -X- _ O
models -X- _ O
, -X- _ O
which -X- _ O
can -X- _ O
perform -X- _ O
well -X- _ O
on -X- _ O
single -X- _ O
- -X- _ O
document -X- _ O
summarization -X- _ O
, -X- _ O
often -X- _ O
struggle -X- _ O
with -X- _ O
extracting -X- _ O
salient -X- _ O
information -X- _ O
and -X- _ O
handling -X- _ O
redundancy -X- _ O
in -X- _ O
the -X- _ O
presence -X- _ O
of -X- _ O
multiple -X- _ O
, -X- _ O
long -X- _ O
documents -X- _ O
. -X- _ O

It -X- _ O
is -X- _ O
a -X- _ O
task -X- _ O
that -X- _ O
can -X- _ O
be -X- _ O
more -X- _ O
challenging -X- _ O
than -X- _ O
single -X- _ O
- -X- _ O
document -X- _ O
summarization -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
presence -X- _ O
of -X- _ O
diverse -X- _ O
and -X- _ O
potentially -X- _ O
conflicting -X- _ O
information -X- _ O
( -X- _ O
Ma -X- _ O
et -X- _ O
al. -X- _ O
, -X- _ O
2020 -X- _ O
) -X- _ O
. -X- _ O

Introduction -X- _ O
Multi -X- _ B-TaskName
- -X- _ I-TaskName
document -X- _ I-TaskName
summarization -X- _ I-TaskName
aims -X- _ O
at -X- _ O
generating -X- _ O
a -X- _ O
short -X- _ O
and -X- _ O
informative -X- _ O
summary -X- _ O
across -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
topic -X- _ O
- -X- _ O
related -X- _ O
documents -X- _ O
. -X- _ O

We -X- _ O
conduct -X- _ O
comprehensive -X- _ O
experiments -X- _ O
on -X- _ O
the -X- _ O
standard -X- _ O
datasets -X- _ O
and -X- _ O
the -X- _ O
results -X- _ O
show -X- _ O
the -X- _ O
effectiveness -X- _ O
of -X- _ O
our -X- _ O
approach -X- _ O
. -X- _ O

Our -X- _ O
model -X- _ O
can -X- _ O
also -X- _ O
be -X- _ O
used -X- _ O
together -X- _ O
with -X- _ O
pre -X- _ O
- -X- _ O
trained -X- _ O
language -X- _ O
models -X- _ O
, -X- _ O
arriving -X- _ O
at -X- _ O
improved -X- _ O
performance -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
decoding -X- _ O
process -X- _ O
, -X- _ O
we -X- _ O
design -X- _ O
a -X- _ O
novel -X- _ O
two -X- _ O
- -X- _ O
level -X- _ O
attention -X- _ O
mechanism -X- _ O
, -X- _ O
allowing -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
deal -X- _ O
with -X- _ O
saliency -X- _ O
and -X- _ O
redundancy -X- _ O
issues -X- _ O
explicitly -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
paper -X- _ O
, -X- _ O
we -X- _ O
present -X- _ O
EMSum -X- _ B-MethodName
, -X- _ O
an -X- _ O
entityaware -X- _ O
model -X- _ O
for -X- _ O
abstractive -X- _ O
multi -X- _ B-TaskName
- -X- _ I-TaskName
document -X- _ I-TaskName
summarization -X- _ I-TaskName
. -X- _ O

Capturing -X- _ O
such -X- _ O
cross -X- _ O
- -X- _ O
document -X- _ O
entity -X- _ O
information -X- _ O
can -X- _ O
be -X- _ O
beneficial -X- _ O
– -X- _ O
intuitively -X- _ O
, -X- _ O
it -X- _ O
allows -X- _ O
the -X- _ O
system -X- _ O
to -X- _ O
aggregate -X- _ O
diverse -X- _ O
useful -X- _ O
information -X- _ O
around -X- _ O
the -X- _ O
same -X- _ O
entity -X- _ O
for -X- _ O
better -X- _ O
summarization -X- _ B-TaskName
. -X- _ O

In -X- _ O
multidocument -X- _ B-TaskName
summarization -X- _ I-TaskName
, -X- _ O
the -X- _ O
same -X- _ O
entity -X- _ O
may -X- _ O
appear -X- _ O
across -X- _ O
different -X- _ O
documents -X- _ O
. -X- _ O

