-DOCSTART- -X- O
4 -X- _ O
5 -X- _ O
The -X- _ O
problem -X- _ O
of -X- _ O
language -X- _ O
modeling -X- _ O
is -X- _ O
essentially -X- _ O
density -X- _ O
estimation -X- _ O
for -X- _ O
text -X- _ O
data -X- _ O
. -X- _ O

Visualizing -X- _ O
Memory -X- _ O
and -X- _ O
Permutation -X- _ O
In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
provide -X- _ O
a -X- _ O
detailed -X- _ O
visualization -X- _ O
of -X- _ O
the -X- _ O
proposed -X- _ O
permutation -X- _ O
language -X- _ O
modeling -X- _ O
objective -X- _ O
, -X- _ O
including -X- _ O
the -X- _ O
mechanism -X- _ O
of -X- _ O
reusing -X- _ O
memory -X- _ O
( -X- _ O
aka -X- _ O
the -X- _ O
recurrence -X- _ O
mechanism -X- _ O
) -X- _ O
, -X- _ O
how -X- _ O
we -X- _ O
use -X- _ O
attention -X- _ O
masks -X- _ O
to -X- _ O
permute -X- _ O
the -X- _ O
factorization -X- _ O
order -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
difference -X- _ O
of -X- _ O
the -X- _ O
two -X- _ O
attention -X- _ O
streams -X- _ O
. -X- _ O

On -X- _ O
the -X- _ O
other -X- _ O
hand -X- _ O
, -X- _ O
the -X- _ O
proposed -X- _ O
permutation -X- _ O
LM -X- _ O
objective -X- _ O
mostly -X- _ O
contributes -X- _ O
to -X- _ O
a -X- _ O
better -X- _ O
data -X- _ O
efficiency -X- _ O
, -X- _ O
whose -X- _ O
effects -X- _ O
may -X- _ O
not -X- _ O
be -X- _ O
obvious -X- _ O
from -X- _ O
qualitative -X- _ O
visualization -X- _ O
. -X- _ O

Moreover -X- _ O
, -X- _ O
it -X- _ O
becomes -X- _ O
possible -X- _ O
to -X- _ O
leverage -X- _ O
the -X- _ O
rapid -X- _ O
progress -X- _ O
of -X- _ O
language -X- _ O
modeling -X- _ O
research -X- _ O
for -X- _ O
pretraining -X- _ O
. -X- _ O

As -X- _ O
a -X- _ O
result -X- _ O
, -X- _ O
it -X- _ O
further -X- _ O
“ -X- _ O
justifies -X- _ O
” -X- _ O
language -X- _ O
modeling -X- _ O
research -X- _ O
. -X- _ O

XLNet -X- _ B-MethodName
generalizes -X- _ O
language -X- _ O
modeling -X- _ O
and -X- _ O
bridges -X- _ O
such -X- _ O
a -X- _ O
gap -X- _ O
. -X- _ O

It -X- _ O
has -X- _ O
even -X- _ O
been -X- _ O
challenged -X- _ O
by -X- _ O
some -X- _ O
machine -X- _ O
learning -X- _ O
practitioners -X- _ O
whether -X- _ O
language -X- _ O
modeling -X- _ O
is -X- _ O
a -X- _ O
meaningful -X- _ O
pursuit -X- _ O
if -X- _ O
it -X- _ O
does -X- _ O
not -X- _ O
directly -X- _ O
improve -X- _ O
downstream -X- _ O
tasks -X- _ O
5 -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
there -X- _ O
has -X- _ O
been -X- _ O
a -X- _ O
gap -X- _ O
between -X- _ O
language -X- _ O
modeling -X- _ O
and -X- _ O
pretraining -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
lack -X- _ O
of -X- _ O
the -X- _ O
capability -X- _ O
of -X- _ O
bidirectional -X- _ O
context -X- _ O
modeling -X- _ O
, -X- _ O
as -X- _ O
analyzed -X- _ O
in -X- _ O
Section -X- _ O
A.5.2 -X- _ O
. -X- _ O

The -X- _ O
representations -X- _ O
of -X- _ O
“ -X- _ O
Thom -X- _ O
Yorke -X- _ O
” -X- _ O
are -X- _ O
not -X- _ O
dependent -X- _ O
on -X- _ O
“ -X- _ O
Radiohead -X- _ O
” -X- _ O
with -X- _ O
AR -X- _ O
language -X- _ O
modeling -X- _ O
and -X- _ O
thus -X- _ O
they -X- _ O
will -X- _ O
not -X- _ O
be -X- _ O
chosen -X- _ O
as -X- _ O
the -X- _ O
answer -X- _ O
by -X- _ O
the -X- _ O
standard -X- _ O
approach -X- _ O
that -X- _ O
employs -X- _ O
softmax -X- _ O
over -X- _ O
all -X- _ O
token -X- _ O
representations -X- _ O
. -X- _ O

Such -X- _ O
a -X- _ O
limitation -X- _ O
of -X- _ O
AR -X- _ O
language -X- _ O
modeling -X- _ O
can -X- _ O
be -X- _ O
critical -X- _ O
in -X- _ O
real -X- _ O
- -X- _ O
world -X- _ O
applications -X- _ O
. -X- _ O

A.5 -X- _ O
A.5.1 -X- _ O
Yelp-5 -X- _ O
512 -X- _ O
128 -X- _ O
1e-5 -X- _ O
10 -X- _ O

Number -X- _ O
of -X- _ O
steps -X- _ O
12 -X- _ O
K -X- _ O
8 -X- _ O
K -X- _ O
10 -X- _ O
K -X- _ O
Learning -X- _ O
rate -X- _ O
decay -X- _ O
linear -X- _ O
Weight -X- _ O
decay -X- _ O
0.01 -X- _ O
Adam -X- _ O
epsilon -X- _ O
1e-6 -X- _ O
1e-6 -X- _ O
1e-6 -X- _ O
Layer -X- _ O
- -X- _ O
wise -X- _ O
lr -X- _ O
decay -X- _ O
1.0 -X- _ O
0.75 -X- _ O
1.0 -X- _ O
Table -X- _ O
8 -X- _ O
: -X- _ O
Hyperparameters -X- _ O
for -X- _ O
finetuning -X- _ O
. -X- _ O

2e-5 -X- _ O
3e-5 -X- _ O
2e-5 -X- _ O

Hparam -X- _ O
RACE -X- _ O
SQuAD -X- _ O
MNLI -X- _ O
Dropout -X- _ O
0.1 -X- _ O
Attention -X- _ O
dropout -X- _ O
0.1 -X- _ O
Max -X- _ O
sequence -X- _ O
length -X- _ O
512 -X- _ O
512 -X- _ O
128 -X- _ O
Batch -X- _ O
size -X- _ O
32 -X- _ O
48 -X- _ O
128 -X- _ O
Learning -X- _ O
rate -X- _ O

For -X- _ O
example -X- _ O
, -X- _ O
suppose -X- _ O
the -X- _ O
24 -X- _ O
- -X- _ O
th -X- _ O
layer -X- _ O
uses -X- _ O
a -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
l -X- _ B-HyperparameterName
, -X- _ O
and -X- _ O
the -X- _ O
Layer -X- _ B-HyperparameterName
- -X- _ I-HyperparameterName
wise -X- _ I-HyperparameterName
decay -X- _ I-HyperparameterName
rate -X- _ I-HyperparameterName
is -X- _ O
α -X- _ B-HyperparameterName
, -X- _ O
then -X- _ O
the -X- _ O
learning -X- _ B-HyperparameterName
rate -X- _ I-HyperparameterName
of -X- _ O
layer -X- _ O
m -X- _ O
is -X- _ O
lα24−m -X- _ B-HyperparameterValue
. -X- _ O

“ -X- _ O
Layer -X- _ B-HyperparameterName
- -X- _ I-HyperparameterName
wise -X- _ I-HyperparameterName
decay -X- _ I-HyperparameterName
” -X- _ O
means -X- _ O
exponentially -X- _ O
decaying -X- _ O
the -X- _ O
learning -X- _ B-HyperparameterName
rates -X- _ I-HyperparameterName
of -X- _ O
individual -X- _ O
layers -X- _ O
in -X- _ O
a -X- _ O
top -X- _ O
- -X- _ O
down -X- _ O
manner -X- _ O
. -X- _ O

Learning -X- _ O
rate -X- _ O
decay -X- _ O
linear -X- _ O
Adam -X- _ O
epsilon -X- _ O
1e-6 -X- _ O
Weight -X- _ O
decay -X- _ O
0.01 -X- _ O
Table -X- _ O
7 -X- _ O
: -X- _ O
Hyperparameters -X- _ O
for -X- _ O
pretraining -X- _ O
. -X- _ O

40,000 -X- _ O

Dropout -X- _ O
0.0 -X- _ O
Attention -X- _ O
dropout -X- _ O
0.1 -X- _ O
Partial -X- _ O
prediction -X- _ O
K -X- _ O
6 -X- _ O
Max -X- _ O
sequence -X- _ O
length -X- _ O
512 -X- _ O
Batch -X- _ O
size -X- _ O
8192 -X- _ O
Learning -X- _ O
rate -X- _ O
4e-4 -X- _ O
Number -X- _ O
of -X- _ O
steps -X- _ O
500 -X- _ O
K -X- _ O
Warmup -X- _ O
steps -X- _ O

Hidden -X- _ O
size -X- _ O
1024 -X- _ O
Number -X- _ O
of -X- _ O
attention -X- _ O
heads -X- _ O
16 -X- _ O
Attention -X- _ O
head -X- _ O
size -X- _ O
64 -X- _ O
FFN -X- _ O
inner -X- _ O
hidden -X- _ O
size -X- _ O
4096 -X- _ O
Hidden -X- _ O
Dropout -X- _ O
0.1 -X- _ O
GeLU -X- _ O

A.4 -X- _ O
A.4.1 -X- _ O
Hyperparameters -X- _ O
Pretraining -X- _ O
Hyperparameters -X- _ O
Hparam -X- _ O
Value -X- _ O
Number -X- _ O
of -X- _ O
layers -X- _ O
24 -X- _ O

Here -X- _ O
, -X- _ O
we -X- _ O
provide -X- _ O
the -X- _ O
implementation -X- _ O
details -X- _ O
of -X- _ O
the -X- _ O
two -X- _ O
- -X- _ O
stream -X- _ O
attention -X- _ O
with -X- _ O
a -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
backbone -X- _ O
. -X- _ O

A -X- _ O
Target -X- _ O
- -X- _ O
Aware -X- _ O
Representation -X- _ O
via -X- _ O
Two -X- _ O
- -X- _ O
Stream -X- _ O
Self -X- _ O
- -X- _ O
Attention -X- _ O
A.1 -X- _ O

The -X- _ O
neural -X- _ O
architecture -X- _ O
of -X- _ O
XLNet -X- _ B-MethodName
is -X- _ O
developed -X- _ O
to -X- _ O
work -X- _ O
seamlessly -X- _ O
with -X- _ O
the -X- _ O
AR -X- _ O
objective -X- _ O
, -X- _ O
including -X- _ O
integrating -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
and -X- _ O
the -X- _ O
careful -X- _ O
design -X- _ O
of -X- _ O
the -X- _ O
two -X- _ O
- -X- _ O
stream -X- _ O
attention -X- _ O
mechanism -X- _ O
. -X- _ O

4 -X- _ O
Conclusions -X- _ O
XLNet -X- _ B-MethodName
is -X- _ O
a -X- _ O
generalized -X- _ O
AR -X- _ O
pretraining -X- _ O
method -X- _ O
that -X- _ O
uses -X- _ O
a -X- _ O
permutation -X- _ O
language -X- _ O
modeling -X- _ O
objective -X- _ O
to -X- _ O
combine -X- _ O
the -X- _ O
advantages -X- _ O
of -X- _ O
AR -X- _ O
and -X- _ O
AE -X- _ O
methods -X- _ O
. -X- _ O

Hence -X- _ O
, -X- _ O
we -X- _ O
exclude -X- _ O
the -X- _ O
next -X- _ O
- -X- _ O
sentence -X- _ O
prediction -X- _ O
objective -X- _ O
from -X- _ O
XLNet -X- _ B-MethodName
. -X- _ O

Finally -X- _ O
, -X- _ O
we -X- _ O
unexpectedly -X- _ O
find -X- _ O
the -X- _ O
the -X- _ O
next -X- _ O
- -X- _ O
sentence -X- _ O
prediction -X- _ O
objective -X- _ O
proposed -X- _ O
in -X- _ O
the -X- _ O
original -X- _ O
BERT -X- _ B-MethodName
does -X- _ O
not -X- _ O
necessarily -X- _ O
lead -X- _ O
to -X- _ O
an -X- _ O
improvement -X- _ O
in -X- _ O
our -X- _ O
setting -X- _ O
. -X- _ O

Examining -X- _ O
rows -X- _ O
1 -X- _ O
- -X- _ O
4 -X- _ O
of -X- _ O
Table -X- _ O
6 -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
see -X- _ O
both -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
and -X- _ O
the -X- _ O
permutation -X- _ O
LM -X- _ O
clearly -X- _ O
contribute -X- _ O
the -X- _ O
superior -X- _ O
performance -X- _ O
of -X- _ O
XLNet -X- _ B-MethodName
over -X- _ O
BERT -X- _ B-MethodName
. -X- _ O

DAE -X- _ O
+ -X- _ O
Transformer -X- _ O
- -X- _ O
XL -X- _ O
3 -X- _ O
XLNet -X- _ O
- -X- _ O
Base -X- _ O
( -X- _ O
K -X- _ O
= -X- _ O
7 -X- _ O
) -X- _ O
66.05 -X- _ O
81.33 -X- _ O
78.46 -X- _ O
85.84/85.43 -X- _ O
92.66 -X- _ O
4 -X- _ O
XLNet -X- _ O
- -X- _ O
Base -X- _ O
( -X- _ O
K -X- _ O
= -X- _ O
6 -X- _ O
) -X- _ O
66.66 -X- _ O
80.98 -X- _ O
78.18 -X- _ O
85.63/85.12 -X- _ O
93.35 -X- _ O
5 -X- _ O
- -X- _ O
memory -X- _ O
65.55 -X- _ O
80.15 -X- _ O
77.27 -X- _ O
85.32/85.05 -X- _ O
92.78 -X- _ O
6 -X- _ O
- -X- _ O
span -X- _ O
- -X- _ O
based -X- _ O
pred -X- _ O
65.95 -X- _ O
80.61 -X- _ O
77.91 -X- _ O
85.49/85.02 -X- _ O
93.12 -X- _ O
7 -X- _ O
- -X- _ O
bidirectional -X- _ O
data -X- _ O
66.34 -X- _ O
80.65 -X- _ O
77.87 -X- _ O
85.31/84.99 -X- _ O
92.66 -X- _ O
66.76 -X- _ O
79.83 -X- _ O
76.94 -X- _ O
85.32/85.09 -X- _ O
92.89 -X- _ O
8 -X- _ O
+ -X- _ O
next -X- _ O
- -X- _ O
sent -X- _ O
pred -X- _ O
Table -X- _ O
6 -X- _ O
: -X- _ O
The -X- _ O
results -X- _ O
of -X- _ O
BERT -X- _ O
on -X- _ O
RACE -X- _ O
are -X- _ O
taken -X- _ O
from -X- _ O
[ -X- _ O
38 -X- _ O
] -X- _ O
. -X- _ O

m -X- _ O
/ -X- _ O
mm -X- _ O
SST-2 -X- _ O
1 -X- _ O
BERT -X- _ O
- -X- _ O
Base -X- _ O
64.3 -X- _ O
76.30 -X- _ O
73.66 -X- _ O
84.34/84.65 -X- _ O
92.78 -X- _ O
65.03 -X- _ O
79.56 -X- _ O
76.80 -X- _ O
84.88/84.45 -X- _ O
92.60 -X- _ O
2 -X- _ O

# -X- _ O
Model -X- _ O
RACE -X- _ O
SQuAD2.0 -X- _ O
F1 -X- _ O
EM -X- _ O
MNLI -X- _ O

All -X- _ O
results -X- _ O
reported -X- _ O
are -X- _ O
the -X- _ O
median -X- _ O
of -X- _ O
5 -X- _ B-HyperparameterValue
runs -X- _ B-HyperparameterName
. -X- _ O

With -X- _ O
these -X- _ O
purposes -X- _ O
in -X- _ O
mind -X- _ O
, -X- _ O
in -X- _ O
Table -X- _ O
6 -X- _ O
, -X- _ O
we -X- _ O
compare -X- _ O
6 -X- _ O
XLNet -X- _ B-MethodName
- -X- _ O
Base -X- _ O
variants -X- _ O
with -X- _ O
different -X- _ O
implementation -X- _ O
details -X- _ O
( -X- _ O
rows -X- _ O
3 -X- _ O
- -X- _ O
8) -X- _ O
, -X- _ O
the -X- _ O
original -X- _ O
BERT -X- _ B-MethodName
- -X- _ I-MethodName
Base -X- _ I-MethodName
model -X- _ O
( -X- _ O
row -X- _ O
1 -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
an -X- _ O
additional -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
baseline -X- _ O
trained -X- _ O
with -X- _ O
the -X- _ O
denoising -X- _ O
auto -X- _ O
- -X- _ O
encoding -X- _ O
( -X- _ O
DAE -X- _ O
) -X- _ O
objective -X- _ O
used -X- _ O
in -X- _ O
BERT -X- _ B-MethodName
but -X- _ O
with -X- _ O
the -X- _ O
bidirectional -X- _ O
input -X- _ O
pipeline -X- _ O
( -X- _ O
row -X- _ O
2 -X- _ O
) -X- _ O
. -X- _ O

• -X- _ O
The -X- _ O
necessity -X- _ O
of -X- _ O
some -X- _ O
implementation -X- _ O
details -X- _ O
including -X- _ O
span -X- _ O
- -X- _ O
based -X- _ O
prediction -X- _ O
, -X- _ O
the -X- _ O
bidirectional -X- _ O
input -X- _ O
pipeline -X- _ O
, -X- _ O
and -X- _ O
next -X- _ O
- -X- _ O
sentence -X- _ O
prediction -X- _ O
. -X- _ O

Specifically -X- _ O
, -X- _ O
there -X- _ O
are -X- _ O
three -X- _ O
main -X- _ O
aspects -X- _ O
we -X- _ O
hope -X- _ O
to -X- _ O
study -X- _ O
: -X- _ O
• -X- _ O
The -X- _ O
effectiveness -X- _ O
of -X- _ O
the -X- _ O
permutation -X- _ O
language -X- _ O
modeling -X- _ O
objective -X- _ O
alone -X- _ O
, -X- _ O
especially -X- _ O
compared -X- _ O
to -X- _ O
the -X- _ O
denoising -X- _ O
auto -X- _ O
- -X- _ O
encoding -X- _ O
objective -X- _ O
used -X- _ O
by -X- _ O
BERT -X- _ B-MethodName
. -X- _ O

90.4† -X- _ O
88.5 -X- _ O
97.1† -X- _ O
92.9 -X- _ O
70.2 -X- _ O
93.0 -X- _ O
92.5 -X- _ O
Table -X- _ O
5 -X- _ O
: -X- _ O
Results -X- _ O
on -X- _ O
GLUE -X- _ O
. -X- _ O

[ -X- _ O
2 -X- _ O
] -X- _ O
86.6/92.3 -X- _ O
RoBERTa -X- _ O
[ -X- _ O
21 -X- _ O
] -X- _ O
90.2/90.2 -X- _ O
94.7 -X- _ O
XLNet -X- _ O
90.8/90.8 -X- _ O
94.9 -X- _ O
91.3 -X- _ O
92.2 -X- _ O
92.3 -X- _ O
70.4 -X- _ O
86.6 -X- _ O
85.9 -X- _ O
93.2 -X- _ O
96.4 -X- _ O
97.0 -X- _ O
88.0 -X- _ O
90.9 -X- _ O
90.8 -X- _ O
60.6 -X- _ O
68.0 -X- _ O
69.0 -X- _ O
90.0 -X- _ O
92.4 -X- _ O
92.5 -X- _ O
Multi -X- _ O
- -X- _ O
task -X- _ O
ensembles -X- _ O
on -X- _ O
test -X- _ O
( -X- _ O
from -X- _ O
leaderboard -X- _ O
as -X- _ O
of -X- _ O
Oct -X- _ O
28 -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
MT -X- _ O
- -X- _ O
DNN∗ -X- _ O
[ -X- _ O
20 -X- _ O
] -X- _ O
87.9/87.4 -X- _ O
96.0 -X- _ O
89.9 -X- _ O
86.3 -X- _ O
96.5 -X- _ O
92.7 -X- _ O
68.4 -X- _ O
91.1 -X- _ O
89.0 -X- _ O
RoBERTa∗ -X- _ O
[ -X- _ O
21 -X- _ O
] -X- _ O
90.8/90.2 -X- _ O
98.9 -X- _ O
90.2 -X- _ O
88.2 -X- _ O
96.7 -X- _ O
92.3 -X- _ O
67.8 -X- _ O
92.2 -X- _ O
89.0 -X- _ O
XLNet∗ -X- _ O
90.9/90.9† -X- _ O
99.0† -X- _ O

Single -X- _ O
- -X- _ O
task -X- _ O
single -X- _ O
models -X- _ O
on -X- _ O
dev -X- _ O
BERT -X- _ O

QNLI -X- _ O
QQP -X- _ O
RTE -X- _ O
SST-2 -X- _ O
MRPC -X- _ O
CoLA -X- _ O
STS -X- _ O
- -X- _ O
B -X- _ O
WNLI -X- _ O

Model -X- _ O
MNLI -X- _ O

All -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
XLNet -X- _ B-MethodName
results -X- _ O
are -X- _ O
obtained -X- _ O
with -X- _ O
a -X- _ O
24 -X- _ B-HyperparameterValue
- -X- _ O
layer -X- _ B-HyperparameterName
architecture -X- _ O
with -X- _ O
similar -X- _ O
model -X- _ O
sizes -X- _ O
( -X- _ O
aka -X- _ O
BERT -X- _ B-MethodName
- -X- _ I-MethodName
Large -X- _ I-MethodName
) -X- _ O
. -X- _ O

[ -X- _ O
35 -X- _ O
] -X- _ O
4.32 -X- _ O
4.6 -X- _ O
4.51 -X- _ O
2.90 -X- _ O
2.64 -X- _ O
2.16 -X- _ O
1.89 -X- _ O
32.39 -X- _ O
30.58 -X- _ O
29.98 -X- _ O
29.32 -X- _ O
0.84 -X- _ O
0.88 -X- _ O
0.70 -X- _ O
0.80 -X- _ O
0.64 -X- _ O
6.57 -X- _ O
6.87 -X- _ O
4.95 -X- _ O
5.01 -X- _ O
3.79 -X- _ O
3.32 -X- _ O
2.63 -X- _ O
36.24 -X- _ O
34.81 -X- _ O
34.17 -X- _ O
XLNet -X- _ O
3.20 -X- _ O
1.37 -X- _ O
27.05 -X- _ O
0.60 -X- _ O
4.45 -X- _ O
2.11 -X- _ O
31.67 -X- _ O
Table -X- _ O
4 -X- _ O
: -X- _ O
Comparison -X- _ O
with -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
error -X- _ O
rates -X- _ O
on -X- _ O
the -X- _ O
test -X- _ O
sets -X- _ O
of -X- _ O
several -X- _ O
text -X- _ O
classification -X- _ O
datasets -X- _ O
. -X- _ O

[ -X- _ O
15 -X- _ O
] -X- _ O
Mixed -X- _ O
VAT -X- _ O
[ -X- _ O
31 -X- _ O
, -X- _ O
23 -X- _ O
] -X- _ O
ULMFiT -X- _ O
[ -X- _ O
14 -X- _ O
] -X- _ O
BERT -X- _ O

Model -X- _ O
IMDB -X- _ O
Yelp-2 -X- _ O
Yelp-5 -X- _ O
DBpedia -X- _ O
AG -X- _ O
Amazon-2 -X- _ O
Amazon-5 -X- _ O
CNN -X- _ O
[ -X- _ O
15 -X- _ O
] -X- _ O
DPCNN -X- _ O

[ -X- _ O
10 -X- _ O
] -X- _ O
80.005 -X- _ O
83.061 -X- _ O
BERT -X- _ O
[ -X- _ O
10 -X- _ O
] -X- _ O
85.083 -X- _ O
91.835 -X- _ O
RoBERTa -X- _ O
[ -X- _ O
21 -X- _ O
] -X- _ O
86.820 -X- _ O
89.795 -X- _ O
BERT∗ -X- _ O
[ -X- _ O
10 -X- _ O
] -X- _ O
87.433 -X- _ O
93.294 -X- _ O
XLNet -X- _ O
87.926 -X- _ O
90.689 -X- _ O
XLNet -X- _ O
89.898‡ -X- _ O
95.080‡ -X- _ O
Table -X- _ O
3 -X- _ O
: -X- _ O
Results -X- _ O
on -X- _ O
SQuAD -X- _ O
, -X- _ O
a -X- _ O
reading -X- _ O
comprehension -X- _ O
dataset -X- _ O
. -X- _ O

[ -X- _ O
10 -X- _ O
] -X- _ O
RoBERTa -X- _ O
[ -X- _ O
21 -X- _ O
] -X- _ O
XLNet -X- _ O
84.1 -X- _ O
88.9 -X- _ O
89.7 -X- _ O
90.9 -X- _ O
94.6 -X- _ O
95.1 -X- _ O
Test -X- _ O
set -X- _ O
results -X- _ O
on -X- _ O
leaderboard -X- _ O
( -X- _ O
single -X- _ O
model -X- _ O
, -X- _ O
as -X- _ O
of -X- _ O
Dec -X- _ O
14 -X- _ O
, -X- _ O
2019 -X- _ O
) -X- _ O
BERT -X- _ O

SQuAD2.0 -X- _ O
EM -X- _ O
F1 -X- _ O
Dev -X- _ O
set -X- _ O
results -X- _ O
( -X- _ O
single -X- _ O
model -X- _ O
) -X- _ O
BERT -X- _ O
[ -X- _ O
10 -X- _ O
] -X- _ O
78.98 -X- _ O
81.77 -X- _ O
RoBERTa -X- _ O
[ -X- _ O
21 -X- _ O
] -X- _ O
86.5 -X- _ O
89.4 -X- _ O
XLNet -X- _ O
87.9 -X- _ O
90.6 -X- _ O
SQuAD1.1 -X- _ O
EM -X- _ O
F1 -X- _ O
BERT† -X- _ O

Since -X- _ O
ALBERT -X- _ B-MethodName
involves -X- _ O
increasing -X- _ O
the -X- _ O
model -X- _ O
hidden -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
from -X- _ O
1024 -X- _ B-HyperparameterValue
to -X- _ O
2048/4096 -X- _ B-HyperparameterValue
and -X- _ O
thus -X- _ O
substantially -X- _ O
increases -X- _ O
the -X- _ O
amount -X- _ O
of -X- _ O
computation -X- _ O
in -X- _ O
terms -X- _ O
of -X- _ O
FLOPs -X- _ O
, -X- _ O
we -X- _ O
exclude -X- _ O
ALBERT -X- _ B-MethodName
from -X- _ O
the -X- _ O
following -X- _ O
results -X- _ O
as -X- _ O
it -X- _ O
is -X- _ O
hard -X- _ O
to -X- _ O
lead -X- _ O
to -X- _ O
scientific -X- _ O
conclusions -X- _ O
. -X- _ O

All -X- _ O
BERT -X- _ O
, -X- _ O
RoBERTa -X- _ O
, -X- _ O
and -X- _ O
XLNet -X- _ O
results -X- _ O
are -X- _ O
obtained -X- _ O
with -X- _ O
a -X- _ O
24 -X- _ O
- -X- _ O
layer -X- _ O
architecture -X- _ O
with -X- _ O
similar -X- _ O
model -X- _ O
sizes -X- _ O
( -X- _ O
aka -X- _ O
BERT -X- _ O
- -X- _ O
Large -X- _ O
) -X- _ O
. -X- _ O

BERT+DCMN∗ -X- _ O
[ -X- _ O
38 -X- _ O
] -X- _ O
RoBERTa -X- _ O
[ -X- _ O
21 -X- _ O
] -X- _ O
Accuracy -X- _ O
Middle -X- _ O
High -X- _ O
Model -X- _ O
59.0 -X- _ O
72.0 -X- _ O
74.1 -X- _ O
83.2 -X- _ O
62.9 -X- _ O
76.6 -X- _ O
79.5 -X- _ O
86.5 -X- _ O
57.4 -X- _ O
70.1 -X- _ O
71.8 -X- _ O
81.8 -X- _ O
DRMM -X- _ O
[ -X- _ O
13 -X- _ O
] -X- _ O
KNRM -X- _ O
[ -X- _ O
8 -X- _ O
] -X- _ O
Conv -X- _ O
[ -X- _ O
8 -X- _ O
] -X- _ O
BERT† -X- _ O
NDCG@20 -X- _ O
ERR@20 -X- _ O
24.3 -X- _ O
26.9 -X- _ O
28.7 -X- _ O
30.53 -X- _ O
13.8 -X- _ O
14.9 -X- _ O
18.1 -X- _ O
18.67 -X- _ O
XLNet -X- _ O
85.4 -X- _ O
88.6 -X- _ O
84.0 -X- _ O
XLNet -X- _ O
31.10 -X- _ O
20.28 -X- _ O
Table -X- _ O
2 -X- _ O
: -X- _ O
Comparison -X- _ O
with -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
results -X- _ O
on -X- _ O
the -X- _ O
test -X- _ O
set -X- _ O
of -X- _ O
RACE -X- _ O
, -X- _ O
a -X- _ O
reading -X- _ O
comprehension -X- _ O
task -X- _ O
, -X- _ O
and -X- _ O
on -X- _ O
ClueWeb09 -X- _ O
- -X- _ O
B -X- _ O
, -X- _ O
a -X- _ O
document -X- _ O
ranking -X- _ O
task -X- _ O
. -X- _ O

3.3 -X- _ O
Comparison -X- _ O
with -X- _ O
RoBERTa -X- _ B-MethodName
: -X- _ O
Scaling -X- _ O
Up -X- _ O
RACE -X- _ O
GPT -X- _ O
[ -X- _ O
28 -X- _ O
] -X- _ O
BERT -X- _ O
[ -X- _ O
25 -X- _ O
] -X- _ O

We -X- _ O
use -X- _ O
the -X- _ O
best -X- _ O
of -X- _ O
3 -X- _ O
BERT -X- _ B-MethodName
variants -X- _ O
for -X- _ O
comparison -X- _ O
; -X- _ O
i.e. -X- _ O
, -X- _ O
the -X- _ O
original -X- _ O
BERT -X- _ B-MethodName
, -X- _ O
BERT -X- _ B-MethodName
with -X- _ O
whole -X- _ O
word -X- _ O
masking -X- _ O
, -X- _ O
and -X- _ O
BERT -X- _ B-MethodName
without -X- _ O
next -X- _ O
sentence -X- _ O
prediction -X- _ O
. -X- _ O

Since -X- _ O
the -X- _ O
recurrence -X- _ O
mechanism -X- _ O
is -X- _ O
introduced -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
bidirectional -X- _ O
data -X- _ O
input -X- _ O
pipeline -X- _ O
where -X- _ O
each -X- _ O
of -X- _ O
the -X- _ O
forward -X- _ O
and -X- _ O
backward -X- _ O
directions -X- _ O
takes -X- _ O
half -X- _ B-HyperparameterValue
of -X- _ O
the -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
. -X- _ O

Specifically -X- _ O
, -X- _ O
we -X- _ O
train -X- _ O
on -X- _ O
512 -X- _ O
TPU -X- _ O
v3 -X- _ O
chips -X- _ O
for -X- _ O
500 -X- _ B-HyperparameterValue
K -X- _ I-HyperparameterValue
steps -X- _ B-HyperparameterName
with -X- _ O
an -X- _ O
Adam -X- _ B-HyperparameterName
weight -X- _ I-HyperparameterName
decay -X- _ I-HyperparameterName
optimizer -X- _ I-HyperparameterName
, -X- _ O
linear -X- _ B-HyperparameterName
learning -X- _ I-HyperparameterName
rate -X- _ I-HyperparameterName
decay -X- _ I-HyperparameterName
, -X- _ O
and -X- _ O
a -X- _ O
batch -X- _ B-HyperparameterName
size -X- _ I-HyperparameterName
of -X- _ O
8192 -X- _ B-HyperparameterValue
, -X- _ O
which -X- _ O
takes -X- _ O
about -X- _ O
5.5 -X- _ O
days -X- _ O
. -X- _ O

Firstly -X- _ O
, -X- _ O
to -X- _ O
provide -X- _ O
a -X- _ O
fair -X- _ O
comparison -X- _ O
with -X- _ O
BERT -X- _ B-MethodName
( -X- _ O
section -X- _ O
3.2 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
also -X- _ O
trained -X- _ O
XLNet -X- _ B-MethodName
- -X- _ I-MethodName
Large -X- _ I-MethodName
- -X- _ I-MethodName
wikibooks -X- _ I-MethodName
on -X- _ O
BooksCorpus -X- _ B-DatasetName
and -X- _ O
Wikipedia -X- _ B-DatasetName
only -X- _ O
, -X- _ O
where -X- _ O
we -X- _ O
reuse -X- _ O
all -X- _ O
pretraining -X- _ O
hyper -X- _ O
- -X- _ O
parameters -X- _ O
as -X- _ O
in -X- _ O
the -X- _ O
original -X- _ O
BERT -X- _ B-MethodName
. -X- _ O

During -X- _ O
pretraining -X- _ O
, -X- _ O
we -X- _ O
always -X- _ O
use -X- _ O
a -X- _ O
full -X- _ O
sequence -X- _ B-HyperparameterName
length -X- _ I-HyperparameterName
of -X- _ O
512 -X- _ B-HyperparameterValue
. -X- _ O

After -X- _ O
tokenization -X- _ O
with -X- _ O
SentencePiece -X- _ O
[ -X- _ O
17 -X- _ O
] -X- _ O
, -X- _ O
we -X- _ O
obtain -X- _ O
2.78B -X- _ O
, -X- _ O
1.09B -X- _ O
, -X- _ O
4.75B -X- _ O
, -X- _ O
4.30B -X- _ O
, -X- _ O
and -X- _ O
19.97B -X- _ O
subword -X- _ O
pieces -X- _ O
for -X- _ O
Wikipedia -X- _ B-DatasetName
, -X- _ O
BooksCorpus -X- _ B-DatasetName
, -X- _ O
Giga5 -X- _ B-DatasetName
, -X- _ O
ClueWeb -X- _ B-DatasetName
, -X- _ O
and -X- _ O
Common -X- _ B-DatasetName
Crawl -X- _ I-DatasetName
respectively -X- _ O
, -X- _ O
which -X- _ O
are -X- _ O
32.89B -X- _ O
in -X- _ O
total -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
, -X- _ O
for -X- _ O
both -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
XLNet -X- _ B-MethodName
, -X- _ O
partial -X- _ O
prediction -X- _ O
plays -X- _ O
a -X- _ O
role -X- _ O
of -X- _ O
reducing -X- _ O
optimization -X- _ O
difficulty -X- _ O
by -X- _ O
only -X- _ O
predicting -X- _ O
tokens -X- _ O
with -X- _ O
sufficient -X- _ O
context -X- _ O
. -X- _ O

2.6 -X- _ O
Discussion -X- _ O
Comparing -X- _ O
Eq -X- _ O
. -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
and -X- _ O
( -X- _ O
5 -X- _ O
) -X- _ O
, -X- _ O
we -X- _ O
observe -X- _ O
that -X- _ O
both -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
XLNet -X- _ B-MethodName
perform -X- _ O
partial -X- _ O
prediction -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
only -X- _ O
predicting -X- _ O
a -X- _ O
subset -X- _ O
of -X- _ O
tokens -X- _ O
in -X- _ O
the -X- _ O
sequence -X- _ O
. -X- _ O

we -X- _ O
follow -X- _ O
the -X- _ O
two -X- _ O
- -X- _ O
segment -X- _ O
data -X- _ O
format -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
- -X- _ I-MethodName
Large -X- _ I-MethodName
does -X- _ O
not -X- _ O
use -X- _ O
the -X- _ O
objective -X- _ O
of -X- _ O
next -X- _ O
sentence -X- _ O
prediction -X- _ O
[ -X- _ O
10 -X- _ O
] -X- _ O
as -X- _ O
it -X- _ O
does -X- _ O
not -X- _ O
show -X- _ O
consistent -X- _ O
improvement -X- _ O
in -X- _ O
our -X- _ O
ablation -X- _ O
study -X- _ O
( -X- _ O
see -X- _ O
Section -X- _ O
3.4 -X- _ O
) -X- _ O
. -X- _ O

During -X- _ O
the -X- _ O
pretraining -X- _ O
phase -X- _ O
, -X- _ O
following -X- _ O
BERT -X- _ B-MethodName
, -X- _ O
we -X- _ O
randomly -X- _ O
sample -X- _ O
two -X- _ O
segments -X- _ O
( -X- _ O
either -X- _ O
from -X- _ O
the -X- _ O
same -X- _ O
context -X- _ O
or -X- _ O
not -X- _ O
) -X- _ O
and -X- _ O
treat -X- _ O
the -X- _ O
concatenation -X- _ O
of -X- _ O
two -X- _ O
segments -X- _ O
as -X- _ O
one -X- _ O
sequence -X- _ O
to -X- _ O
perform -X- _ O
permutation -X- _ O
language -X- _ O
modeling -X- _ O
. -X- _ O

We -X- _ O
now -X- _ O
discuss -X- _ O
how -X- _ O
we -X- _ O
pretrain -X- _ O
XLNet -X- _ B-MethodName
to -X- _ O
model -X- _ O
multiple -X- _ O
segments -X- _ O
in -X- _ O
the -X- _ O
autoregressive -X- _ O
framework -X- _ O
. -X- _ O

2.5 -X- _ O
Modeling -X- _ O
Multiple -X- _ O
Segments -X- _ O
Many -X- _ O
downstream -X- _ O
tasks -X- _ O
have -X- _ O
multiple -X- _ O
input -X- _ O
segments -X- _ O
, -X- _ O
e.g. -X- _ O
, -X- _ O
a -X- _ O
question -X- _ O
and -X- _ O
a -X- _ O
context -X- _ O
paragraph -X- _ O
in -X- _ O
question -X- _ B-TaskName
answering -X- _ I-TaskName
. -X- _ O

Finally -X- _ O
, -X- _ O
Figure -X- _ O
1 -X- _ O
( -X- _ O
c -X- _ O
) -X- _ O
presents -X- _ O
an -X- _ O
overview -X- _ O
of -X- _ O
the -X- _ O
proposed -X- _ O
permutation -X- _ O
language -X- _ O
modeling -X- _ O
with -X- _ O
two -X- _ O
- -X- _ O
stream -X- _ O
attention -X- _ O
( -X- _ O
see -X- _ O
Appendix -X- _ O
A.7 -X- _ O
for -X- _ O
more -X- _ O
detailed -X- _ O
illustration -X- _ O
) -X- _ O
. -X- _ O

While -X- _ O
the -X- _ O
permutation -X- _ O
language -X- _ O
modeling -X- _ O
objective -X- _ O
( -X- _ O
3 -X- _ O
) -X- _ O
has -X- _ O
several -X- _ O
benefits -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
a -X- _ O
much -X- _ O
more -X- _ O
challenging -X- _ O
optimization -X- _ O
problem -X- _ O
due -X- _ O
to -X- _ O
the -X- _ O
permutation -X- _ O
and -X- _ O
causes -X- _ O
slow -X- _ O
convergence -X- _ O
in -X- _ O
preliminary -X- _ O
experiments -X- _ O
. -X- _ O

, -X- _ O
M -X- _ O
, -X- _ O
the -X- _ O
two -X- _ O
streams -X- _ O
of -X- _ O
representations -X- _ O
are -X- _ O
schematically2 -X- _ O
updated -X- _ O
2 -X- _ O
To -X- _ O
avoid -X- _ O
clutter -X- _ O
, -X- _ O
we -X- _ O
omit -X- _ O
the -X- _ O
implementation -X- _ O
details -X- _ O
including -X- _ O
multi -X- _ O
- -X- _ O
head -X- _ O
attention -X- _ O
, -X- _ O
residual -X- _ O
connection -X- _ O
, -X- _ O
layer -X- _ O
normalization -X- _ O
and -X- _ O
position -X- _ O
- -X- _ O
wise -X- _ O
feed -X- _ O
- -X- _ O
forward -X- _ O
as -X- _ O
used -X- _ O
in -X- _ O
Transformer(-XL -X- _ B-MethodName
) -X- _ I-MethodName
. -X- _ O

While -X- _ O
the -X- _ O
idea -X- _ O
of -X- _ O
target -X- _ O
- -X- _ O
aware -X- _ O
representations -X- _ O
removes -X- _ O
the -X- _ O
ambiguity -X- _ O
in -X- _ O
target -X- _ O
prediction -X- _ O
, -X- _ O
how -X- _ O
to -X- _ O
formulate -X- _ O
gθ -X- _ O
( -X- _ O
xz -X- _ O
< -X- _ O
t -X- _ O
, -X- _ O
zt -X- _ O
) -X- _ O
remains -X- _ O
a -X- _ O
non -X- _ O
- -X- _ O
trivial -X- _ O
problem -X- _ O
. -X- _ O

Two -X- _ O
- -X- _ O
Stream -X- _ O
Self -X- _ O
- -X- _ O
Attention -X- _ O

To -X- _ O
avoid -X- _ O
this -X- _ O
problem -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
to -X- _ O
re -X- _ O
- -X- _ O
parameterize -X- _ O
the -X- _ O
next -X- _ O
- -X- _ O
token -X- _ O
distribution -X- _ O
to -X- _ O
be -X- _ O
target -X- _ O
position -X- _ O
aware -X- _ O
: -X- _ O
 -X- _ O
exp -X- _ O
e(x -X- _ O
) -X- _ O
> -X- _ O

While -X- _ O
the -X- _ O
permutation -X- _ O
language -X- _ O
modeling -X- _ O
objective -X- _ O
has -X- _ O
desired -X- _ O
properties -X- _ O
, -X- _ O
naive -X- _ O
implementation -X- _ O
with -X- _ O
standard -X- _ O
Transformer -X- _ O
parameterization -X- _ O
may -X- _ O
not -X- _ O
work -X- _ O
. -X- _ O

( -X- _ O
b -X- _ O
): -X- _ O
Query -X- _ O
stream -X- _ O
attention -X- _ O
, -X- _ O
which -X- _ O
does -X- _ O
not -X- _ O
have -X- _ O
access -X- _ O
information -X- _ O
about -X- _ O
the -X- _ O
content -X- _ O
xzt -X- _ O
. -X- _ O

( -X- _ O
, -X- _ O
) -X- _ O
g$ -X- _ O
( -X- _ O
, -X- _ O
) -X- _ O
h -X- _ O
' -X- _ O
Masked -X- _ O
Two -X- _ O
- -X- _ O
stream -X- _ O
Attention -X- _ O
K -X- _ O
, -X- _ O
V -X- _ O
( -X- _ O
, -X- _ O
) -X- _ O
( -X- _ O
, -X- _ O
) -X- _ O

Masked -X- _ O
Two -X- _ O
- -X- _ O
stream -X- _ O
Attention -X- _ O
( -X- _ O
, -X- _ O
) -X- _ O
g -X- _ O
) -X- _ O
Content -X- _ O
stream -X- _ O
: -X- _ O
can -X- _ O
see -X- _ O
self -X- _ O
( -X- _ O
a -X- _ O
) -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
h$ -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
h$ -X- _ O
g$ -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
g$ -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
h -X- _ O
' -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
g -X- _ O
' -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
h -X- _ O
( -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
h -X- _ O
) -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
g -X- _ O
) -X- _ O
Query -X- _ O
stream -X- _ O
: -X- _ O
can -X- _ O
not -X- _ O
see -X- _ O
self -X- _ O
Attention -X- _ O
Q -X- _ O
( -X- _ O
, -X- _ O
) -X- _ O
h$ -X- _ O

2.3 -X- _ O
Architecture -X- _ O
: -X- _ O
Two -X- _ O
- -X- _ O
Stream -X- _ O
Self -X- _ O
- -X- _ O
Attention -X- _ O
for -X- _ O
Target -X- _ O
- -X- _ O
Aware -X- _ O
Representations -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
h$ -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
x$ -X- _ O

Since -X- _ O
the -X- _ O
same -X- _ O
model -X- _ O
parameter -X- _ O
θ -X- _ O
is -X- _ O
shared -X- _ O
across -X- _ O
all -X- _ O
factorization -X- _ O
orders -X- _ O
during -X- _ O
training -X- _ O
, -X- _ O
in -X- _ O
expectation -X- _ O
, -X- _ O
xt -X- _ O
has -X- _ O
seen -X- _ O
every -X- _ O
possible -X- _ O
element -X- _ O
xi -X- _ O
6= -X- _ O
xt -X- _ O
in -X- _ O
the -X- _ O
sequence -X- _ O
, -X- _ O
hence -X- _ O
being -X- _ O
able -X- _ O
to -X- _ O
capture -X- _ O
the -X- _ O
bidirectional -X- _ O
context -X- _ O
. -X- _ O

Then -X- _ O
, -X- _ O
our -X- _ O
proposed -X- _ O
permutation -X- _ O
language -X- _ O
modeling -X- _ O
objective -X- _ O
can -X- _ O
be -X- _ O
expressed -X- _ O
as -X- _ O
follows -X- _ O
: -X- _ O
" -X- _ O
T -X- _ O
# -X- _ O
X -X- _ O
max -X- _ O

[ -X- _ O
32 -X- _ O
] -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
the -X- _ O
permutation -X- _ O
language -X- _ O
modeling -X- _ O
objective -X- _ O
that -X- _ O
not -X- _ O
only -X- _ O
retains -X- _ O
the -X- _ O
benefits -X- _ O
of -X- _ O
AR -X- _ O
models -X- _ O
but -X- _ O
also -X- _ O
allows -X- _ O
models -X- _ O
to -X- _ O
capture -X- _ O
bidirectional -X- _ O
contexts -X- _ O
. -X- _ O

Borrowing -X- _ O
ideas -X- _ O
from -X- _ O
orderless -X- _ B-MethodName
NADE -X- _ I-MethodName

According -X- _ O
to -X- _ O
the -X- _ O
comparison -X- _ O
above -X- _ O
, -X- _ O
AR -X- _ O
language -X- _ O
modeling -X- _ O
and -X- _ O
BERT -X- _ B-MethodName
possess -X- _ O
their -X- _ O
unique -X- _ O
advantages -X- _ O
over -X- _ O
the -X- _ O
other -X- _ O
. -X- _ O

2.2 -X- _ O
Objective -X- _ O
: -X- _ O
Permutation -X- _ O
Language -X- _ O
Modeling -X- _ O

• -X- _ O
Context -X- _ O
dependency -X- _ O
: -X- _ O
The -X- _ O
AR -X- _ O
representation -X- _ O
hθ -X- _ O
( -X- _ O
x1 -X- _ O
: -X- _ O
t−1 -X- _ O
) -X- _ O
is -X- _ O
only -X- _ O
conditioned -X- _ O
on -X- _ O
the -X- _ O
tokens -X- _ O
up -X- _ O
to -X- _ O
position -X- _ O
t -X- _ O
( -X- _ O
i.e. -X- _ O
tokens -X- _ O
to -X- _ O
the -X- _ O
left -X- _ O
) -X- _ O
, -X- _ O
while -X- _ O
the -X- _ O
BERT -X- _ B-MethodName
representation -X- _ O
Hθ -X- _ O
( -X- _ O
x)t -X- _ O
has -X- _ O
access -X- _ O
to -X- _ O
the -X- _ O
contextual -X- _ O
information -X- _ O
on -X- _ O
both -X- _ O
sides -X- _ O
. -X- _ O

In -X- _ O
comparison -X- _ O
, -X- _ O
AR -X- _ O
language -X- _ O
modeling -X- _ O
does -X- _ O
not -X- _ O
rely -X- _ O
on -X- _ O
any -X- _ O
input -X- _ O
corruption -X- _ O
and -X- _ O
does -X- _ O
not -X- _ O
suffer -X- _ O
from -X- _ O
this -X- _ O
issue -X- _ O
. -X- _ O

In -X- _ O
comparison -X- _ O
, -X- _ O
the -X- _ O
AR -X- _ O
language -X- _ O
modeling -X- _ O
objective -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
factorizes -X- _ O
pθ -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
using -X- _ O
the -X- _ O
product -X- _ O
rule -X- _ O
that -X- _ O
holds -X- _ O
universally -X- _ O
without -X- _ O
such -X- _ O
an -X- _ O
independence -X- _ O
assumption -X- _ O
. -X- _ O

θ -X- _ O
x0 -X- _ O
exp -X- _ O
Hθ -X- _ O
( -X- _ O
x̂)t -X- _ O
e(x -X- _ O
) -X- _ O
t=1 -X- _ O
t=1 -X- _ O
where -X- _ O
mt -X- _ O
= -X- _ O
1 -X- _ O
indicates -X- _ O
xt -X- _ O
is -X- _ O
masked -X- _ O
, -X- _ O
and -X- _ O
Hθ -X- _ O
is -X- _ O
a -X- _ O
Transformer -X- _ O
that -X- _ O
maps -X- _ O
a -X- _ O
length -X- _ O
- -X- _ O
T -X- _ O
text -X- _ O
sequence -X- _ O
x -X- _ O
into -X- _ O
a -X- _ O
sequence -X- _ O
of -X- _ O
hidden -X- _ O
vectors -X- _ O

In -X- _ O
comparison -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
is -X- _ O
based -X- _ O
on -X- _ O
denoising -X- _ O
auto -X- _ O
- -X- _ O
encoding -X- _ O
. -X- _ O

where -X- _ O
hθ -X- _ O
( -X- _ O
x1 -X- _ O
: -X- _ O
t−1 -X- _ O
) -X- _ O
is -X- _ O
a -X- _ O
context -X- _ O
representation -X- _ O
produced -X- _ O
by -X- _ O
neural -X- _ O
models -X- _ O
, -X- _ O
such -X- _ O
as -X- _ O
RNNs -X- _ O
or -X- _ O
Transformers -X- _ O
, -X- _ O
and -X- _ O
e(x -X- _ O
) -X- _ O
denotes -X- _ O
the -X- _ O
embedding -X- _ O
of -X- _ O
x. -X- _ O

Given -X- _ O
a -X- _ O
text -X- _ O
sequence -X- _ O
x -X- _ O
= -X- _ O
[ -X- _ O
x1 -X- _ O
, -X- _ O
· -X- _ O
· -X- _ O
· -X- _ O
, -X- _ O
xT -X- _ O
] -X- _ O
, -X- _ O
AR -X- _ O
language -X- _ O
modeling -X- _ O
performs -X- _ O
pretraining -X- _ O
by -X- _ O
maximizing -X- _ O
the -X- _ O
likelihood -X- _ O
under -X- _ O
the -X- _ O
forward -X- _ O
autoregressive -X- _ O
factorization -X- _ O
: -X- _ O

2 -X- _ O
Proposed -X- _ O
Method -X- _ O
2.1 -X- _ O
Background -X- _ O
In -X- _ O
this -X- _ O
section -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
review -X- _ O
and -X- _ O
compare -X- _ O
the -X- _ O
conventional -X- _ O
AR -X- _ O
language -X- _ O
modeling -X- _ O
and -X- _ O
BERT -X- _ B-MethodName
for -X- _ O
language -X- _ O
pretraining -X- _ O
. -X- _ O

Another -X- _ O
related -X- _ O
idea -X- _ O
is -X- _ O
to -X- _ O
perform -X- _ O
autoregressive -X- _ O
denoising -X- _ O
in -X- _ O
the -X- _ O
context -X- _ O
of -X- _ O
text -X- _ B-TaskName
generation -X- _ I-TaskName

Technically -X- _ O
, -X- _ O
to -X- _ O
construct -X- _ O
a -X- _ O
valid -X- _ O
target -X- _ O
- -X- _ O
aware -X- _ O
prediction -X- _ O
distribution -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
incorporates -X- _ O
the -X- _ O
target -X- _ O
position -X- _ O
into -X- _ O
the -X- _ O
hidden -X- _ O
state -X- _ O
via -X- _ O
two -X- _ O
- -X- _ O
stream -X- _ O
attention -X- _ O
while -X- _ O
previous -X- _ O
permutation -X- _ O
- -X- _ O
based -X- _ O
AR -X- _ O
models -X- _ O
relied -X- _ O
on -X- _ O
implicit -X- _ O
position -X- _ O
awareness -X- _ O
inherent -X- _ O
to -X- _ O
their -X- _ O
MLP -X- _ O
architectures -X- _ O
. -X- _ O

Firstly -X- _ O
, -X- _ O
previous -X- _ O
models -X- _ O
aim -X- _ O
to -X- _ O
improve -X- _ O
density -X- _ O
estimation -X- _ O
by -X- _ O
baking -X- _ O
an -X- _ O
“ -X- _ O
orderless -X- _ O
” -X- _ O
inductive -X- _ O
bias -X- _ O
into -X- _ O
the -X- _ O
model -X- _ O
while -X- _ O
XLNet -X- _ B-MethodName
is -X- _ O
motivated -X- _ O
by -X- _ O
enabling -X- _ O
AR -X- _ O
language -X- _ O
models -X- _ O
to -X- _ O
learn -X- _ O
bidirectional -X- _ O
contexts -X- _ O
. -X- _ O

Empirically -X- _ O
, -X- _ O
under -X- _ O
comparable -X- _ O
experiment -X- _ O
setting -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
consistently -X- _ O
outperforms -X- _ O
BERT -X- _ B-MethodName
[ -X- _ O
10 -X- _ O
] -X- _ O
on -X- _ O
a -X- _ O
wide -X- _ O
spectrum -X- _ O
of -X- _ O
problems -X- _ O
including -X- _ O
GLUE -X- _ B-DatasetName
language -X- _ B-TaskName
understanding -X- _ I-TaskName
tasks -X- _ O
, -X- _ O
reading -X- _ B-TaskName
comprehension -X- _ I-TaskName
tasks -X- _ O
like -X- _ O
SQuAD -X- _ B-DatasetName
and -X- _ O
RACE -X- _ B-DatasetName
, -X- _ O
text -X- _ B-TaskName
classification -X- _ I-TaskName
tasks -X- _ O
such -X- _ O
as -X- _ O
Yelp -X- _ B-DatasetName
and -X- _ O
IMDB -X- _ B-DatasetName
, -X- _ O
and -X- _ O
the -X- _ O
ClueWeb09 -X- _ B-DatasetName
- -X- _ I-DatasetName
B -X- _ I-DatasetName
document -X- _ B-TaskName
ranking -X- _ I-TaskName
task -X- _ O
. -X- _ O

The -X- _ O
idea -X- _ O
of -X- _ O
permutation -X- _ O
- -X- _ O
based -X- _ O
AR -X- _ O
modeling -X- _ O
has -X- _ O
been -X- _ O
explored -X- _ O
in -X- _ O
[ -X- _ O
32 -X- _ O
, -X- _ O
12 -X- _ O
] -X- _ O
, -X- _ O
but -X- _ O
there -X- _ O
are -X- _ O
several -X- _ O
key -X- _ O
differences -X- _ O
. -X- _ O

• -X- _ O
Naively -X- _ O
applying -X- _ O
a -X- _ O
Transformer(-XL -X- _ B-TaskName
) -X- _ I-TaskName
architecture -X- _ O
to -X- _ O
permutation -X- _ O
- -X- _ O
based -X- _ O
language -X- _ O
modeling -X- _ O
does -X- _ O
not -X- _ O
work -X- _ O
because -X- _ O
the -X- _ O
factorization -X- _ O
order -X- _ O
is -X- _ O
arbitrary -X- _ O
and -X- _ O
the -X- _ O
target -X- _ O
is -X- _ O
ambiguous -X- _ O
. -X- _ O

• -X- _ O
Inspired -X- _ O
by -X- _ O
the -X- _ O
latest -X- _ O
advancements -X- _ O
in -X- _ O
AR -X- _ O
language -X- _ O
modeling -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
integrates -X- _ O
the -X- _ O
segment -X- _ O
recurrence -X- _ O
mechanism -X- _ O
and -X- _ O
relative -X- _ O
encoding -X- _ O
scheme -X- _ O
of -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
[ -X- _ O
9 -X- _ O
] -X- _ O
into -X- _ O
pretraining -X- _ O
, -X- _ O
which -X- _ O
empirically -X- _ O
improves -X- _ O
the -X- _ O
performance -X- _ O
especially -X- _ O
for -X- _ O
tasks -X- _ O
involving -X- _ O
a -X- _ O
longer -X- _ O
text -X- _ O
sequence -X- _ O
. -X- _ O

Faced -X- _ O
with -X- _ O
the -X- _ O
pros -X- _ O
and -X- _ O
cons -X- _ O
of -X- _ O
existing -X- _ O
language -X- _ O
pretraining -X- _ O
objectives -X- _ O
, -X- _ O
in -X- _ O
this -X- _ O
work -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
XLNet -X- _ B-MethodName
, -X- _ O
a -X- _ O
generalized -X- _ O
autoregressive -X- _ O
method -X- _ O
that -X- _ O
leverages -X- _ O
the -X- _ O
best -X- _ O
of -X- _ O
both -X- _ O
AR -X- _ O
language -X- _ O
modeling -X- _ O
and -X- _ O
AE -X- _ O
while -X- _ O
avoiding -X- _ O
their -X- _ O
limitations -X- _ O
. -X- _ O

In -X- _ O
other -X- _ O
words -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
assumes -X- _ O
the -X- _ O
predicted -X- _ O
tokens -X- _ O
are -X- _ O
independent -X- _ O
of -X- _ O
each -X- _ O
other -X- _ O
given -X- _ O
the -X- _ O
unmasked -X- _ O
tokens -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
oversimplified -X- _ O
as -X- _ O
high -X- _ O
- -X- _ O
order -X- _ O
, -X- _ O
long -X- _ O
- -X- _ O
range -X- _ O
dependency -X- _ O
is -X- _ O
prevalent -X- _ O
in -X- _ O
natural -X- _ O
language -X- _ O
[ -X- _ O
9 -X- _ O
] -X- _ O
. -X- _ O

Moreover -X- _ O
, -X- _ O
since -X- _ O
the -X- _ O
predicted -X- _ O
tokens -X- _ O
are -X- _ O
masked -X- _ O
in -X- _ O
the -X- _ O
input -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
is -X- _ O
not -X- _ O
able -X- _ O
to -X- _ O
model -X- _ O
the -X- _ O
joint -X- _ O
probability -X- _ O
using -X- _ O
the -X- _ O
product -X- _ O
rule -X- _ O
as -X- _ O
in -X- _ O
AR -X- _ O
language -X- _ O
modeling -X- _ O
. -X- _ O

As -X- _ O
an -X- _ O
immediate -X- _ O
benefit -X- _ O
, -X- _ O
this -X- _ O
closes -X- _ O
the -X- _ O
aforementioned -X- _ O
bidirectional -X- _ O
information -X- _ O
gap -X- _ O
in -X- _ O
AR -X- _ O
language -X- _ O
modeling -X- _ O
, -X- _ O
leading -X- _ O
to -X- _ O
improved -X- _ O
performance -X- _ O
. -X- _ O

bidirectional -X- _ O
contexts -X- _ O
for -X- _ O
reconstruction -X- _ O
. -X- _ O

Since -X- _ O
density -X- _ O
estimation -X- _ O
is -X- _ O
not -X- _ O
part -X- _ O
of -X- _ O
the -X- _ O
objective -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
is -X- _ O
allowed -X- _ O
to -X- _ O
utilize -X- _ O
∗ -X- _ O
1 -X- _ O
Equal -X- _ O
contribution -X- _ O
. -X- _ O

In -X- _ O
comparison -X- _ O
, -X- _ O
AE -X- _ O
based -X- _ O
pretraining -X- _ O
does -X- _ O
not -X- _ O
perform -X- _ O
explicit -X- _ O
density -X- _ O
estimation -X- _ O
but -X- _ O
instead -X- _ O
aims -X- _ O
to -X- _ O
reconstruct -X- _ O
the -X- _ O
original -X- _ O
data -X- _ O
from -X- _ O
corrupted -X- _ O
input -X- _ O
. -X- _ O

This -X- _ O
results -X- _ O
in -X- _ O
a -X- _ O
gap -X- _ O
between -X- _ O
AR -X- _ O
language -X- _ O
modeling -X- _ O
and -X- _ O
effective -X- _ O
pretraining -X- _ O
. -X- _ O

On -X- _ O
the -X- _ O
contrary -X- _ O
, -X- _ O
downstream -X- _ O
language -X- _ B-TaskName
understanding -X- _ I-TaskName
tasks -X- _ O
often -X- _ O
require -X- _ O
bidirectional -X- _ O
context -X- _ O
information -X- _ O
. -X- _ O

Specifically -X- _ O
, -X- _ O
given -X- _ O
a -X- _ O
text -X- _ O
sequence -X- _ O
x -X- _ O
= -X- _ O
( -X- _ O
x1 -X- _ O
, -X- _ O
· -X- _ O
· -X- _ O
· -X- _ O
, -X- _ O
xT -X- _ O
) -X- _ O
, -X- _ O
AR -X- _ O
language -X- _ O
QT -X- _ O
modeling -X- _ O
factorizes -X- _ O
the -X- _ O
likelihood -X- _ O
into -X- _ O
a -X- _ O
forward -X- _ O
product -X- _ O
p(x -X- _ O
) -X- _ O
= -X- _ O
t=1 -X- _ O
p(xt -X- _ O
| -X- _ O
x -X- _ O
< -X- _ O
t -X- _ O
) -X- _ O
or -X- _ O
a -X- _ O
backward -X- _ O
Q1 -X- _ O
one -X- _ O
p(x -X- _ O
) -X- _ O
= -X- _ O

AR -X- _ O
language -X- _ O
modeling -X- _ O
seeks -X- _ O
to -X- _ O
estimate -X- _ O
the -X- _ O
probability -X- _ O
distribution -X- _ O
of -X- _ O
a -X- _ O
text -X- _ O
corpus -X- _ O
with -X- _ O
an -X- _ O
autoregressive -X- _ O
model -X- _ O
[ -X- _ O
7 -X- _ O
, -X- _ O
27 -X- _ O
, -X- _ O
28 -X- _ O
] -X- _ O
. -X- _ O

Among -X- _ O
them -X- _ O
, -X- _ O
autoregressive -X- _ O
( -X- _ O
AR -X- _ O
) -X- _ O
language -X- _ O
modeling -X- _ O
and -X- _ O
autoencoding -X- _ O
( -X- _ O
AE -X- _ O
) -X- _ O
have -X- _ O
been -X- _ O
the -X- _ O
two -X- _ O
most -X- _ O
successful -X- _ O
pretraining -X- _ O
objectives -X- _ O
. -X- _ O

1 -X- _ O
Introduction -X- _ O
Unsupervised -X- _ O
representation -X- _ O
learning -X- _ O
has -X- _ O
been -X- _ O
highly -X- _ O
successful -X- _ O
in -X- _ O
the -X- _ O
domain -X- _ O
of -X- _ O
natural -X- _ B-TaskName
language -X- _ I-TaskName
processing -X- _ I-TaskName
[ -X- _ O
7 -X- _ O
, -X- _ O
22 -X- _ O
, -X- _ O
27 -X- _ O
, -X- _ O
28 -X- _ O
, -X- _ O
10 -X- _ O
] -X- _ O
. -X- _ O

Abstract -X- _ O
With -X- _ O
the -X- _ O
capability -X- _ O
of -X- _ O
modeling -X- _ O
bidirectional -X- _ O
contexts -X- _ O
, -X- _ O
denoising -X- _ O
autoencoding -X- _ O
based -X- _ O
pretraining -X- _ O
like -X- _ O
BERT -X- _ B-MethodName
achieves -X- _ O
better -X- _ O
performance -X- _ O
than -X- _ O
pretraining -X- _ O
approaches -X- _ O
based -X- _ O
on -X- _ O
autoregressive -X- _ O
language -X- _ O
modeling -X- _ O
. -X- _ O

XLNet -X- _ B-MethodName
: -X- _ O
Generalized -X- _ O
Autoregressive -X- _ O
Pretraining -X- _ O
for -X- _ O
Language -X- _ B-TaskName
Understanding -X- _ I-TaskName
arXiv:1906.08237v2 -X- _ O


18 -X- _ O

The -X- _ O
dash -X- _ O
arrows -X- _ O
indicate -X- _ O
that -X- _ O
the -X- _ O
query -X- _ O
stream -X- _ O
can -X- _ O
not -X- _ O
access -X- _ O
the -X- _ O
token -X- _ O
( -X- _ O
content -X- _ O
) -X- _ O
at -X- _ O
the -X- _ O
same -X- _ O
position -X- _ O
, -X- _ O
but -X- _ O
only -X- _ O
the -X- _ O
location -X- _ O
information -X- _ O
. -X- _ O

g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
' -X- _ O
' -X- _ O
h(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
( -X- _ O
( -X- _ O
h#(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
# -X- _ O
mem(+ -X- _ O
) -X- _ O
x% -X- _ O
x -X- _ O
' -X- _ O
x -X- _ O
( -X- _ O
x -X- _ O
# -X- _ O
Position-4 -X- _ O
View -X- _ O
w -X- _ O
w -X- _ O
w -X- _ O
w -X- _ O
Position-1 -X- _ O
View -X- _ O
Split -X- _ O
View -X- _ O
of -X- _ O
the -X- _ O
Query -X- _ O
Stream -X- _ O
( -X- _ O
Factorization -X- _ O
order -X- _ O
: -X- _ O
3 -X- _ O
à -X- _ O
2 -X- _ O
à -X- _ O
4 -X- _ O
à -X- _ O
1 -X- _ O
) -X- _ O
Figure -X- _ O
6 -X- _ O
: -X- _ O
A -X- _ O
detailed -X- _ O
illustration -X- _ O
of -X- _ O
the -X- _ O
query -X- _ O
stream -X- _ O
of -X- _ O
the -X- _ O
proposed -X- _ O
objective -X- _ O
with -X- _ O
both -X- _ O
the -X- _ O
joint -X- _ O
view -X- _ O
and -X- _ O
split -X- _ O
views -X- _ O
based -X- _ O
on -X- _ O
a -X- _ O
length-4 -X- _ B-HyperparameterName
sequence -X- _ O
under -X- _ O
the -X- _ O
factorization -X- _ B-HyperparameterName
order -X- _ I-HyperparameterName
[ -X- _ B-HyperparameterValue
3 -X- _ I-HyperparameterValue
, -X- _ I-HyperparameterValue
2 -X- _ I-HyperparameterValue
, -X- _ I-HyperparameterValue
4 -X- _ I-HyperparameterValue
, -X- _ I-HyperparameterValue
1 -X- _ I-HyperparameterValue
] -X- _ I-HyperparameterValue
. -X- _ O

g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
% -X- _ O
% -X- _ O
h(% -X- _ O
) -X- _ O

( -X- _ O
( -X- _ O
h -X- _ O
# -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
# -X- _ O
mem(% -X- _ O
) -X- _ O
h(% -X- _ O
) -X- _ O

% -X- _ O
% -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
' -X- _ O
' -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O

% -X- _ O
% -X- _ O
w -X- _ O
w -X- _ O
w -X- _ O
w -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O

g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
' -X- _ O
' -X- _ O
h(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
( -X- _ O
( -X- _ O
h#(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
# -X- _ O
mem(+ -X- _ O
) -X- _ O
x% -X- _ O
x -X- _ O
' -X- _ O
x -X- _ O
( -X- _ O
x -X- _ O
# -X- _ O
w -X- _ O
w -X- _ O
w -X- _ O
Position-2 -X- _ O
View -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O

g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
% -X- _ O
% -X- _ O
h(% -X- _ O
) -X- _ O

h(% -X- _ O
) -X- _ O
h(% -X- _ O
) -X- _ O
% -X- _ O
% -X- _ O
h(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
' -X- _ O
' -X- _ O
h(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
( -X- _ O
( -X- _ O
h#(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
# -X- _ O
mem(+ -X- _ O
) -X- _ O
x% -X- _ O
x -X- _ O
' -X- _ O
x -X- _ O
( -X- _ O
x -X- _ O
# -X- _ O
w -X- _ O
Position-3 -X- _ O
View -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
' -X- _ O
' -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
( -X- _ O
( -X- _ O
h -X- _ O
# -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
# -X- _ O
mem(% -X- _ O
) -X- _ O
h(% -X- _ O
) -X- _ O

g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
' -X- _ O
' -X- _ O
h(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
( -X- _ O
( -X- _ O
h#(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
# -X- _ O
mem(+ -X- _ O
) -X- _ O
x% -X- _ O
x -X- _ O
' -X- _ O
x -X- _ O
( -X- _ O
x -X- _ O
# -X- _ O
w -X- _ O
w -X- _ O
w -X- _ O
w -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
% -X- _ O
% -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
' -X- _ O
' -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
( -X- _ O
( -X- _ O
h -X- _ O
# -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
# -X- _ O
mem(% -X- _ O
) -X- _ O

g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
% -X- _ O
% -X- _ O
h(% -X- _ O
) -X- _ O

( -X- _ O
( -X- _ O
h -X- _ O
# -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
# -X- _ O
mem(% -X- _ O
) -X- _ O
h(% -X- _ O
) -X- _ O

% -X- _ O
% -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
' -X- _ O
' -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O

g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
' -X- _ O
' -X- _ O
h(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
( -X- _ O
( -X- _ O
h#(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
# -X- _ O
mem(+ -X- _ O
) -X- _ O
x% -X- _ O
x -X- _ O
' -X- _ O
x -X- _ O
( -X- _ O
x -X- _ O
# -X- _ O
w -X- _ O
w -X- _ O
w -X- _ O
w -X- _ O
Joint -X- _ O
View -X- _ O
of -X- _ O
the -X- _ O
Query -X- _ O
Stream -X- _ O
( -X- _ O
Factorization -X- _ O
order -X- _ O
: -X- _ O
3 -X- _ O
à -X- _ O
2 -X- _ O
à -X- _ O
4 -X- _ O
à -X- _ O
1 -X- _ O
) -X- _ O
Split -X- _ O
View -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O

g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
% -X- _ O
% -X- _ O
h(% -X- _ O
) -X- _ O

h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
% -X- _ O
% -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
' -X- _ O
' -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
( -X- _ O
( -X- _ O
h -X- _ O
# -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
# -X- _ O
mem(% -X- _ O
) -X- _ O
h(% -X- _ O
) -X- _ O

17 -X- _ O

Note -X- _ O
that -X- _ O
if -X- _ O
we -X- _ O
ignore -X- _ O
the -X- _ O
query -X- _ O
representation -X- _ O
, -X- _ O
the -X- _ O
computation -X- _ O
in -X- _ O
this -X- _ O
figure -X- _ O
is -X- _ O
simply -X- _ O
the -X- _ O
standard -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
, -X- _ O
though -X- _ O
with -X- _ O
a -X- _ O
particular -X- _ O
attention -X- _ O
mask -X- _ O
. -X- _ O

x -X- _ O
' -X- _ O
x -X- _ O
( -X- _ O
x -X- _ O
# -X- _ O
w -X- _ O
w -X- _ O
w -X- _ O
w -X- _ O
Position-4 -X- _ O
View -X- _ O
w -X- _ O
w -X- _ O
w -X- _ O
w -X- _ O
Position-1 -X- _ O
View -X- _ O
Split -X- _ O
View -X- _ O
of -X- _ O
the -X- _ O
Content -X- _ O
Stream -X- _ O
( -X- _ O
Factorization -X- _ O
order -X- _ O
: -X- _ O
3 -X- _ O
à -X- _ O
2 -X- _ O
à -X- _ O
4 -X- _ O
à -X- _ O
1 -X- _ O
) -X- _ O
Figure -X- _ O
5 -X- _ O
: -X- _ O
A -X- _ O
detailed -X- _ O
illustration -X- _ O
of -X- _ O
the -X- _ O
content -X- _ O
stream -X- _ O
of -X- _ O
the -X- _ O
proposed -X- _ O
objective -X- _ O
with -X- _ O
both -X- _ O
the -X- _ O
joint -X- _ O
view -X- _ O
and -X- _ O
split -X- _ O
views -X- _ O
based -X- _ O
on -X- _ O
a -X- _ O
length-4 -X- _ B-HyperparameterName
sequence -X- _ O
under -X- _ O
the -X- _ O
factorization -X- _ B-HyperparameterName
order -X- _ I-HyperparameterName
[ -X- _ B-HyperparameterValue
3 -X- _ I-HyperparameterValue
, -X- _ I-HyperparameterValue
2 -X- _ I-HyperparameterValue
, -X- _ I-HyperparameterValue
4 -X- _ I-HyperparameterValue
, -X- _ I-HyperparameterValue
1 -X- _ I-HyperparameterValue
] -X- _ I-HyperparameterValue
. -X- _ O

x% -X- _ O

h(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
% -X- _ O
% -X- _ O
h(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
' -X- _ O
' -X- _ O
h(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
( -X- _ O
( -X- _ O
h#(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
# -X- _ O
mem(+ -X- _ O
) -X- _ O
x% -X- _ O
x -X- _ O
' -X- _ O
x -X- _ O
( -X- _ O
x -X- _ O
# -X- _ O
mem(+ -X- _ O
) -X- _ O

g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
' -X- _ O
' -X- _ O
h(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
( -X- _ O
( -X- _ O
h#(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
# -X- _ O
mem(% -X- _ O
) -X- _ O

h(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
% -X- _ O
% -X- _ O
h(% -X- _ O
) -X- _ O

g -X- _ O
' -X- _ O
h -X- _ O
( -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
mem(% -X- _ O
) -X- _ O

g -X- _ O
' -X- _ O
w -X- _ O
Position-2 -X- _ O
View -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
h -X- _ O
( -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
h -X- _ O
# -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
# -X- _ O
h% -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g% -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
h -X- _ O
' -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O

h(% -X- _ O
) -X- _ O
h(% -X- _ O
) -X- _ O
% -X- _ O
% -X- _ O
h(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
' -X- _ O
' -X- _ O
h(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
( -X- _ O
( -X- _ O
h#(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
# -X- _ O
mem(+ -X- _ O
) -X- _ O
x% -X- _ O
x -X- _ O
' -X- _ O
x -X- _ O
( -X- _ O
w -X- _ O
x -X- _ O
# -X- _ O
w -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
h -X- _ O
# -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
# -X- _ O
w -X- _ O
Position-3 -X- _ O
View -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
h% -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g% -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
h -X- _ O
' -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O

g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
' -X- _ O
' -X- _ O
h(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
( -X- _ O
( -X- _ O
h#(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
# -X- _ O
mem(+ -X- _ O
) -X- _ O
x% -X- _ O
x -X- _ O
' -X- _ O
x -X- _ O
( -X- _ O
x -X- _ O
# -X- _ O
w -X- _ O
w -X- _ O
w -X- _ O
w -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
% -X- _ O
% -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
' -X- _ O
' -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
( -X- _ O
( -X- _ O
h -X- _ O
# -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
# -X- _ O
mem(% -X- _ O
) -X- _ O

g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
% -X- _ O
% -X- _ O
h(% -X- _ O
) -X- _ O

g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
% -X- _ O
% -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
' -X- _ O
' -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
( -X- _ O
( -X- _ O
h -X- _ O
# -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
# -X- _ O
mem(% -X- _ O
) -X- _ O
h(% -X- _ O
) -X- _ O

g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
' -X- _ O
' -X- _ O
h(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
( -X- _ O
( -X- _ O
h#(% -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
# -X- _ O
mem(+ -X- _ O
) -X- _ O
x% -X- _ O
x -X- _ O
' -X- _ O
x -X- _ O
( -X- _ O
x -X- _ O
# -X- _ O
w -X- _ O
w -X- _ O
w -X- _ O
w -X- _ O
Joint -X- _ O
View -X- _ O
of -X- _ O
the -X- _ O
Content -X- _ O
Stream -X- _ O
( -X- _ O
Factorization -X- _ O
order -X- _ O
: -X- _ O
3 -X- _ O
à -X- _ O
2 -X- _ O
à -X- _ O
4 -X- _ O
à -X- _ O
1 -X- _ O
) -X- _ O
Split -X- _ O
View -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O

g -X- _ O
( -X- _ O
% -X- _ O
) -X- _ O
% -X- _ O
% -X- _ O
h(% -X- _ O
) -X- _ O

h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
% -X- _ O
% -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
' -X- _ O
' -X- _ O
h -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
( -X- _ O
( -X- _ O
h -X- _ O
# -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
# -X- _ O
mem(% -X- _ O
) -X- _ O
h(% -X- _ O
) -X- _ O

https://openreview.net/forum?id=HJePno0cYm -X- _ O
16 -X- _ O

The -X- _ O
main -X- _ O
difference -X- _ O
is -X- _ O
that -X- _ O
the -X- _ O
query -X- _ O
stream -X- _ O
can -X- _ O
not -X- _ O
do -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
and -X- _ O
does -X- _ O
not -X- _ O
have -X- _ O
access -X- _ O
to -X- _ O
the -X- _ O
token -X- _ O
at -X- _ O
the -X- _ O
position -X- _ O
, -X- _ O
while -X- _ O
the -X- _ O
content -X- _ O
stream -X- _ O
performs -X- _ O
normal -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
. -X- _ O

Moreover -X- _ O
, -X- _ O
comparing -X- _ O
Figure -X- _ O
5 -X- _ O
and -X- _ O
6 -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
see -X- _ O
how -X- _ O
the -X- _ O
query -X- _ O
stream -X- _ O
and -X- _ O
the -X- _ O
content -X- _ O
stream -X- _ O
work -X- _ O
differently -X- _ O
with -X- _ O
a -X- _ O
specific -X- _ O
permutation -X- _ O
through -X- _ O
attention -X- _ O
masks -X- _ O
. -X- _ O

As -X- _ O
shown -X- _ O
in -X- _ O
Figure -X- _ O
5 -X- _ O
and -X- _ O
6 -X- _ O
, -X- _ O
given -X- _ O
the -X- _ O
current -X- _ O
position -X- _ O
zt -X- _ O
, -X- _ O
the -X- _ O
attention -X- _ O
mask -X- _ O
is -X- _ O
decided -X- _ O
by -X- _ O
the -X- _ O
permutation -X- _ O
( -X- _ O
or -X- _ O
factorization -X- _ O
order -X- _ O
) -X- _ O
z -X- _ O
such -X- _ O
that -X- _ O
only -X- _ O
tokens -X- _ O
the -X- _ O
occur -X- _ O
before -X- _ O
zt -X- _ O
in -X- _ O
the -X- _ O
permutation -X- _ O
can -X- _ O
be -X- _ O
attended -X- _ O
; -X- _ O
i.e. -X- _ O
, -X- _ O
positions -X- _ O
zi -X- _ O
with -X- _ O
i -X- _ O
< -X- _ O
t. -X- _ O

A.7 -X- _ O

but -X- _ O
with -X- _ O
different -X- _ O
factorization -X- _ O
orders -X- _ O
. -X- _ O

x% -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
h$ -X- _ O
h -X- _ O
# -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
h% -X- _ O
x% -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
h -X- _ O
" -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
h$ -X- _ O
h -X- _ O
# -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
h% -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
h -X- _ O
" -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
mem -X- _ O
( -X- _ O
# -X- _ O
) -X- _ O
h -X- _ O
# -X- _ O
( -X- _ O
# -X- _ O
) -X- _ O
h$ -X- _ O
( -X- _ O
# -X- _ O
) -X- _ O
h% -X- _ O
( -X- _ O
# -X- _ O
) -X- _ O
h -X- _ O
" -X- _ O
( -X- _ O
# -X- _ O
) -X- _ O
mem -X- _ O
( -X- _ O
# -X- _ O
) -X- _ O
h -X- _ O
# -X- _ O
( -X- _ O
# -X- _ O
) -X- _ O
h$ -X- _ O
( -X- _ O
# -X- _ O
) -X- _ O
h% -X- _ O
( -X- _ O
# -X- _ O
) -X- _ O
h -X- _ O
" -X- _ O
mem -X- _ O
( -X- _ O
+ -X- _ O
) -X- _ O
x -X- _ O
# -X- _ O
x$ -X- _ O
x% -X- _ O
x -X- _ O
" -X- _ O
mem -X- _ O
( -X- _ O
+ -X- _ O
) -X- _ O
x -X- _ O
# -X- _ O
x$ -X- _ O
x% -X- _ O
x -X- _ O
" -X- _ O
Factorization -X- _ O
order -X- _ O
: -X- _ O
3 -X- _ O
à -X- _ O
2 -X- _ O
à -X- _ O
4 -X- _ O
à -X- _ O
1 -X- _ O
Factorization -X- _ O
order -X- _ O
: -X- _ O
2 -X- _ O
à -X- _ O
4 -X- _ O
à -X- _ O
3 -X- _ O
à -X- _ O
1 -X- _ O
x% -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
h$ -X- _ O
h -X- _ O
# -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
h% -X- _ O
( -X- _ O
# -X- _ O
) -X- _ O
x% -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
h -X- _ O
" -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
h$ -X- _ O
h -X- _ O
# -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
h% -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
h -X- _ O
" -X- _ O
( -X- _ O
$ -X- _ O
) -X- _ O
mem -X- _ O
( -X- _ O
# -X- _ O
) -X- _ O
h -X- _ O
# -X- _ O
( -X- _ O
# -X- _ O
) -X- _ O
h$ -X- _ O
( -X- _ O
# -X- _ O
) -X- _ O
h% -X- _ O
( -X- _ O
# -X- _ O
) -X- _ O
h -X- _ O
" -X- _ O
( -X- _ O
# -X- _ O
) -X- _ O
mem -X- _ O
( -X- _ O
+ -X- _ O
) -X- _ O
h -X- _ O
# -X- _ O
( -X- _ O
# -X- _ O
) -X- _ O
h$ -X- _ O
( -X- _ O
# -X- _ O
) -X- _ O
h% -X- _ O
( -X- _ O
# -X- _ O
) -X- _ O
h -X- _ O
" -X- _ O
mem -X- _ O
( -X- _ O
+ -X- _ O
) -X- _ O
x -X- _ O
# -X- _ O
x$ -X- _ O
x% -X- _ O
x -X- _ O
" -X- _ O
mem -X- _ O
( -X- _ O
+ -X- _ O
) -X- _ O
x -X- _ O
# -X- _ O
x$ -X- _ O
x% -X- _ O
x -X- _ O
" -X- _ O
Factorization -X- _ O
order -X- _ O
: -X- _ O
1 -X- _ O
à -X- _ O
4 -X- _ O
à -X- _ O
2 -X- _ O
à -X- _ O
3 -X- _ O
( -X- _ O
# -X- _ O
) -X- _ O
Factorization -X- _ O
order -X- _ O
: -X- _ O
4 -X- _ O
à -X- _ O
3 -X- _ O
à -X- _ O
1 -X- _ O
à -X- _ O
2 -X- _ O
Figure -X- _ O
4 -X- _ O
: -X- _ O
Illustration -X- _ O
of -X- _ O
the -X- _ O
permutation -X- _ O
language -X- _ O
modeling -X- _ O
objective -X- _ O
for -X- _ O
predicting -X- _ O
x3 -X- _ O
given -X- _ O
the -X- _ O
same -X- _ O
input -X- _ O
sequence -X- _ O
x -X- _ O

15 -X- _ O

Rows -X- _ O
and -X- _ O
columns -X- _ O
represent -X- _ O
query -X- _ O
and -X- _ O
key -X- _ O
respectively -X- _ O
. -X- _ O

( -X- _ O
a -X- _ O
) -X- _ O
Self -X- _ O
exclusion -X- _ O
( -X- _ O
b -X- _ O
) -X- _ O
Relative -X- _ O
stride -X- _ O
( -X- _ O
c -X- _ O
) -X- _ O
One -X- _ O
- -X- _ O
side -X- _ O
masked -X- _ O
Figure -X- _ O
3 -X- _ O
: -X- _ O
Attention -X- _ O
patterns -X- _ O
that -X- _ O
appear -X- _ O
only -X- _ O
in -X- _ O
XLNet -X- _ B-MethodName
. -X- _ O

We -X- _ O
conjecture -X- _ O
these -X- _ O
unique -X- _ O
patterns -X- _ O
contribute -X- _ O
to -X- _ O
the -X- _ O
performance -X- _ O
advantage -X- _ O
of -X- _ O
XLNet -X- _ B-MethodName
. -X- _ O

Note -X- _ O
that -X- _ O
all -X- _ O
these -X- _ O
three -X- _ O
unique -X- _ O
patterns -X- _ O
involve -X- _ O
the -X- _ O
relative -X- _ O
positions -X- _ O
rather -X- _ O
than -X- _ O
absolute -X- _ O
ones -X- _ O
, -X- _ O
and -X- _ O
hence -X- _ O
are -X- _ O
likely -X- _ O
enabled -X- _ O
by -X- _ O
the -X- _ O
“ -X- _ O
relative -X- _ O
attention -X- _ O
” -X- _ O
mechanism -X- _ O
in -X- _ O
XLNet -X- _ B-MethodName
. -X- _ O

It -X- _ O
seems -X- _ O
that -X- _ O
the -X- _ O
model -X- _ O
learns -X- _ O
not -X- _ O
to -X- _ O
attend -X- _ O
the -X- _ O
relative -X- _ O
right -X- _ O
half -X- _ O
. -X- _ O

More -X- _ O
interestingly -X- _ O
, -X- _ O
in -X- _ O
Fig -X- _ O
. -X- _ O
3 -X- _ O
, -X- _ O
we -X- _ O
present -X- _ O
3 -X- _ O
patterns -X- _ O
that -X- _ O
only -X- _ O
appear -X- _ O
in -X- _ O
XLNet -X- _ B-MethodName
but -X- _ O
not -X- _ O
BERT -X- _ B-MethodName
: -X- _ O
( -X- _ O
a -X- _ O
) -X- _ O
The -X- _ O
self -X- _ O
- -X- _ O
exclusion -X- _ O
pattern -X- _ O
attends -X- _ O
to -X- _ O
all -X- _ O
other -X- _ O
tokens -X- _ O
but -X- _ O
itself -X- _ O
, -X- _ O
probably -X- _ O
offering -X- _ O
a -X- _ O
fast -X- _ O
way -X- _ O
to -X- _ O
gather -X- _ O
global -X- _ O
information -X- _ O
; -X- _ O
( -X- _ O
b -X- _ O
) -X- _ O
The -X- _ O
relative -X- _ O
- -X- _ O
stride -X- _ O
pattern -X- _ O
attends -X- _ O
to -X- _ O
positions -X- _ O
every -X- _ O
a -X- _ O
few -X- _ O
stride -X- _ O
apart -X- _ O
relative -X- _ O
to -X- _ O
the -X- _ O
query -X- _ O
position -X- _ O
; -X- _ O
( -X- _ O
c -X- _ O
) -X- _ O
The -X- _ O
one -X- _ O
- -X- _ O
side -X- _ O
masked -X- _ O
pattern -X- _ O
is -X- _ O
very -X- _ O
similar -X- _ O
to -X- _ O
the -X- _ O
lower -X- _ O
- -X- _ O
left -X- _ O
part -X- _ O
of -X- _ O
Fig -X- _ O
. -X- _ O
1-(d -X- _ O
) -X- _ O
, -X- _ O
with -X- _ O
the -X- _ O
upper -X- _ O
- -X- _ O
right -X- _ O
triangle -X- _ O
masked -X- _ O
out -X- _ O
. -X- _ O

Rows -X- _ O
and -X- _ O
columns -X- _ O
represent -X- _ O
query -X- _ O
and -X- _ O
key -X- _ O
respectively -X- _ O
. -X- _ O

( -X- _ O
a -X- _ O
) -X- _ O
Content -X- _ O
stripes -X- _ O
( -X- _ O
b -X- _ O
) -X- _ O
Local -X- _ O
/ -X- _ O
Self -X- _ O
focus -X- _ O
( -X- _ O
c -X- _ O
) -X- _ O
Two -X- _ O
segments -X- _ O
( -X- _ O
d -X- _ O
) -X- _ O
Content -X- _ O
- -X- _ O
based -X- _ O
symmetry -X- _ O
Figure -X- _ O
2 -X- _ O
: -X- _ O
Attention -X- _ O
patterns -X- _ O
shared -X- _ O
by -X- _ O
XLNet -X- _ B-MethodName
and -X- _ O
BERT -X- _ B-MethodName
. -X- _ O

Firstly -X- _ O
, -X- _ O
we -X- _ O
found -X- _ O
4 -X- _ O
typical -X- _ O
patterns -X- _ O
shared -X- _ O
by -X- _ O
both -X- _ O
, -X- _ O
as -X- _ O
shown -X- _ O
in -X- _ O
Fig -X- _ O
. -X- _ O
2 -X- _ O
. -X- _ O

A.6 -X- _ O
Qualitative -X- _ O
Analysis -X- _ O
of -X- _ O
Attention -X- _ O
Patterns -X- _ O
We -X- _ O
compare -X- _ O
the -X- _ O
attention -X- _ O
pattern -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
XLNet -X- _ B-MethodName
without -X- _ O
finetuning -X- _ O
. -X- _ O

As -X- _ O
an -X- _ O
example -X- _ O
, -X- _ O
we -X- _ O
integrate -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
into -X- _ O
XLNet -X- _ B-MethodName
to -X- _ O
demonstrate -X- _ O
the -X- _ O
usefulness -X- _ O
of -X- _ O
the -X- _ O
latest -X- _ O
language -X- _ O
modeling -X- _ O
progress -X- _ O
. -X- _ O

[ -X- _ O
4 -X- _ O
, -X- _ O
32 -X- _ O
, -X- _ O
24 -X- _ O
] -X- _ O
, -X- _ O
language -X- _ O
modeling -X- _ O
has -X- _ O
been -X- _ O
a -X- _ O
rapidly -X- _ O
- -X- _ O
developing -X- _ O
research -X- _ O
area -X- _ O
[ -X- _ O
9 -X- _ O
, -X- _ O
1 -X- _ O
, -X- _ O
3 -X- _ O
] -X- _ O
. -X- _ O

A.5.3 -X- _ O
Bridging -X- _ O
the -X- _ O
Gap -X- _ O
Between -X- _ O
Language -X- _ O
Modeling -X- _ O
and -X- _ O
Pretraining -X- _ O
With -X- _ O
a -X- _ O
deep -X- _ O
root -X- _ O
in -X- _ O
density -X- _ O
estimation4 -X- _ O

Approaches -X- _ O
like -X- _ O
ELMo -X- _ B-MethodName
[ -X- _ O
27 -X- _ O
] -X- _ O
concatenate -X- _ O
forward -X- _ O
and -X- _ O
backward -X- _ O
language -X- _ O
models -X- _ O
in -X- _ O
a -X- _ O
shallow -X- _ O
manner -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
not -X- _ O
sufficient -X- _ O
for -X- _ O
modeling -X- _ O
deep -X- _ O
interactions -X- _ O
between -X- _ O
the -X- _ O
two -X- _ O
directions -X- _ O
. -X- _ O

In -X- _ O
comparison -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
is -X- _ O
able -X- _ O
to -X- _ O
cover -X- _ O
all -X- _ O
dependencies -X- _ O
in -X- _ O
expectation -X- _ O
. -X- _ O

• -X- _ O

14 -X- _ O

More -X- _ O
formally -X- _ O
, -X- _ O
consider -X- _ O
a -X- _ O
context -X- _ O
- -X- _ O
target -X- _ O
pair -X- _ O
( -X- _ O
x -X- _ O
, -X- _ O
U -X- _ O
): -X- _ O
• -X- _ O
If -X- _ O
U -X- _ O
6⊆ -X- _ O
T -X- _ O
< -X- _ O
x -X- _ O
, -X- _ O
where -X- _ O
T -X- _ O
< -X- _ O
x -X- _ O
denotes -X- _ O
the -X- _ O
tokens -X- _ O
prior -X- _ O
to -X- _ O
x -X- _ O
in -X- _ O
the -X- _ O
original -X- _ O
sequence -X- _ O
, -X- _ O
AR -X- _ O
language -X- _ O
modeling -X- _ O
is -X- _ O
not -X- _ O
able -X- _ O
to -X- _ O
cover -X- _ O
the -X- _ O
dependency -X- _ O
. -X- _ O

For -X- _ O
example -X- _ O
, -X- _ O
consider -X- _ O
a -X- _ O
span -X- _ B-TaskName
extraction -X- _ I-TaskName
question -X- _ I-TaskName
answering -X- _ I-TaskName
task -X- _ O
with -X- _ O
the -X- _ O
context -X- _ O
“ -X- _ O
Thom -X- _ O
Yorke -X- _ O
is -X- _ O
the -X- _ O
singer -X- _ O
of -X- _ O
Radiohead -X- _ O
” -X- _ O
and -X- _ O
the -X- _ O
question -X- _ O
“ -X- _ O
Who -X- _ O
is -X- _ O
the -X- _ O
singer -X- _ O
of -X- _ O
Radiohead -X- _ O
” -X- _ O
. -X- _ O

XLNet -X- _ B-MethodName
, -X- _ O
on -X- _ O
the -X- _ O
other -X- _ O
hand -X- _ O
, -X- _ O
is -X- _ O
able -X- _ O
to -X- _ O
cover -X- _ O
both -X- _ O
in -X- _ O
expectation -X- _ O
over -X- _ O
all -X- _ O
factorization -X- _ O
orders -X- _ O
. -X- _ O

A.5.2 -X- _ O
Comparison -X- _ O
with -X- _ O
Language -X- _ O
Modeling -X- _ O
Borrowing -X- _ O
examples -X- _ O
and -X- _ O
notations -X- _ O
from -X- _ O
Section -X- _ O
A.5.1 -X- _ O
, -X- _ O
a -X- _ O
standard -X- _ O
AR -X- _ O
language -X- _ O
model -X- _ O
like -X- _ O
GPT -X- _ B-MethodName
[ -X- _ O
28 -X- _ O
] -X- _ O
is -X- _ O
only -X- _ O
able -X- _ O
to -X- _ O
cover -X- _ O
the -X- _ O
dependency -X- _ O
( -X- _ O
x -X- _ O
= -X- _ O
York -X- _ O
, -X- _ O
U -X- _ O
= -X- _ O
{ -X- _ O
New -X- _ O
} -X- _ O
) -X- _ O
but -X- _ O
not -X- _ O
( -X- _ O
x -X- _ O
= -X- _ O
New -X- _ O
, -X- _ O
U -X- _ O
= -X- _ O
{ -X- _ O
York -X- _ O
} -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
other -X- _ O
words -X- _ O
, -X- _ O
the -X- _ O
XLNet -X- _ B-MethodName
objective -X- _ O
contains -X- _ O
more -X- _ O
effective -X- _ O
training -X- _ O
signals -X- _ O
, -X- _ O
which -X- _ O
empirically -X- _ O
leads -X- _ O
to -X- _ O
better -X- _ O
performance -X- _ O
in -X- _ O
Section -X- _ O
3 -X- _ O
. -X- _ O

As -X- _ O
a -X- _ O
result -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
is -X- _ O
able -X- _ O
to -X- _ O
cover -X- _ O
more -X- _ O
dependencies -X- _ O
than -X- _ O
BERT -X- _ B-MethodName
. -X- _ O

• -X- _ O
If -X- _ O
U -X- _ O
⊆ -X- _ O
N -X- _ O
∪ -X- _ O
T -X- _ O
< -X- _ O
x -X- _ O
and -X- _ O
U -X- _ O
∩ -X- _ O
T -X- _ O
< -X- _ O
x -X- _ O
6= -X- _ O
∅ -X- _ O
, -X- _ O
the -X- _ O
dependency -X- _ O
can -X- _ O
only -X- _ O
be -X- _ O
covered -X- _ O
by -X- _ O
XLNet -X- _ B-MethodName
but -X- _ O
not -X- _ O
BERT -X- _ B-MethodName
. -X- _ O

Given -X- _ O
the -X- _ O
definition -X- _ O
, -X- _ O
let -X- _ O
’s -X- _ O
consider -X- _ O
two -X- _ O
cases -X- _ O
: -X- _ O
• -X- _ O
If -X- _ O
U -X- _ O
⊆ -X- _ O
N -X- _ O
, -X- _ O
the -X- _ O
dependency -X- _ O
( -X- _ O
x -X- _ O
, -X- _ O
U -X- _ O
) -X- _ O
is -X- _ O
covered -X- _ O
by -X- _ O
both -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
XLNet -X- _ B-MethodName
. -X- _ O

I -X- _ O
is -X- _ O
covered -X- _ O
by -X- _ O
a -X- _ O
model -X- _ O
( -X- _ O
objective -X- _ O
) -X- _ O
if -X- _ O
U -X- _ O
⊆ -X- _ O
Vx -X- _ O
. -X- _ O

For -X- _ O
convenience -X- _ O
, -X- _ O
we -X- _ O
say -X- _ O
a -X- _ O
target -X- _ O
- -X- _ O
context -X- _ O
pair -X- _ O
( -X- _ O
x -X- _ O
, -X- _ O
U -X- _ O
) -X- _ O
∈ -X- _ O

I -X- _ O
such -X- _ O
that -X- _ O
U -X- _ O
⊆ -X- _ O
Vx -X- _ O
, -X- _ O
then -X- _ O
the -X- _ O
loss -X- _ O
term -X- _ O
log -X- _ O
p(x -X- _ O
| -X- _ O
Vx -X- _ O
) -X- _ O
provides -X- _ O
a -X- _ O
training -X- _ O
signal -X- _ O
to -X- _ O
the -X- _ O
dependency -X- _ O
between -X- _ O
x -X- _ O
and -X- _ O
U. -X- _ O

Intuitively -X- _ O
, -X- _ O
if -X- _ O
there -X- _ O
exists -X- _ O
a -X- _ O
target -X- _ O
- -X- _ O
context -X- _ O
pair -X- _ O
( -X- _ O
x -X- _ O
, -X- _ O
U -X- _ O
) -X- _ O
∈ -X- _ O

Both -X- _ O
objectives -X- _ O
consist -X- _ O
of -X- _ O
multiple -X- _ O
loss -X- _ O
terms -X- _ O
in -X- _ O
the -X- _ O
form -X- _ O
of -X- _ O
log -X- _ O
p(x -X- _ O
| -X- _ O
Vx -X- _ O
) -X- _ O
. -X- _ O

x∈T -X- _ O
x∈T -X- _ O
where -X- _ O
T -X- _ O
< -X- _ O
x -X- _ O
denote -X- _ O
tokens -X- _ O
in -X- _ O
T -X- _ O
that -X- _ O
have -X- _ O
a -X- _ O
factorization -X- _ O
order -X- _ O
prior -X- _ O
to -X- _ O
x. -X- _ O

Given -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
target -X- _ O
tokens -X- _ O
T -X- _ O
and -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
non -X- _ O
- -X- _ O
target -X- _ O
tokens -X- _ O
N -X- _ O
= -X- _ O
x\T -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
XLNet -X- _ B-MethodName
both -X- _ O
maximize -X- _ O
log -X- _ O
p(T -X- _ O
| -X- _ O
N -X- _ O
) -X- _ O
but -X- _ O
with -X- _ O
different -X- _ O
formulations -X- _ O
: -X- _ O
X -X- _ O
X -X- _ O
JBERT -X- _ O
= -X- _ O
log -X- _ O
p(x -X- _ O
| -X- _ O
N -X- _ O
) -X- _ O
; -X- _ O
JXLNet -X- _ O
= -X- _ O
log -X- _ O
p(x -X- _ O
| -X- _ O
N -X- _ O
∪ -X- _ O
T -X- _ O
< -X- _ O
x -X- _ O
) -X- _ O

Note -X- _ O
that -X- _ O
I -X- _ O
is -X- _ O
merely -X- _ O
a -X- _ O
virtual -X- _ O
notion -X- _ O
without -X- _ O
unique -X- _ O
ground -X- _ O
truth -X- _ O
, -X- _ O
and -X- _ O
our -X- _ O
analysis -X- _ O
will -X- _ O
hold -X- _ O
regardless -X- _ O
of -X- _ O
how -X- _ O
I -X- _ O
is -X- _ O
instantiated -X- _ O
. -X- _ O

I -X- _ O
= -X- _ O
x -X- _ O
= -X- _ O
York -X- _ O
, -X- _ O
U -X- _ O
= -X- _ O
{ -X- _ O
New -X- _ O
} -X- _ O
, -X- _ O
x -X- _ O
= -X- _ O
York -X- _ O
, -X- _ O
U -X- _ O
= -X- _ O
{ -X- _ O
city -X- _ O
} -X- _ O
, -X- _ O
x -X- _ O
= -X- _ O
York -X- _ O
, -X- _ O
U -X- _ O
= -X- _ O
{ -X- _ O
New -X- _ O
, -X- _ O
city -X- _ O
} -X- _ O
, -X- _ O
· -X- _ O
· -X- _ O
· -X- _ O
. -X- _ O

 -X- _ O
 -X- _ O

For -X- _ O
example -X- _ O
, -X- _ O
given -X- _ O
the -X- _ O
above -X- _ O
sentence -X- _ O
, -X- _ O
the -X- _ O
pairs -X- _ O
of -X- _ O
interest -X- _ O
I -X- _ O
could -X- _ O
be -X- _ O
instantiated -X- _ O
as -X- _ O
: -X- _ O
n -X- _ O
o -X- _ O
 -X- _ O

Inspired -X- _ O
by -X- _ O
previous -X- _ O
work -X- _ O
[ -X- _ O
37 -X- _ O
] -X- _ O
, -X- _ O
given -X- _ O
a -X- _ O
sequence -X- _ O
x -X- _ O
= -X- _ O
[ -X- _ O
x1 -X- _ O
, -X- _ O
· -X- _ O
· -X- _ O
· -X- _ O
, -X- _ O
xT -X- _ O
] -X- _ O
, -X- _ O
we -X- _ O
define -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
target -X- _ O
- -X- _ O
context -X- _ O
pairs -X- _ O
of -X- _ O
interest -X- _ O
, -X- _ O
I -X- _ O
= -X- _ O
{ -X- _ O
( -X- _ O
x -X- _ O
, -X- _ O
U -X- _ O
) -X- _ O
} -X- _ O
, -X- _ O
where -X- _ O
U -X- _ O
is -X- _ O
a -X- _ O
set -X- _ O
of -X- _ O
tokens -X- _ O
in -X- _ O
x -X- _ O
that -X- _ O
form -X- _ O
a -X- _ O
context -X- _ O
of -X- _ O
x. -X- _ O
Intuitively -X- _ O
, -X- _ O
we -X- _ O
want -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
learn -X- _ O
the -X- _ O
dependency -X- _ O
of -X- _ O
x -X- _ O
on -X- _ O
U -X- _ O
through -X- _ O
a -X- _ O
pretraining -X- _ O
loss -X- _ O
term -X- _ O
log -X- _ O
p(x -X- _ O
| -X- _ O
U -X- _ O
) -X- _ O
. -X- _ O

K -X- _ O
1e-6 -X- _ O
1.0 -X- _ O
Discussion -X- _ O
and -X- _ O
Analysis -X- _ O
Comparison -X- _ O
with -X- _ O
BERT -X- _ B-MethodName
To -X- _ O
prove -X- _ O
a -X- _ O
general -X- _ O
point -X- _ O
beyond -X- _ O
one -X- _ O
example -X- _ O
, -X- _ O
we -X- _ O
now -X- _ O
turn -X- _ O
to -X- _ O
more -X- _ O
formal -X- _ O
expressions -X- _ O
. -X- _ O

13 -X- _ O

A.4.2 -X- _ O
Hyperparameters -X- _ O
for -X- _ O
Finetuning -X- _ O
The -X- _ O
hyperparameters -X- _ O
used -X- _ O
for -X- _ O
finetuning -X- _ O
XLNet -X- _ B-MethodName
on -X- _ O
various -X- _ O
tasks -X- _ O
are -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
8 -X- _ O
. -X- _ O

The -X- _ O
hyperparameters -X- _ O
used -X- _ O
for -X- _ O
pretraining -X- _ O
XLNet -X- _ B-MethodName
are -X- _ O
shown -X- _ O
in -X- _ O
Table -X- _ O
7 -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
a -X- _ O
pretrained -X- _ O
XLNet -X- _ B-MethodName
to -X- _ O
extract -X- _ O
word -X- _ O
embeddings -X- _ O
for -X- _ O
the -X- _ O
documents -X- _ O
and -X- _ O
queries -X- _ O
without -X- _ O
finetuning -X- _ O
, -X- _ O
and -X- _ O
employ -X- _ O
a -X- _ O
kernel -X- _ O
pooling -X- _ O
network -X- _ O
[ -X- _ O
36 -X- _ O
] -X- _ O
to -X- _ O
rank -X- _ O
the -X- _ O
documents -X- _ O
. -X- _ O

Since -X- _ O
document -X- _ B-TaskName
ranking -X- _ I-TaskName
, -X- _ O
or -X- _ O
ad -X- _ B-TaskName
- -X- _ I-TaskName
hoc -X- _ I-TaskName
retrieval -X- _ I-TaskName
, -X- _ O
mainly -X- _ O
concerns -X- _ O
the -X- _ O
low -X- _ O
- -X- _ O
level -X- _ O
representations -X- _ O
instead -X- _ O
of -X- _ O
high -X- _ O
- -X- _ O
level -X- _ O
semantics -X- _ O
, -X- _ O
this -X- _ O
dataset -X- _ O
serves -X- _ O
as -X- _ O
a -X- _ O
testbed -X- _ O
for -X- _ O
evaluating -X- _ O
the -X- _ O
quality -X- _ O
of -X- _ O
word -X- _ O
embeddings -X- _ O
. -X- _ O

The -X- _ O
queries -X- _ O
were -X- _ O
created -X- _ O
by -X- _ O
the -X- _ O
TREC -X- _ O
2009 -X- _ O
- -X- _ O
2012 -X- _ O
Web -X- _ O
Tracks -X- _ O
based -X- _ O
on -X- _ O
50 -X- _ O
M -X- _ O
documents -X- _ O
and -X- _ O
the -X- _ O
task -X- _ O
is -X- _ O
to -X- _ O
rerank -X- _ O
the -X- _ O
top -X- _ O
100 -X- _ O
documents -X- _ O
retrieved -X- _ O
using -X- _ O
a -X- _ O
standard -X- _ O
retrieval -X- _ O
method -X- _ O
. -X- _ O

Following -X- _ O
the -X- _ O
setting -X- _ O
in -X- _ O
previous -X- _ O
work -X- _ O
[ -X- _ O
8 -X- _ O
] -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
ClueWeb09 -X- _ B-DatasetName
- -X- _ I-DatasetName
B -X- _ I-DatasetName
dataset -X- _ O
to -X- _ O
evaluate -X- _ O
the -X- _ O
performance -X- _ O
on -X- _ O
document -X- _ B-TaskName
ranking -X- _ I-TaskName
. -X- _ O

A.3.5 -X- _ O
ClueWeb09 -X- _ B-DatasetName
- -X- _ I-DatasetName
B -X- _ I-DatasetName
Dataset -X- _ O

For -X- _ O
WNLI -X- _ B-DatasetName
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
loss -X- _ O
described -X- _ O
in -X- _ O
[ -X- _ O
16 -X- _ O
] -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
for -X- _ O
fair -X- _ O
comparison -X- _ O
with -X- _ O
BERT -X- _ B-MethodName
, -X- _ O
our -X- _ O
result -X- _ O
on -X- _ O
the -X- _ O
QNLI -X- _ B-DatasetName
dev -X- _ O
set -X- _ O
is -X- _ O
based -X- _ O
on -X- _ O
a -X- _ O
standard -X- _ O
classification -X- _ O
paradigm -X- _ O
. -X- _ O

For -X- _ O
QNLI -X- _ B-DatasetName
, -X- _ O
we -X- _ O
employed -X- _ O
a -X- _ O
pairwise -X- _ B-TaskName
relevance -X- _ I-TaskName
ranking -X- _ I-TaskName
scheme -X- _ O
as -X- _ O
in -X- _ O
[ -X- _ O
20 -X- _ O
] -X- _ O
for -X- _ O
our -X- _ O
test -X- _ O
set -X- _ O
submission -X- _ O
. -X- _ O

Only -X- _ O
single -X- _ O
- -X- _ O
task -X- _ O
training -X- _ O
is -X- _ O
employed -X- _ O
for -X- _ O
the -X- _ O
four -X- _ O
large -X- _ O
datasets -X- _ O
. -X- _ O

In -X- _ O
the -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
setting -X- _ O
, -X- _ O
we -X- _ O
jointly -X- _ O
train -X- _ O
an -X- _ O
XLNet -X- _ B-MethodName
on -X- _ O
the -X- _ O
four -X- _ O
largest -X- _ O
datasets -X- _ O
— -X- _ O
MNLI -X- _ B-DatasetName
, -X- _ O
SST-2 -X- _ B-DatasetName
, -X- _ O
QNLI -X- _ B-DatasetName
, -X- _ O
and -X- _ O
QQP -X- _ B-DatasetName
— -X- _ O
and -X- _ O
finetune -X- _ O
the -X- _ O
network -X- _ O
on -X- _ O
the -X- _ O
other -X- _ O
datasets -X- _ O
. -X- _ O

In -X- _ O
Table -X- _ O
5 -X- _ O
, -X- _ O
we -X- _ O
present -X- _ O
results -X- _ O
of -X- _ O
multiple -X- _ O
settings -X- _ O
, -X- _ O
including -X- _ O
single -X- _ O
- -X- _ O
task -X- _ O
and -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
, -X- _ O
as -X- _ O
well -X- _ O
as -X- _ O
single -X- _ O
models -X- _ O
and -X- _ O
ensembles -X- _ O
. -X- _ O

The -X- _ O
test -X- _ O
set -X- _ O
labels -X- _ O
are -X- _ O
removed -X- _ O
from -X- _ O
the -X- _ O
publicly -X- _ O
released -X- _ O
version -X- _ O
, -X- _ O
and -X- _ O
all -X- _ O
the -X- _ O
practitioners -X- _ O
must -X- _ O
submit -X- _ O
their -X- _ O
predictions -X- _ O
on -X- _ O
the -X- _ O
evaluation -X- _ O
server -X- _ O
to -X- _ O
obtain -X- _ O
test -X- _ O
set -X- _ O
results -X- _ O
. -X- _ O

The -X- _ O
GLUE -X- _ B-DatasetName
dataset -X- _ O
[ -X- _ O
34 -X- _ O
] -X- _ O
is -X- _ O
a -X- _ O
collection -X- _ O
of -X- _ O
9 -X- _ O
natural -X- _ B-TaskName
language -X- _ I-TaskName
understanding -X- _ I-TaskName
tasks -X- _ O
. -X- _ O

A.3.4 -X- _ O
GLUE -X- _ B-DatasetName
Dataset -X- _ O

A.3.3 -X- _ O
Text -X- _ B-TaskName
classification -X- _ I-TaskName
Datasets -X- _ O
Following -X- _ O
previous -X- _ O
work -X- _ O
on -X- _ O
text -X- _ B-TaskName
classification -X- _ I-TaskName
[ -X- _ O
39 -X- _ O
, -X- _ O
23 -X- _ O
] -X- _ O
, -X- _ O
we -X- _ O
evaluate -X- _ O
XLNet -X- _ B-MethodName
on -X- _ O
the -X- _ O
following -X- _ O
benchmarks -X- _ O
: -X- _ O
IMDB -X- _ B-DatasetName
, -X- _ O
Yelp-2 -X- _ B-DatasetName
, -X- _ O
Yelp-5 -X- _ B-DatasetName
, -X- _ O
DBpedia -X- _ B-DatasetName
, -X- _ O
AG -X- _ B-DatasetName
, -X- _ O
Amazon-2 -X- _ B-DatasetName
, -X- _ O
and -X- _ O
Amazon-5 -X- _ B-DatasetName
. -X- _ O

12 -X- _ O

To -X- _ O
finetune -X- _ O
an -X- _ O
XLNet -X- _ B-MethodName
on -X- _ O
SQuAD2.0 -X- _ B-DatasetName
, -X- _ O
we -X- _ O
jointly -X- _ O
apply -X- _ O
a -X- _ O
logistic -X- _ O
regression -X- _ O
loss -X- _ O
for -X- _ O
answerability -X- _ B-TaskName
prediction -X- _ I-TaskName
similar -X- _ O
to -X- _ O
classification -X- _ B-TaskName
tasks -X- _ O
and -X- _ O
a -X- _ O
standard -X- _ O
span -X- _ O
extraction -X- _ O
loss -X- _ O
for -X- _ O
question -X- _ B-TaskName
answering -X- _ I-TaskName
[ -X- _ O
10 -X- _ O
] -X- _ O
. -X- _ O

SQuAD1.1 -X- _ B-DatasetName
[ -X- _ O
30 -X- _ O
] -X- _ O
contains -X- _ O
questions -X- _ O
that -X- _ O
always -X- _ O
have -X- _ O
a -X- _ O
corresponding -X- _ O
answer -X- _ O
in -X- _ O
the -X- _ O
given -X- _ O
passages -X- _ O
, -X- _ O
while -X- _ O
SQuAD2.0 -X- _ B-DatasetName
[ -X- _ O
29 -X- _ O
] -X- _ O
introduces -X- _ O
unanswerable -X- _ O
questions -X- _ O
. -X- _ O

A.3.2 -X- _ O
SQuAD -X- _ B-DatasetName
SQuAD -X- _ B-DatasetName
is -X- _ O
a -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
reading -X- _ B-TaskName
comprehension -X- _ I-TaskName
dataset -X- _ O
with -X- _ O
two -X- _ O
tasks -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
a -X- _ O
sequence -X- _ B-HyperparameterName
length -X- _ I-HyperparameterName
of -X- _ O
512 -X- _ B-HyperparameterValue
during -X- _ O
finetuning -X- _ O
. -X- _ O

As -X- _ O
a -X- _ O
result -X- _ O
, -X- _ O
this -X- _ O
dataset -X- _ O
serves -X- _ O
as -X- _ O
a -X- _ O
challenging -X- _ O
benchmark -X- _ O
for -X- _ O
long -X- _ B-TaskName
text -X- _ I-TaskName
understanding -X- _ I-TaskName
. -X- _ O

Moreover -X- _ O
, -X- _ O
the -X- _ O
average -X- _ O
length -X- _ O
of -X- _ O
the -X- _ O
passages -X- _ O
in -X- _ O
RACE -X- _ B-DatasetName
are -X- _ O
longer -X- _ O
than -X- _ O
300 -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
significantly -X- _ O
longer -X- _ O
than -X- _ O
other -X- _ O
popular -X- _ O
reading -X- _ O
comprehension -X- _ O
datasets -X- _ O
such -X- _ O
as -X- _ O
SQuAD -X- _ B-DatasetName
[ -X- _ O
29 -X- _ O
] -X- _ O
. -X- _ O

This -X- _ O
is -X- _ O
one -X- _ O
of -X- _ O
the -X- _ O
most -X- _ O
difficult -X- _ O
reading -X- _ B-TaskName
comprehension -X- _ I-TaskName
datasets -X- _ O
that -X- _ O
involve -X- _ O
challenging -X- _ O
reasoning -X- _ O
questions -X- _ O
. -X- _ O

The -X- _ O
RACE -X- _ B-DatasetName
dataset -X- _ O
[ -X- _ O
18 -X- _ O
] -X- _ O
contains -X- _ O
near -X- _ O
100 -X- _ O
K -X- _ O
questions -X- _ O
taken -X- _ O
from -X- _ O
the -X- _ O
English -X- _ O
exams -X- _ O
for -X- _ O
middle -X- _ O
and -X- _ O
high -X- _ O
school -X- _ O
Chinese -X- _ O
students -X- _ O
in -X- _ O
the -X- _ O
age -X- _ O
range -X- _ O
between -X- _ O
12 -X- _ O
to -X- _ O
18 -X- _ O
, -X- _ O
with -X- _ O
the -X- _ O
answers -X- _ O
generated -X- _ O
by -X- _ O
human -X- _ O
experts -X- _ O
. -X- _ O

P -X- _ O
0 -X- _ O
> -X- _ O
( -X- _ O
M -X- _ O
) -X- _ O
x0 -X- _ O
exp -X- _ O
e(x -X- _ O
) -X- _ O
gzt -X- _ O
A.3 -X- _ O
A.3.1 -X- _ O
Datasets -X- _ O
RACE -X- _ B-DatasetName
Dataset -X- _ O

| -X- _ O
xz -X- _ O
< -X- _ O
t -X- _ O
) -X- _ O
= -X- _ O

t -X- _ O
t -X- _ O
t -X- _ O
Target -X- _ O
- -X- _ O
aware -X- _ O
prediction -X- _ O
distribution -X- _ O
: -X- _ O
 -X- _ O
 -X- _ O
( -X- _ O
M -X- _ O
) -X- _ O
exp -X- _ O
e(x -X- _ O
) -X- _ O
> -X- _ O
gzt -X- _ O
 -X- _ O
 -X- _ O
, -X- _ O
pθ -X- _ O
( -X- _ O
Xzt -X- _ O
= -X- _ O
x -X- _ O

zt -X- _ O
zt -X- _ O
z -X- _ O
< -X- _ O
t -X- _ O
t -X- _ O
 -X- _ O
 -X- _ O
 -X- _ O
gz(m -X- _ O
) -X- _ O
= -X- _ O
LayerNorm -X- _ O
ĝz(m -X- _ O
) -X- _ O
+ -X- _ O
PosFF -X- _ O
ĝz(m -X- _ O
) -X- _ O

LayerNorm -X- _ O
g -X- _ O
+ -X- _ O
RelAttn -X- _ O
g -X- _ O
, -X- _ O
h̃ -X- _ O
, -X- _ O
h -X- _ O

LayerNorm -X- _ O
h -X- _ O
+ -X- _ O
RelAttn -X- _ O
h -X- _ O
, -X- _ O
h̃ -X- _ O
, -X- _ O
h -X- _ O
zt -X- _ O
zt -X- _ O
zt -X- _ O
z≤t -X- _ O
 -X- _ O
 -X- _ O
 -X- _ O
( -X- _ O
m -X- _ O
) -X- _ O
( -X- _ O
m -X- _ O
) -X- _ O
h(m -X- _ O
) -X- _ O
zt -X- _ O
= -X- _ O
LayerNorm -X- _ O
ĥzt -X- _ O
+ -X- _ O
PosFF -X- _ O
ĥzt -X- _ O
 -X- _ O
 -X- _ O
h -X- _ O
i -X- _ O
( -X- _ O
m−1 -X- _ O
) -X- _ O
( -X- _ O
m−1 -X- _ O
) -X- _ O
( -X- _ O
m−1 -X- _ O
) -X- _ O
( -X- _ O
m−1 -X- _ O
) -X- _ O
ĝz(m -X- _ O
) -X- _ O
= -X- _ O

, -X- _ O
T -X- _ O
: -X- _ O
ĥ(m -X- _ O
) -X- _ O
= -X- _ O

For -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
layer -X- _ O
m -X- _ O
= -X- _ O
1 -X- _ O
, -X- _ O
· -X- _ O
· -X- _ O
· -X- _ O
, -X- _ O
M -X- _ O
, -X- _ O
attention -X- _ O
with -X- _ O
relative -X- _ O
positional -X- _ O
encoding -X- _ O
and -X- _ O
position -X- _ O
- -X- _ O
wise -X- _ O
feed -X- _ O
- -X- _ O
forward -X- _ O
are -X- _ O
consecutively -X- _ O
employed -X- _ O
to -X- _ O
update -X- _ O
the -X- _ O
represetntations -X- _ O
: -X- _ O
 -X- _ O
 -X- _ O
h -X- _ O
i -X- _ O
( -X- _ O
m−1 -X- _ O
) -X- _ O
( -X- _ O
m−1 -X- _ O
) -X- _ O
( -X- _ O
m−1 -X- _ O
) -X- _ O
( -X- _ O
m−1 -X- _ O
) -X- _ O
∀t -X- _ O
= -X- _ O
1 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O

, -X- _ O
T -X- _ O
: -X- _ O
ht -X- _ O
= -X- _ O
e(xt -X- _ O
) -X- _ O
and -X- _ O
gt -X- _ O
= -X- _ O
w -X- _ O
Cached -X- _ O
layer -X- _ O
- -X- _ O
m -X- _ O
content -X- _ O
represetation -X- _ O
( -X- _ O
memory -X- _ O
) -X- _ O
from -X- _ O
previous -X- _ O
segment -X- _ O
: -X- _ O
h̃(m -X- _ O
) -X- _ O

Initial -X- _ O
represetation -X- _ O
: -X- _ O
∀t -X- _ O
= -X- _ O
1 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O

References -X- _ O

ZD -X- _ O
and -X- _ O
YY -X- _ O
were -X- _ O
supported -X- _ O
in -X- _ O
part -X- _ O
by -X- _ O
NSF -X- _ O
under -X- _ O
the -X- _ O
grant -X- _ O
IIS-1546329 -X- _ O
and -X- _ O
by -X- _ O
the -X- _ O
DOE -X- _ O
- -X- _ O
Office -X- _ O
of -X- _ O
Science -X- _ O
under -X- _ O
the -X- _ O
grant -X- _ O
ASCR -X- _ O
# -X- _ O
KJ040201 -X- _ O
. -X- _ O

ZY -X- _ O
and -X- _ O
RS -X- _ O
were -X- _ O
supported -X- _ O
by -X- _ O
the -X- _ O
Office -X- _ O
of -X- _ O
Naval -X- _ O
Research -X- _ O
grant -X- _ O
N000141812861 -X- _ O
, -X- _ O
the -X- _ O
National -X- _ O
Science -X- _ O
Foundation -X- _ O
( -X- _ O
NSF -X- _ O
) -X- _ O
grant -X- _ O
IIS1763562 -X- _ O
, -X- _ O
the -X- _ O
Nvidia -X- _ O
fellowship -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
Siebel -X- _ O
scholarship -X- _ O
. -X- _ O

Acknowledgments -X- _ O
The -X- _ O
authors -X- _ O
would -X- _ O
like -X- _ O
to -X- _ O
thank -X- _ O
Qizhe -X- _ O
Xie -X- _ O
and -X- _ O
Adams -X- _ O
Wei -X- _ O
Yu -X- _ O
for -X- _ O
providing -X- _ O
useful -X- _ O
feedback -X- _ O
on -X- _ O
the -X- _ O
project -X- _ O
, -X- _ O
Jamie -X- _ O
Callan -X- _ O
for -X- _ O
providing -X- _ O
the -X- _ O
ClueWeb -X- _ O
dataset -X- _ O
, -X- _ O
Youlong -X- _ O
Cheng -X- _ O
, -X- _ O
Yanping -X- _ O
Huang -X- _ O
and -X- _ O
Shibo -X- _ O
Wang -X- _ O
for -X- _ O
providing -X- _ O
ideas -X- _ O
to -X- _ O
improve -X- _ O
our -X- _ O
TPU -X- _ O
implementation -X- _ O
, -X- _ O
Chenyan -X- _ O
Xiong -X- _ O
and -X- _ O
Zhuyun -X- _ O
Dai -X- _ O
for -X- _ O
clarifying -X- _ O
the -X- _ O
setting -X- _ O
of -X- _ O
the -X- _ O
document -X- _ B-TaskName
ranking -X- _ I-TaskName
task -X- _ O
. -X- _ O

XLNet -X- _ B-MethodName
achieves -X- _ O
substantial -X- _ O
improvement -X- _ O
over -X- _ O
previous -X- _ O
pretraining -X- _ O
objectives -X- _ O
on -X- _ O
various -X- _ O
tasks -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
we -X- _ O
also -X- _ O
perform -X- _ O
a -X- _ O
qualitative -X- _ O
study -X- _ O
of -X- _ O
the -X- _ O
attention -X- _ O
patterns -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
included -X- _ O
in -X- _ O
Appendix -X- _ O
A.6 -X- _ O
due -X- _ O
to -X- _ O
page -X- _ O
limit -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
, -X- _ O
rows -X- _ O
6 -X- _ O
- -X- _ O
7 -X- _ O
show -X- _ O
that -X- _ O
both -X- _ O
span -X- _ O
- -X- _ O
based -X- _ O
prediction -X- _ O
and -X- _ O
the -X- _ O
bidirectional -X- _ O
input -X- _ O
pipeline -X- _ O
play -X- _ O
important -X- _ O
roles -X- _ O
in -X- _ O
XLNet -X- _ B-MethodName
. -X- _ O

Moreover -X- _ O
, -X- _ O
if -X- _ O
we -X- _ O
remove -X- _ O
the -X- _ O
memory -X- _ O
caching -X- _ O
mechanism -X- _ O
( -X- _ O
row -X- _ O
5 -X- _ O
) -X- _ O
, -X- _ O
the -X- _ O
performance -X- _ O
clearly -X- _ O
drops -X- _ O
, -X- _ O
especially -X- _ O
for -X- _ O
RACE -X- _ B-DatasetName
which -X- _ O
involves -X- _ O
the -X- _ O
longest -X- _ O
context -X- _ O
among -X- _ O
the -X- _ O
4 -X- _ O
tasks -X- _ O
. -X- _ O

K -X- _ B-HyperparameterName
is -X- _ O
a -X- _ O
hyperparameter -X- _ O
to -X- _ O
control -X- _ O
the -X- _ O
optimization -X- _ O
difficulty -X- _ O
( -X- _ O
see -X- _ O
Section -X- _ O
2.3 -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
run -X- _ O
BERT -X- _ B-MethodName
on -X- _ O
the -X- _ O
other -X- _ O
datasets -X- _ O
using -X- _ O
the -X- _ O
official -X- _ O
implementation -X- _ O
and -X- _ O
the -X- _ O
same -X- _ O
hyperparameter -X- _ O
search -X- _ O
space -X- _ O
as -X- _ O
XLNet -X- _ B-MethodName
. -X- _ O

For -X- _ O
fair -X- _ O
comparison -X- _ O
, -X- _ O
all -X- _ O
models -X- _ O
are -X- _ O
based -X- _ O
on -X- _ O
a -X- _ O
12 -X- _ B-HyperparameterValue
- -X- _ O
layer -X- _ B-HyperparameterName
architecture -X- _ O
with -X- _ O
the -X- _ O
same -X- _ O
model -X- _ O
hyper -X- _ O
- -X- _ O
parameters -X- _ O
as -X- _ O
BERT -X- _ B-MethodName
- -X- _ I-MethodName
Base -X- _ I-MethodName
and -X- _ O
are -X- _ O
trained -X- _ O
on -X- _ O
only -X- _ O
Wikipedia -X- _ B-DatasetName
and -X- _ O
the -X- _ O
BooksCorpus -X- _ B-DatasetName
. -X- _ O

8 -X- _ O

• -X- _ O
The -X- _ O
importance -X- _ O
of -X- _ O
using -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
as -X- _ O
the -X- _ O
backbone -X- _ O
neural -X- _ O
architecture -X- _ O
. -X- _ O

3.4 -X- _ O
Ablation -X- _ O
Study -X- _ O
We -X- _ O
perform -X- _ O
an -X- _ O
ablation -X- _ O
study -X- _ O
to -X- _ O
understand -X- _ O
the -X- _ O
importance -X- _ O
of -X- _ O
each -X- _ O
design -X- _ O
choice -X- _ O
based -X- _ O
on -X- _ O
four -X- _ O
datasets -X- _ O
with -X- _ O
diverse -X- _ O
characteristics -X- _ O
. -X- _ O

• -X- _ O
For -X- _ O
classification -X- _ O
tasks -X- _ O
that -X- _ O
already -X- _ O
have -X- _ O
abundant -X- _ O
supervised -X- _ O
examples -X- _ O
such -X- _ O
as -X- _ O
MNLI -X- _ B-DatasetName
( -X- _ O
> -X- _ O
390 -X- _ O
K -X- _ O
) -X- _ O
, -X- _ O
Yelp -X- _ B-DatasetName
( -X- _ O
> -X- _ O
560 -X- _ O
K -X- _ O
) -X- _ O
and -X- _ O
Amazon -X- _ B-DatasetName
( -X- _ O
> -X- _ O
3 -X- _ O
M -X- _ O
) -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
still -X- _ O
lead -X- _ O
to -X- _ O
substantial -X- _ O
gains -X- _ O
. -X- _ O

This -X- _ O
superiority -X- _ O
at -X- _ O
dealing -X- _ O
with -X- _ O
longer -X- _ O
context -X- _ O
could -X- _ O
come -X- _ O
from -X- _ O
the -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
backbone -X- _ O
in -X- _ O
XLNet -X- _ B-MethodName
. -X- _ O

• -X- _ O
For -X- _ O
explicit -X- _ O
reasoning -X- _ O
tasks -X- _ O
like -X- _ O
SQuAD -X- _ B-DatasetName
and -X- _ O
RACE -X- _ B-DatasetName
that -X- _ O
involve -X- _ O
longer -X- _ O
context -X- _ O
, -X- _ O
the -X- _ O
performance -X- _ O
gain -X- _ O
of -X- _ O
XLNet -X- _ B-MethodName
is -X- _ O
usually -X- _ O
larger -X- _ O
. -X- _ O

The -X- _ O
upper -X- _ O
section -X- _ O
shows -X- _ O
direct -X- _ O
comparison -X- _ O
on -X- _ O
dev -X- _ O
data -X- _ O
and -X- _ O
the -X- _ O
lower -X- _ O
section -X- _ O
shows -X- _ O
comparison -X- _ O
with -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
results -X- _ O
on -X- _ O
the -X- _ O
public -X- _ O
leaderboard -X- _ O
. -X- _ O

All -X- _ O
dev -X- _ O
results -X- _ O
are -X- _ O
the -X- _ O
median -X- _ O
of -X- _ O
10 -X- _ O
runs -X- _ O
. -X- _ O

∗ -X- _ O
indicates -X- _ O
using -X- _ O
ensembles -X- _ O
, -X- _ O
and -X- _ O
† -X- _ O
denotes -X- _ O
single -X- _ O
- -X- _ O
task -X- _ O
results -X- _ O
in -X- _ O
a -X- _ O
multi -X- _ O
- -X- _ O
task -X- _ O
row -X- _ O
. -X- _ O

‡ -X- _ O
: -X- _ O
We -X- _ O
are -X- _ O
not -X- _ O
able -X- _ O
to -X- _ O
obtain -X- _ O
the -X- _ O
test -X- _ O
results -X- _ O
of -X- _ O
our -X- _ O
latest -X- _ O
model -X- _ O
on -X- _ O
SQuAD1.1 -X- _ B-DatasetName
from -X- _ O
the -X- _ O
organizers -X- _ O
after -X- _ O
submitting -X- _ O
our -X- _ O
result -X- _ O
for -X- _ O
more -X- _ O
than -X- _ O
one -X- _ O
month -X- _ O
, -X- _ O
and -X- _ O
thus -X- _ O
report -X- _ O
the -X- _ O
results -X- _ O
of -X- _ O
an -X- _ O
older -X- _ O
version -X- _ O
for -X- _ O
the -X- _ O
SQuAD1.1 -X- _ B-DatasetName
test -X- _ O
set -X- _ O
. -X- _ O

∗ -X- _ O
indicates -X- _ O
ensembles -X- _ O
. -X- _ O

† -X- _ O
marks -X- _ O
our -X- _ O
runs -X- _ O
with -X- _ O
the -X- _ O
official -X- _ O
code -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
, -X- _ O
we -X- _ O
make -X- _ O
two -X- _ O
more -X- _ O
interesting -X- _ O
observations -X- _ O
: -X- _ O
3 -X- _ O
Hyperparameters -X- _ O
for -X- _ O
pretraining -X- _ O
and -X- _ O
finetuning -X- _ O
are -X- _ O
in -X- _ O
Appendix -X- _ O
A.4 -X- _ O
. -X- _ O
7 -X- _ O

The -X- _ O
results -X- _ O
are -X- _ O
presented -X- _ O
in -X- _ O
Tables -X- _ O
2 -X- _ O
( -X- _ O
reading -X- _ B-TaskName
comprehension -X- _ I-TaskName
& -X- _ O
document -X- _ B-TaskName
ranking -X- _ I-TaskName
) -X- _ O
, -X- _ O
3 -X- _ O
( -X- _ O
question -X- _ B-TaskName
answering -X- _ I-TaskName
) -X- _ O
, -X- _ O
4 -X- _ O
( -X- _ O
text -X- _ B-TaskName
classification -X- _ I-TaskName
) -X- _ O
and -X- _ O
5 -X- _ O
( -X- _ O
natural -X- _ B-TaskName
language -X- _ I-TaskName
understanding -X- _ I-TaskName
) -X- _ O
, -X- _ O
where -X- _ O
XLNet -X- _ B-MethodName
generally -X- _ O
outperforms -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
RoBERTa -X- _ B-MethodName
. -X- _ O

To -X- _ O
obtain -X- _ O
relatively -X- _ O
fair -X- _ O
comparison -X- _ O
with -X- _ O
RoBERTa -X- _ B-MethodName
, -X- _ O
the -X- _ O
experiment -X- _ O
in -X- _ O
this -X- _ O
section -X- _ O
is -X- _ O
based -X- _ O
on -X- _ O
full -X- _ O
data -X- _ O
and -X- _ O
reuses -X- _ O
the -X- _ O
hyper -X- _ O
- -X- _ O
parameters -X- _ O
of -X- _ O
RoBERTa -X- _ B-MethodName
, -X- _ O
as -X- _ O
described -X- _ O
in -X- _ O
section -X- _ O
3.1 -X- _ O
. -X- _ O

After -X- _ O
the -X- _ O
initial -X- _ O
publication -X- _ O
of -X- _ O
our -X- _ O
manuscript -X- _ O
, -X- _ O
a -X- _ O
few -X- _ O
other -X- _ O
pretrained -X- _ O
models -X- _ O
were -X- _ O
released -X- _ O
such -X- _ O
as -X- _ O
RoBERTa -X- _ B-MethodName
[ -X- _ O
21 -X- _ O
] -X- _ O
and -X- _ O
ALBERT -X- _ B-MethodName
[ -X- _ O
19 -X- _ O
] -X- _ O
. -X- _ O

“ -X- _ O
Middle -X- _ O
” -X- _ O
and -X- _ O
“ -X- _ O
High -X- _ O
” -X- _ O
in -X- _ O
RACE -X- _ O
are -X- _ O
two -X- _ O
subsets -X- _ O
representing -X- _ O
middle -X- _ O
and -X- _ O
high -X- _ O
school -X- _ O
difficulty -X- _ O
levels -X- _ O
. -X- _ O

† -X- _ O
indicates -X- _ O
our -X- _ O
implementations -X- _ O
. -X- _ O

∗ -X- _ O
indicates -X- _ O
using -X- _ O
ensembles -X- _ O
. -X- _ O

As -X- _ O
we -X- _ O
can -X- _ O
see -X- _ O
, -X- _ O
trained -X- _ O
on -X- _ O
the -X- _ O
same -X- _ O
data -X- _ O
with -X- _ O
an -X- _ O
almost -X- _ O
identical -X- _ O
training -X- _ O
recipe -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
outperforms -X- _ O
BERT -X- _ B-MethodName
by -X- _ O
a -X- _ O
sizable -X- _ O
margin -X- _ O
on -X- _ O
all -X- _ O
the -X- _ O
considered -X- _ O
datasets -X- _ O
. -X- _ O

In -X- _ O
Table -X- _ O
1 -X- _ O
, -X- _ O
we -X- _ O
compare -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
best -X- _ O
performance -X- _ O
of -X- _ O
three -X- _ O
different -X- _ O
variants -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
XLNet -X- _ B-MethodName
trained -X- _ O
with -X- _ O
the -X- _ O
same -X- _ O
data -X- _ O
and -X- _ O
hyperparameters -X- _ O
. -X- _ O

Here -X- _ O
, -X- _ O
we -X- _ O
first -X- _ O
compare -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
XLNet -X- _ B-MethodName
in -X- _ O
a -X- _ O
fair -X- _ O
setting -X- _ O
to -X- _ O
decouple -X- _ O
the -X- _ O
effects -X- _ O
of -X- _ O
using -X- _ O
more -X- _ O
data -X- _ O
and -X- _ O
the -X- _ O
improvement -X- _ O
from -X- _ O
BERT -X- _ B-MethodName
to -X- _ O
XLNet -X- _ B-MethodName
. -X- _ O

All -X- _ O
models -X- _ O
are -X- _ O
trained -X- _ O
using -X- _ O
the -X- _ O
same -X- _ O
data -X- _ O
and -X- _ O
hyperparameters -X- _ O
as -X- _ O
in -X- _ O
BERT -X- _ B-MethodName
. -X- _ O

Detailed -X- _ O
descriptions -X- _ O
of -X- _ O
the -X- _ O
settings -X- _ O
for -X- _ O
all -X- _ O
the -X- _ O
datasets -X- _ O
can -X- _ O
be -X- _ O
found -X- _ O
in -X- _ O
Appendix -X- _ O
A.3 -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
a -X- _ O
variety -X- _ O
of -X- _ O
natural -X- _ B-TaskName
language -X- _ I-TaskName
understanding -X- _ I-TaskName
datasets -X- _ O
to -X- _ O
evaluate -X- _ O
the -X- _ O
performance -X- _ O
of -X- _ O
our -X- _ O
method -X- _ O
. -X- _ O

[ -X- _ O
1 -X- _ O
, -X- _ O
· -X- _ O
· -X- _ O
· -X- _ O
, -X- _ O
5 -X- _ O
] -X- _ O
, -X- _ O
and -X- _ O
then -X- _ O
randomly -X- _ O
select -X- _ O
a -X- _ O
consecutive -X- _ O
span -X- _ O
of -X- _ O
L -X- _ O
tokens -X- _ O
as -X- _ O
prediction -X- _ O
targets -X- _ O
within -X- _ O
a -X- _ O
context -X- _ O
of -X- _ O
( -X- _ O
KL -X- _ O
) -X- _ O
tokens -X- _ O
. -X- _ O

Our -X- _ O
finetuning -X- _ O
procedure -X- _ O
follows -X- _ O
BERT -X- _ B-MethodName
[ -X- _ O
10 -X- _ O
] -X- _ O
except -X- _ O
otherwise -X- _ O
specified3 -X- _ O
. -X- _ O

For -X- _ O
training -X- _ O
XLNet -X- _ B-MethodName
- -X- _ I-MethodName
Large -X- _ I-MethodName
, -X- _ O
we -X- _ O
set -X- _ O
the -X- _ O
partial -X- _ B-HyperparameterName
prediction -X- _ I-HyperparameterName
constant -X- _ I-HyperparameterName
K -X- _ B-HyperparameterName
as -X- _ O
6 -X- _ B-HyperparameterValue
( -X- _ O
see -X- _ O
Section -X- _ O
2.3 -X- _ O
) -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
we -X- _ O
perform -X- _ O
ablation -X- _ O
study -X- _ O
( -X- _ O
section -X- _ O
3.4 -X- _ O
) -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
XLNet -X- _ B-MethodName
- -X- _ I-MethodName
Base -X- _ I-MethodName
- -X- _ I-MethodName
wikibooks -X- _ I-MethodName
. -X- _ O

observed -X- _ O
that -X- _ O
the -X- _ O
model -X- _ O
still -X- _ O
underfits -X- _ O
the -X- _ O
data -X- _ O
at -X- _ O
the -X- _ O
end -X- _ O
of -X- _ O
training -X- _ O
. -X- _ O

It -X- _ O
was -X- _ O
6 -X- _ O

Then -X- _ O
, -X- _ O
we -X- _ O
scale -X- _ O
up -X- _ O
the -X- _ O
training -X- _ O
of -X- _ O
XLNet -X- _ B-MethodName
- -X- _ I-MethodName
Large -X- _ I-MethodName
by -X- _ O
using -X- _ O
all -X- _ O
the -X- _ O
datasets -X- _ O
described -X- _ O
above -X- _ O
. -X- _ O

Our -X- _ O
largest -X- _ O
model -X- _ O
XLNet -X- _ B-MethodName
- -X- _ O
Large -X- _ O
has -X- _ O
the -X- _ O
same -X- _ O
architecture -X- _ O
hyperparameters -X- _ O
as -X- _ O
BERT -X- _ B-MethodName
- -X- _ I-MethodName
Large -X- _ I-MethodName
, -X- _ O
which -X- _ O
results -X- _ O
in -X- _ O
a -X- _ O
similar -X- _ O
model -X- _ O
size -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
heuristics -X- _ O
to -X- _ O
aggressively -X- _ O
filter -X- _ O
out -X- _ O
short -X- _ O
or -X- _ O
low -X- _ O
- -X- _ O
quality -X- _ O
articles -X- _ O
for -X- _ O
ClueWeb -X- _ B-DatasetName
2012 -X- _ I-DatasetName
- -X- _ I-DatasetName
B -X- _ I-DatasetName
and -X- _ O
Common -X- _ B-DatasetName
Crawl -X- _ I-DatasetName
, -X- _ O
which -X- _ O
results -X- _ O
in -X- _ O
19 -X- _ O
GB -X- _ O
and -X- _ O
110 -X- _ O
GB -X- _ O
text -X- _ O
respectively -X- _ O
. -X- _ O

[ -X- _ O
26 -X- _ O
] -X- _ O
, -X- _ O
ClueWeb -X- _ B-DatasetName
2012 -X- _ I-DatasetName
- -X- _ I-DatasetName
B -X- _ I-DatasetName
( -X- _ O
extended -X- _ O
from -X- _ O
[ -X- _ O
5 -X- _ O
] -X- _ O
) -X- _ O
, -X- _ O
and -X- _ O
Common -X- _ B-DatasetName
Crawl -X- _ I-DatasetName
[ -X- _ O
6 -X- _ O
] -X- _ O
for -X- _ O
pretraining -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
, -X- _ O
we -X- _ O
include -X- _ O
Giga5 -X- _ B-DatasetName
( -X- _ O
16 -X- _ O
GB -X- _ O
text -X- _ O
) -X- _ O

3 -X- _ O
Experiments -X- _ O
3.1 -X- _ O
Pretraining -X- _ O
and -X- _ O
Implementation -X- _ O
Following -X- _ O
BERT -X- _ B-MethodName
[ -X- _ O
10 -X- _ O
] -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
the -X- _ O
BooksCorpus -X- _ B-DatasetName
[ -X- _ O
40 -X- _ O
] -X- _ O
and -X- _ O
English -X- _ O
Wikipedia -X- _ B-DatasetName
as -X- _ O
part -X- _ O
of -X- _ O
our -X- _ O
pretraining -X- _ O
data -X- _ O
, -X- _ O
which -X- _ O
have -X- _ O
13 -X- _ O
GB -X- _ O
plain -X- _ O
text -X- _ O
combined -X- _ O
. -X- _ O

For -X- _ O
more -X- _ O
formal -X- _ O
analysis -X- _ O
and -X- _ O
further -X- _ O
discussion -X- _ O
, -X- _ O
please -X- _ O
refer -X- _ O
to -X- _ O
Appendix -X- _ O
A.5 -X- _ O
. -X- _ O

Although -X- _ O
in -X- _ O
this -X- _ O
example -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
learns -X- _ O
some -X- _ O
dependency -X- _ O
pairs -X- _ O
such -X- _ O
as -X- _ O
( -X- _ O
New -X- _ O
, -X- _ O
city -X- _ O
) -X- _ O
and -X- _ O
( -X- _ O
York -X- _ O
, -X- _ O
city -X- _ O
) -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
obvious -X- _ O
that -X- _ O
XLNet -X- _ B-MethodName
always -X- _ O
learns -X- _ O
more -X- _ O
dependency -X- _ O
pairs -X- _ O
given -X- _ O
the -X- _ O
same -X- _ O
target -X- _ O
and -X- _ O
contains -X- _ O
“ -X- _ O
denser -X- _ O
” -X- _ O
effective -X- _ O
training -X- _ O
signals -X- _ O
. -X- _ O

Notice -X- _ O
that -X- _ O
XLNet -X- _ B-MethodName
is -X- _ O
able -X- _ O
to -X- _ O
capture -X- _ O
the -X- _ O
dependency -X- _ O
between -X- _ O
the -X- _ O
pair -X- _ O
( -X- _ O
New -X- _ O
, -X- _ O
York -X- _ O
) -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
omitted -X- _ O
by -X- _ O
BERT -X- _ B-MethodName
. -X- _ O

| -X- _ O
New -X- _ O
, -X- _ O
is -X- _ O
a -X- _ O
city -X- _ O
) -X- _ O
. -X- _ O

In -X- _ O
this -X- _ O
case -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
XLNet -X- _ B-MethodName
respectively -X- _ O
reduce -X- _ O
to -X- _ O
the -X- _ O
following -X- _ O
objectives -X- _ O
: -X- _ O
JBERT -X- _ O
= -X- _ O
log -X- _ O
p(New -X- _ O
| -X- _ O
is -X- _ O
a -X- _ O
city -X- _ O
) -X- _ O
+ -X- _ O
log -X- _ O
p(York -X- _ O
| -X- _ O
is -X- _ O
a -X- _ O
city -X- _ O
) -X- _ O
, -X- _ O
JXLNet -X- _ O
= -X- _ O
log -X- _ O
p(New -X- _ O
| -X- _ O
is -X- _ O
a -X- _ O
city -X- _ O
) -X- _ O
+ -X- _ O
log -X- _ O
p(York -X- _ O

Also -X- _ O
suppose -X- _ O
that -X- _ O
XLNet -X- _ B-MethodName
samples -X- _ O
the -X- _ O
factorization -X- _ O
order -X- _ O
[ -X- _ O
is -X- _ O
, -X- _ O
a -X- _ O
, -X- _ O
city -X- _ O
, -X- _ O
New -X- _ O
, -X- _ O
York -X- _ O
] -X- _ O
. -X- _ O

Suppose -X- _ O
both -X- _ O
BERT -X- _ B-MethodName
and -X- _ O
XLNet -X- _ B-MethodName
select -X- _ O
the -X- _ O
two -X- _ O
tokens -X- _ O
[ -X- _ O
New -X- _ O
, -X- _ O
York -X- _ O
] -X- _ O
as -X- _ O
the -X- _ O
prediction -X- _ O
targets -X- _ O
and -X- _ O
maximize -X- _ O
log -X- _ O
p(New -X- _ O
York -X- _ O
| -X- _ O
is -X- _ O
a -X- _ O
city -X- _ O
) -X- _ O
. -X- _ O

To -X- _ O
better -X- _ O
understand -X- _ O
the -X- _ O
difference -X- _ O
, -X- _ O
let -X- _ O
’s -X- _ O
consider -X- _ O
a -X- _ O
concrete -X- _ O
example -X- _ O
[ -X- _ O
New -X- _ O
, -X- _ O
York -X- _ O
, -X- _ O
is -X- _ O
, -X- _ O
a -X- _ O
, -X- _ O
city -X- _ O
] -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
the -X- _ O
independence -X- _ O
assumption -X- _ O
discussed -X- _ O
in -X- _ O
Section -X- _ O
2.1 -X- _ O
disables -X- _ O
BERT -X- _ B-MethodName
to -X- _ O
model -X- _ O
dependency -X- _ O
between -X- _ O
targets -X- _ O
. -X- _ O

This -X- _ O
is -X- _ O
a -X- _ O
necessary -X- _ O
choice -X- _ O
for -X- _ O
BERT -X- _ B-MethodName
because -X- _ O
if -X- _ O
all -X- _ O
tokens -X- _ O
are -X- _ O
masked -X- _ O
, -X- _ O
it -X- _ O
is -X- _ O
impossible -X- _ O
to -X- _ O
make -X- _ O
any -X- _ O
meaningful -X- _ O
predictions -X- _ O
. -X- _ O

Second -X- _ O
, -X- _ O
it -X- _ O
opens -X- _ O
the -X- _ O
possibility -X- _ O
of -X- _ O
finetuning -X- _ O
on -X- _ O
tasks -X- _ O
that -X- _ O
have -X- _ O
more -X- _ O
than -X- _ O
two -X- _ O
input -X- _ O
segments -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
not -X- _ O
possible -X- _ O
using -X- _ O
absolute -X- _ O
segment -X- _ O
encodings -X- _ O
. -X- _ O

First -X- _ O
, -X- _ O
the -X- _ O
inductive -X- _ O
bias -X- _ O
of -X- _ O
relative -X- _ O
encodings -X- _ O
improves -X- _ O
generalization -X- _ O
[ -X- _ O
9 -X- _ O
] -X- _ O
. -X- _ O

There -X- _ O
are -X- _ O
two -X- _ O
benefits -X- _ O
of -X- _ O
using -X- _ O
relative -X- _ O
segment -X- _ O
encodings -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
the -X- _ O
value -X- _ O
aij -X- _ O
is -X- _ O
added -X- _ O
to -X- _ O
the -X- _ O
normal -X- _ O
attention -X- _ O
weight -X- _ O
. -X- _ O

When -X- _ O
i -X- _ O
attends -X- _ O
to -X- _ O
j -X- _ O
, -X- _ O
the -X- _ O
segment -X- _ O
encoding -X- _ O
sij -X- _ O
is -X- _ O
used -X- _ O
to -X- _ O
compute -X- _ O
an -X- _ O
attention -X- _ O
weight -X- _ O
aij -X- _ O
= -X- _ O
( -X- _ O
qi -X- _ O
+ -X- _ O
b -X- _ O
) -X- _ O
> -X- _ O
sij -X- _ O
, -X- _ O
where -X- _ O
qi -X- _ O
is -X- _ O
the -X- _ O
query -X- _ O
vector -X- _ O
as -X- _ O
in -X- _ O
a -X- _ O
standard -X- _ O
attention -X- _ O
operation -X- _ O
and -X- _ O
b -X- _ O
is -X- _ O
a -X- _ O
learnable -X- _ O
head -X- _ O
- -X- _ O
specific -X- _ O
bias -X- _ O
vector -X- _ O
. -X- _ O

This -X- _ O
is -X- _ O
consistent -X- _ O
with -X- _ O
the -X- _ O
core -X- _ O
idea -X- _ O
of -X- _ O
relative -X- _ O
encodings -X- _ O
; -X- _ O
i.e. -X- _ O
, -X- _ O
only -X- _ O
modeling -X- _ O
the -X- _ O
relationships -X- _ O
between -X- _ O
positions -X- _ O
. -X- _ O

In -X- _ O
other -X- _ O
words -X- _ O
, -X- _ O
we -X- _ O
only -X- _ O
consider -X- _ O
whether -X- _ O
the -X- _ O
two -X- _ O
positions -X- _ O
are -X- _ O
within -X- _ O
the -X- _ O
same -X- _ O
segment -X- _ O
, -X- _ O
as -X- _ O
opposed -X- _ O
to -X- _ O
considering -X- _ O
which -X- _ O
specific -X- _ O
segments -X- _ O
they -X- _ O
are -X- _ O
from -X- _ O
. -X- _ O

Given -X- _ O
a -X- _ O
pair -X- _ O
of -X- _ O
positions -X- _ O
i -X- _ O
and -X- _ O
j -X- _ O
in -X- _ O
the -X- _ O
sequence -X- _ O
, -X- _ O
if -X- _ O
i -X- _ O
and -X- _ O
j -X- _ O
are -X- _ O
from -X- _ O
the -X- _ O
same -X- _ O
segment -X- _ O
, -X- _ O
we -X- _ O
use -X- _ O
a -X- _ O
segment -X- _ O
encoding -X- _ O
sij -X- _ O
= -X- _ O
s+ -X- _ O
or -X- _ O
otherwise -X- _ O
sij -X- _ O
= -X- _ O
s− -X- _ O
, -X- _ O
where -X- _ O
s+ -X- _ O
and -X- _ O
s− -X- _ O
are -X- _ O
learnable -X- _ O
model -X- _ O
parameters -X- _ O
for -X- _ O
each -X- _ O
attention -X- _ O
head -X- _ O
. -X- _ O

Relative -X- _ O
Segment -X- _ O
Encodings -X- _ O
Architecturally -X- _ O
, -X- _ O
different -X- _ O
from -X- _ O
BERT -X- _ B-MethodName
that -X- _ O
adds -X- _ O
an -X- _ O
absolute -X- _ O
segment -X- _ O
embedding -X- _ O
to -X- _ O
the -X- _ O
word -X- _ O
embedding -X- _ O
at -X- _ O
each -X- _ O
position -X- _ O
, -X- _ O
we -X- _ O
extend -X- _ O
the -X- _ O
idea -X- _ O
of -X- _ O
relative -X- _ O
encodings -X- _ O
from -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
to -X- _ O
also -X- _ O
encode -X- _ O
the -X- _ O
segments -X- _ O
. -X- _ O

Although -X- _ O
5 -X- _ O

[ -X- _ O
CLS -X- _ O
, -X- _ O
A -X- _ O
, -X- _ O
SEP -X- _ O
, -X- _ O
B -X- _ O
, -X- _ O
SEP -X- _ O
] -X- _ O
, -X- _ O
where -X- _ O
“ -X- _ O
SEP -X- _ O
” -X- _ O
and -X- _ O
“ -X- _ O
CLS -X- _ O
” -X- _ O
are -X- _ O
two -X- _ O
special -X- _ O
symbols -X- _ O
and -X- _ O
“ -X- _ O
A -X- _ O
” -X- _ O
and -X- _ O
“ -X- _ O
B -X- _ O
” -X- _ O
are -X- _ O
the -X- _ O
two -X- _ O
segments -X- _ O
. -X- _ O

Specifically -X- _ O
, -X- _ O
the -X- _ O
input -X- _ O
to -X- _ O
our -X- _ O
model -X- _ O
is -X- _ O
the -X- _ O
same -X- _ O
as -X- _ O
BERT -X- _ B-MethodName
: -X- _ O

We -X- _ O
only -X- _ O
reuse -X- _ O
the -X- _ O
memory -X- _ O
that -X- _ O
belongs -X- _ O
to -X- _ O
the -X- _ O
same -X- _ O
context -X- _ O
. -X- _ O

The -X- _ O
query -X- _ O
stream -X- _ O
can -X- _ O
be -X- _ O
computed -X- _ O
in -X- _ O
the -X- _ O
same -X- _ O
way -X- _ O
. -X- _ O

In -X- _ O
expectation -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
learns -X- _ O
to -X- _ O
utilize -X- _ O
the -X- _ O
memory -X- _ O
over -X- _ O
all -X- _ O
factorization -X- _ O
orders -X- _ O
of -X- _ O
the -X- _ O
last -X- _ O
segment -X- _ O
. -X- _ O

This -X- _ O
allows -X- _ O
caching -X- _ O
and -X- _ O
reusing -X- _ O
the -X- _ O
memory -X- _ O
without -X- _ O
knowing -X- _ O
the -X- _ O
factorization -X- _ O
order -X- _ O
of -X- _ O
the -X- _ O
previous -X- _ O
segment -X- _ O
. -X- _ O

Thus -X- _ O
, -X- _ O
the -X- _ O
above -X- _ O
attention -X- _ O
update -X- _ O
is -X- _ O
independent -X- _ O
of -X- _ O
z̃ -X- _ O
once -X- _ O
the -X- _ O
representations -X- _ O
h̃(m -X- _ O
) -X- _ O
are -X- _ O
obtained -X- _ O
. -X- _ O

Notice -X- _ O
that -X- _ O
positional -X- _ O
encodings -X- _ O
only -X- _ O
depend -X- _ O
on -X- _ O
the -X- _ O
actual -X- _ O
positions -X- _ O
in -X- _ O
the -X- _ O
original -X- _ O
sequence -X- _ O
. -X- _ O

denotes -X- _ O
concatenation -X- _ O
along -X- _ O
the -X- _ O
sequence -X- _ O
dimension -X- _ O
. -X- _ O

Then -X- _ O
, -X- _ O
for -X- _ O
the -X- _ O
next -X- _ O
segment -X- _ O
x -X- _ O
, -X- _ O
the -X- _ O
attention -X- _ O
update -X- _ O
with -X- _ O
memory -X- _ O
can -X- _ O
be -X- _ O
written -X- _ O
as -X- _ O
h -X- _ O
i -X- _ O
( -X- _ O
m−1 -X- _ O
) -X- _ O
h(m -X- _ O
) -X- _ O
, -X- _ O
KV -X- _ O
= -X- _ O
h̃(m−1 -X- _ O
) -X- _ O
, -X- _ O
hz(m−1 -X- _ O
) -X- _ O
; -X- _ O
θ -X- _ O
) -X- _ O
zt -X- _ O
← -X- _ O
Attention(Q -X- _ O
= -X- _ O
hzt -X- _ O
≤t -X- _ O
where -X- _ O
[ -X- _ O
. -X- _ O
, -X- _ O
. -X- _ O
] -X- _ O

Then -X- _ O
, -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
permutation -X- _ O
z̃ -X- _ O
, -X- _ O
we -X- _ O
process -X- _ O
the -X- _ O
first -X- _ O
segment -X- _ O
, -X- _ O
and -X- _ O
then -X- _ O
cache -X- _ O
the -X- _ O
obtained -X- _ O
content -X- _ O
representations -X- _ O
h̃(m -X- _ O
) -X- _ O
for -X- _ O
each -X- _ O
layer -X- _ O
m. -X- _ O

+ -X- _ O
1 -X- _ O
· -X- _ O
· -X- _ O
· -X- _ O
2 -X- _ O
T -X- _ O
] -X- _ O
respectively -X- _ O
. -X- _ O

Let -X- _ O
z̃ -X- _ O
and -X- _ O
z -X- _ O
be -X- _ O
permutations -X- _ O
of -X- _ O
[ -X- _ O
1 -X- _ O
· -X- _ O
· -X- _ O
· -X- _ O
T -X- _ O
] -X- _ O
and -X- _ O
[ -X- _ O
T -X- _ O

Without -X- _ O
loss -X- _ O
of -X- _ O
generality -X- _ O
, -X- _ O
suppose -X- _ O
we -X- _ O
have -X- _ O
two -X- _ O
segments -X- _ O
taken -X- _ O
from -X- _ O
a -X- _ O
long -X- _ O
sequence -X- _ O
s -X- _ O
; -X- _ O
i.e. -X- _ O
, -X- _ O
x̃ -X- _ O
= -X- _ O
s1 -X- _ O
: -X- _ O
T -X- _ O
and -X- _ O
x -X- _ O
= -X- _ O
sT -X- _ O
+1:2 -X- _ O
T -X- _ O
. -X- _ O

Now -X- _ O
we -X- _ O
discuss -X- _ O
how -X- _ O
to -X- _ O
integrate -X- _ O
the -X- _ O
recurrence -X- _ O
mechanism -X- _ O
into -X- _ O
the -X- _ O
proposed -X- _ O
permutation -X- _ O
setting -X- _ O
and -X- _ O
enable -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
reuse -X- _ O
hidden -X- _ O
states -X- _ O
from -X- _ O
previous -X- _ O
segments -X- _ O
. -X- _ O

We -X- _ O
apply -X- _ O
relative -X- _ O
positional -X- _ O
encodings -X- _ O
based -X- _ O
on -X- _ O
the -X- _ O
original -X- _ O
sequence -X- _ O
as -X- _ O
discussed -X- _ O
earlier -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
straightforward -X- _ O
. -X- _ O

We -X- _ O
integrate -X- _ O
two -X- _ O
important -X- _ O
techniques -X- _ O
in -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
, -X- _ O
namely -X- _ O
the -X- _ O
relative -X- _ O
positional -X- _ O
encoding -X- _ O
scheme -X- _ O
and -X- _ O
the -X- _ O
segment -X- _ O
recurrence -X- _ O
mechanism -X- _ O
. -X- _ O

2.4 -X- _ O
Incorporating -X- _ O
Ideas -X- _ O
from -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
Since -X- _ O
our -X- _ O
objective -X- _ O
function -X- _ O
fits -X- _ O
in -X- _ O
the -X- _ O
AR -X- _ O
framework -X- _ O
, -X- _ O
we -X- _ O
incorporate -X- _ O
the -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
AR -X- _ O
language -X- _ O
model -X- _ O
, -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
[ -X- _ O
9 -X- _ O
] -X- _ O
, -X- _ O
into -X- _ O
our -X- _ O
pretraining -X- _ O
framework -X- _ O
, -X- _ O
and -X- _ O
name -X- _ O
our -X- _ O
method -X- _ O
after -X- _ O
it -X- _ O
. -X- _ O

For -X- _ O
unselected -X- _ O
tokens -X- _ O
, -X- _ O
their -X- _ O
query -X- _ O
representations -X- _ O
need -X- _ O
not -X- _ O
be -X- _ O
computed -X- _ O
, -X- _ O
which -X- _ O
saves -X- _ O
speed -X- _ O
and -X- _ O
memory -X- _ O
. -X- _ O

A -X- _ O
hyperparameter -X- _ O
K -X- _ B-HyperparameterName
is -X- _ O
used -X- _ O
such -X- _ O
that -X- _ O
about -X- _ O
1 -X- _ O
/ -X- _ O
K -X- _ O
tokens -X- _ O
are -X- _ O
selected -X- _ O
for -X- _ O
predictions -X- _ O
; -X- _ O
i.e. -X- _ O
, -X- _ O
|z| -X- _ O
/(|z| -X- _ O
− -X- _ O
c -X- _ O
) -X- _ O
≈ -X- _ O
K. -X- _ O

Ez∼ZT -X- _ O
 -X- _ O
log -X- _ O
pθ -X- _ O
( -X- _ O
xzt -X- _ O
| -X- _ O
xz -X- _ O
< -X- _ O
t -X- _ O
) -X- _ O
. -X- _ O
( -X- _ O
5 -X- _ O
) -X- _ O
θ -X- _ O
t -X- _ O
= -X- _ O
c+1 -X- _ O
Note -X- _ O
that -X- _ O
z -X- _ O
> -X- _ O
c -X- _ O
is -X- _ O
chosen -X- _ O
as -X- _ O
the -X- _ O
target -X- _ O
because -X- _ O
it -X- _ O
possesses -X- _ O
the -X- _ O
longest -X- _ O
context -X- _ O
in -X- _ O
the -X- _ O
sequence -X- _ O
given -X- _ O
the -X- _ O
current -X- _ O
factorization -X- _ O
order -X- _ O
z. -X- _ O

Ez∼ZT -X- _ O
log -X- _ O
pθ -X- _ O
( -X- _ O
xz -X- _ O
> -X- _ O
c -X- _ O
| -X- _ O
xz≤c -X- _ O
) -X- _ O
= -X- _ O

The -X- _ O
objective -X- _ O
is -X- _ O
to -X- _ O
maximize -X- _ O
the -X- _ O
log -X- _ O
- -X- _ O
likelihood -X- _ O
of -X- _ O
the -X- _ O
target -X- _ O
subsequence -X- _ O
conditioned -X- _ O
on -X- _ O
the -X- _ O
non -X- _ O
- -X- _ O
target -X- _ O
subsequence -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
 -X- _ O
 -X- _ O
|z| -X- _ O
h -X- _ O
i -X- _ O
X -X- _ O
max -X- _ O

Formally -X- _ O
, -X- _ O
we -X- _ O
split -X- _ O
z -X- _ O
into -X- _ O
a -X- _ O
non -X- _ O
- -X- _ O
target -X- _ O
subsequence -X- _ O
z≤c -X- _ O
and -X- _ O
a -X- _ O
target -X- _ O
subsequence -X- _ O
z -X- _ O
> -X- _ O
c -X- _ O
, -X- _ O
where -X- _ O
c -X- _ O
is -X- _ O
the -X- _ O
cutting -X- _ O
point -X- _ O
. -X- _ O

To -X- _ O
reduce -X- _ O
the -X- _ O
optimization -X- _ O
difficulty -X- _ O
, -X- _ O
we -X- _ O
choose -X- _ O
to -X- _ O
only -X- _ O
predict -X- _ O
the -X- _ O
last -X- _ O
tokens -X- _ O
in -X- _ O
a -X- _ O
factorization -X- _ O
order -X- _ O
. -X- _ O

Partial -X- _ O
Prediction -X- _ O

Finally -X- _ O
, -X- _ O
( -X- _ O
M -X- _ O
) -X- _ O
we -X- _ O
can -X- _ O
use -X- _ O
the -X- _ O
last -X- _ O
- -X- _ O
layer -X- _ O
query -X- _ O
representation -X- _ O
gzt -X- _ O
to -X- _ O
compute -X- _ O
Eq -X- _ O
. -X- _ O
( -X- _ O
4 -X- _ O
) -X- _ O
. -X- _ O

The -X- _ O
update -X- _ O
rule -X- _ O
of -X- _ O
the -X- _ O
content -X- _ O
representations -X- _ O
is -X- _ O
exactly -X- _ O
the -X- _ O
same -X- _ O
as -X- _ O
the -X- _ O
standard -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
, -X- _ O
so -X- _ O
during -X- _ O
finetuning -X- _ O
, -X- _ O
we -X- _ O
can -X- _ O
simply -X- _ O
drop -X- _ O
the -X- _ O
query -X- _ O
stream -X- _ O
and -X- _ O
use -X- _ O
the -X- _ O
content -X- _ O
stream -X- _ O
as -X- _ O
a -X- _ O
normal -X- _ O
Transformer(-XL -X- _ B-MethodName
) -X- _ B-MethodName
. -X- _ O

where -X- _ O
Q -X- _ O
, -X- _ O
K -X- _ O
, -X- _ O
V -X- _ O
denote -X- _ O
the -X- _ O
query -X- _ O
, -X- _ O
key -X- _ O
, -X- _ O
and -X- _ O
value -X- _ O
in -X- _ O
an -X- _ O
attention -X- _ O
operation -X- _ O
[ -X- _ O
33 -X- _ O
] -X- _ O
. -X- _ O

← -X- _ O
Attention(Q -X- _ O
= -X- _ O
gz(m−1 -X- _ O
) -X- _ O
, -X- _ O
KV -X- _ O
= -X- _ O
hz(m−1 -X- _ O
) -X- _ O
; -X- _ O
θ -X- _ O
) -X- _ O
, -X- _ O
gz(m -X- _ O
) -X- _ O
t -X- _ O
t -X- _ O
< -X- _ O
t -X- _ O
( -X- _ O
query -X- _ O
stream -X- _ O
: -X- _ O
use -X- _ O
zt -X- _ O
but -X- _ O
can -X- _ O
not -X- _ O
see -X- _ O
xzt -X- _ O
) -X- _ O
( -X- _ O
m−1 -X- _ O
) -X- _ O
h(m -X- _ O
) -X- _ O
, -X- _ O
KV -X- _ O
= -X- _ O
h(m−1 -X- _ O
) -X- _ O
; -X- _ O
θ -X- _ O
) -X- _ O
, -X- _ O
zt -X- _ O
← -X- _ O
Attention(Q -X- _ O
= -X- _ O
hzt -X- _ O
z≤t -X- _ O
( -X- _ O
content -X- _ O
stream -X- _ O
: -X- _ O
use -X- _ O
both -X- _ O
zt -X- _ O
and -X- _ O
xzt -X- _ O
) -X- _ O
. -X- _ O

with -X- _ O
a -X- _ O
shared -X- _ O
set -X- _ O
of -X- _ O
parameters -X- _ O
as -X- _ O
follows -X- _ O
( -X- _ O
illustrated -X- _ O
in -X- _ O
Figures -X- _ O
1 -X- _ O
( -X- _ O
a -X- _ O
) -X- _ O
and -X- _ O
( -X- _ O
b -X- _ O
) -X- _ O
): -X- _ O

4 -X- _ O

The -X- _ O
details -X- _ O
are -X- _ O
included -X- _ O
in -X- _ O
Appendix -X- _ O
A.2 -X- _ O
for -X- _ O
reference -X- _ O
. -X- _ O

For -X- _ O
each -X- _ O
self -X- _ O
- -X- _ O
attention -X- _ O
layer -X- _ O
m -X- _ O
= -X- _ O
1 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O

( -X- _ O
0 -X- _ O
) -X- _ O
Computationally -X- _ O
, -X- _ O
the -X- _ O
first -X- _ O
layer -X- _ O
query -X- _ O
stream -X- _ O
is -X- _ O
initialized -X- _ O
with -X- _ O
a -X- _ O
trainable -X- _ O
vector -X- _ O
, -X- _ O
i.e. -X- _ O
gi -X- _ O
= -X- _ O
w -X- _ O
, -X- _ O
( -X- _ O
0 -X- _ O
) -X- _ O
while -X- _ O
the -X- _ O
content -X- _ O
stream -X- _ O
is -X- _ O
set -X- _ O
to -X- _ O
the -X- _ O
corresponding -X- _ O
word -X- _ O
embedding -X- _ O
, -X- _ O
i.e. -X- _ O
hi -X- _ O
= -X- _ O
e(xi -X- _ O
) -X- _ O
. -X- _ O

zt -X- _ O
, -X- _ O
but -X- _ O
not -X- _ O
the -X- _ O
content -X- _ O
xzt -X- _ O
, -X- _ O
as -X- _ O
discussed -X- _ O
above -X- _ O
. -X- _ O

This -X- _ O
representation -X- _ O
encodes -X- _ O
both -X- _ O
the -X- _ O
context -X- _ O
and -X- _ O
xzt -X- _ O
itself -X- _ O
. -X- _ O

To -X- _ O
resolve -X- _ O
such -X- _ O
a -X- _ O
contradiction -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
to -X- _ O
use -X- _ O
two -X- _ O
sets -X- _ O
of -X- _ O
hidden -X- _ O
representations -X- _ O
instead -X- _ O
of -X- _ O
one -X- _ O
: -X- _ O
• -X- _ O
The -X- _ O
content -X- _ O
representation -X- _ O
hθ -X- _ O
( -X- _ O
xz≤t -X- _ O
) -X- _ O
, -X- _ O
or -X- _ O
abbreviated -X- _ O
as -X- _ O
hzt -X- _ O
, -X- _ O
which -X- _ O
serves -X- _ O
a -X- _ O
similar -X- _ O
role -X- _ O
to -X- _ O
the -X- _ O
standard -X- _ O
hidden -X- _ O
states -X- _ O
in -X- _ O
Transformer -X- _ O
. -X- _ O

For -X- _ O
this -X- _ O
parameterization -X- _ O
to -X- _ O
work -X- _ O
, -X- _ O
there -X- _ O
are -X- _ O
two -X- _ O
requirements -X- _ O
that -X- _ O
are -X- _ O
contradictory -X- _ O
in -X- _ O
a -X- _ O
standard -X- _ O
Transformer -X- _ O
architecture -X- _ O
: -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
token -X- _ O
xzt -X- _ O
, -X- _ O
gθ -X- _ O
( -X- _ O
xz -X- _ O
< -X- _ O
t -X- _ O
, -X- _ O
zt -X- _ O
) -X- _ O
should -X- _ O
only -X- _ O
use -X- _ O
the -X- _ O
position -X- _ O
zt -X- _ O
and -X- _ O
not -X- _ O
the -X- _ O
content -X- _ O
xzt -X- _ O
, -X- _ O
otherwise -X- _ O
the -X- _ O
objective -X- _ O
becomes -X- _ O
trivial -X- _ O
; -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
to -X- _ O
predict -X- _ O
the -X- _ O
other -X- _ O
tokens -X- _ O
xzj -X- _ O
with -X- _ O
j -X- _ O
> -X- _ O
t -X- _ O
, -X- _ O
gθ -X- _ O
( -X- _ O
xz -X- _ O
< -X- _ O
t -X- _ O
, -X- _ O
zt -X- _ O
) -X- _ O
should -X- _ O
also -X- _ O
encode -X- _ O
the -X- _ O
content -X- _ O
xzt -X- _ O
to -X- _ O
provide -X- _ O
full -X- _ O
contextual -X- _ O
information -X- _ O
. -X- _ O

Among -X- _ O
other -X- _ O
possibilities -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
to -X- _ O
“ -X- _ O
stand -X- _ O
” -X- _ O
at -X- _ O
the -X- _ O
target -X- _ O
position -X- _ O
zt -X- _ O
and -X- _ O
rely -X- _ O
on -X- _ O
the -X- _ O
position -X- _ O
zt -X- _ O
to -X- _ O
gather -X- _ O
information -X- _ O
from -X- _ O
the -X- _ O
context -X- _ O
xz -X- _ O
< -X- _ O
t -X- _ O
through -X- _ O
attention -X- _ O
. -X- _ O

where -X- _ O
gθ -X- _ O
( -X- _ O
xz -X- _ O
< -X- _ O
t -X- _ O
, -X- _ O
zt -X- _ O
) -X- _ O
denotes -X- _ O
a -X- _ O
new -X- _ O
type -X- _ O
of -X- _ O
representations -X- _ O
which -X- _ O
additionally -X- _ O
take -X- _ O
the -X- _ O
target -X- _ O
position -X- _ O
zt -X- _ O
as -X- _ O
input -X- _ O
. -X- _ O

P -X- _ O
0 -X- _ O
> -X- _ O
x0 -X- _ O
exp -X- _ O
( -X- _ O
e(x -X- _ O
) -X- _ O
gθ -X- _ O
( -X- _ O
xz -X- _ O
< -X- _ O
t -X- _ O
, -X- _ O
zt -X- _ O
) -X- _ O
) -X- _ O

| -X- _ O
xz -X- _ O
< -X- _ O
t -X- _ O
) -X- _ O
= -X- _ O

gθ -X- _ O
( -X- _ O
xz -X- _ O
< -X- _ O
t -X- _ O
, -X- _ O
zt -X- _ O
) -X- _ O
, -X- _ O
( -X- _ O
4 -X- _ O
) -X- _ O
pθ -X- _ O
( -X- _ O
Xzt -X- _ O
= -X- _ O
x -X- _ O

Consequently -X- _ O
, -X- _ O
the -X- _ O
same -X- _ O
distribution -X- _ O
is -X- _ O
predicted -X- _ O
regardless -X- _ O
of -X- _ O
the -X- _ O
target -X- _ O
position -X- _ O
, -X- _ O
which -X- _ O
is -X- _ O
not -X- _ O
able -X- _ O
to -X- _ O
learn -X- _ O
useful -X- _ O
representations -X- _ O
( -X- _ O
see -X- _ O
Appendix -X- _ O
A.1 -X- _ O
for -X- _ O
a -X- _ O
concrete -X- _ O
example -X- _ O
) -X- _ O
. -X- _ O

Now -X- _ O
notice -X- _ O
that -X- _ O
the -X- _ O
representation -X- _ O
hθ -X- _ O
( -X- _ O
xz -X- _ O
< -X- _ O
t -X- _ O
) -X- _ O
does -X- _ O
not -X- _ O
depend -X- _ O
on -X- _ O
which -X- _ O
position -X- _ O
it -X- _ O
will -X- _ O
predict -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
the -X- _ O
value -X- _ O
of -X- _ O
zt -X- _ O
. -X- _ O

θ -X- _ O
x0 -X- _ O
produced -X- _ O
by -X- _ O
the -X- _ O
shared -X- _ O
Transformer -X- _ O
network -X- _ O
after -X- _ O
proper -X- _ O
masking -X- _ O
. -X- _ O

P -X- _ O
exp -X- _ O
e(x0 -X- _ O
) -X- _ O
> -X- _ O
h -X- _ O
( -X- _ O
x -X- _ O
, -X- _ O
where -X- _ O
hθ -X- _ O
( -X- _ O
xz -X- _ O
< -X- _ O
t -X- _ O
) -X- _ O
denotes -X- _ O
the -X- _ O
hidden -X- _ O
representation -X- _ O
of -X- _ O
xz -X- _ O
< -X- _ O
t -X- _ O
( -X- _ O
z -X- _ O
< -X- _ O
t -X- _ O
) -X- _ O
) -X- _ O

| -X- _ O
xz -X- _ O
< -X- _ O
t -X- _ O
) -X- _ O
= -X- _ O

( -X- _ O
xz -X- _ O
< -X- _ O
t -X- _ O
) -X- _ O
) -X- _ O
x -X- _ O

hθ -X- _ O

To -X- _ O
see -X- _ O
the -X- _ O
problem -X- _ O
, -X- _ O
assume -X- _ O
we -X- _ O
parameterize -X- _ O
the -X- _ O
next -X- _ O
- -X- _ O
token -X- _ O
distribution -X- _ O
pθ -X- _ O
( -X- _ O
Xzt -X- _ O
| -X- _ O
xz -X- _ O
< -X- _ O
t -X- _ O
) -X- _ O
using -X- _ O
the -X- _ O
standard -X- _ O
Softmax -X- _ O
formulation -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
pθ -X- _ O
( -X- _ O
Xzt -X- _ O
= -X- _ O
exp(e(x -X- _ O
) -X- _ O
> -X- _ O

( -X- _ O
c -X- _ O
): -X- _ O
Overview -X- _ O
of -X- _ O
the -X- _ O
permutation -X- _ B-TaskName
language -X- _ I-TaskName
modeling -X- _ I-TaskName
training -X- _ O
with -X- _ O
two -X- _ B-MethodName
- -X- _ I-MethodName
stream -X- _ I-MethodName
attention -X- _ I-MethodName
. -X- _ O

w -X- _ O
Sample -X- _ O
a -X- _ O
factorization -X- _ O
order -X- _ O
: -X- _ O
3à2à4à1 -X- _ O
( -X- _ O
c -X- _ O
) -X- _ O
Figure -X- _ O
1 -X- _ O
: -X- _ O
( -X- _ O
a -X- _ O
): -X- _ O
Content -X- _ B-MethodName
stream -X- _ I-MethodName
attention -X- _ I-MethodName
, -X- _ O
which -X- _ O
is -X- _ O
the -X- _ O
same -X- _ O
as -X- _ O
the -X- _ O
standard -X- _ O
self -X- _ B-MethodName
- -X- _ I-MethodName
attention -X- _ I-MethodName
. -X- _ O

g -X- _ O
' -X- _ O
h -X- _ O
( -X- _ O
( -X- _ O
, -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
( -X- _ O
, -X- _ O
) -X- _ O
h -X- _ O
) -X- _ O
( -X- _ O
, -X- _ O
) -X- _ O
g -X- _ O
) -X- _ O
e(x$ -X- _ O
) -X- _ O
w -X- _ O
e(x -X- _ O
' -X- _ O
) -X- _ O
( -X- _ O
b -X- _ O
) -X- _ O
w -X- _ O
e(x -X- _ O
( -X- _ O
) -X- _ O
w -X- _ O
e(x -X- _ O
) -X- _ O
) -X- _ O

( -X- _ O
, -X- _ O
) -X- _ O
h -X- _ O
' -X- _ O
( -X- _ O
, -X- _ O
) -X- _ O
( -X- _ O
, -X- _ O
) -X- _ O
g -X- _ O
' -X- _ O
h -X- _ O
( -X- _ O
( -X- _ O
, -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
( -X- _ O
, -X- _ O
) -X- _ O
h -X- _ O
) -X- _ O

g -X- _ O
' -X- _ O
x -X- _ O
( -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
h -X- _ O
( -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
h -X- _ O
) -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g -X- _ O
) -X- _ O
K -X- _ O
, -X- _ O
V -X- _ O
Attention -X- _ O
Masks -X- _ O
( -X- _ O
, -X- _ O
) -X- _ O
h$ -X- _ O
( -X- _ O
, -X- _ O
) -X- _ O
g$ -X- _ O

g$ -X- _ O
Attention -X- _ O
Q -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
h$ -X- _ O
x -X- _ O
' -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
g$ -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O
h -X- _ O
' -X- _ O
( -X- _ O
' -X- _ O
) -X- _ O

To -X- _ O
provide -X- _ O
an -X- _ O
overall -X- _ O
picture -X- _ O
, -X- _ O
we -X- _ O
show -X- _ O
an -X- _ O
example -X- _ O
of -X- _ O
predicting -X- _ O
the -X- _ O
token -X- _ O
x3 -X- _ O
given -X- _ O
the -X- _ O
same -X- _ O
input -X- _ O
sequence -X- _ O
x -X- _ O
but -X- _ O
under -X- _ O
different -X- _ O
factorization -X- _ O
orders -X- _ O
in -X- _ O
the -X- _ O
Appendix -X- _ O
A.7 -X- _ O
with -X- _ O
Figure -X- _ O
4 -X- _ O
. -X- _ O
3 -X- _ O

Note -X- _ O
that -X- _ O
this -X- _ O
choice -X- _ O
is -X- _ O
necessary -X- _ O
, -X- _ O
since -X- _ O
the -X- _ O
model -X- _ O
will -X- _ O
only -X- _ O
encounter -X- _ O
text -X- _ O
sequences -X- _ O
with -X- _ O
the -X- _ O
natural -X- _ O
order -X- _ O
during -X- _ O
finetuning -X- _ O
. -X- _ O

In -X- _ O
other -X- _ O
words -X- _ O
, -X- _ O
we -X- _ O
keep -X- _ O
the -X- _ O
original -X- _ O
sequence -X- _ O
order -X- _ O
, -X- _ O
use -X- _ O
the -X- _ O
positional -X- _ O
encodings -X- _ O
corresponding -X- _ O
to -X- _ O
the -X- _ O
original -X- _ O
sequence -X- _ O
, -X- _ O
and -X- _ O
rely -X- _ O
on -X- _ O
a -X- _ O
proper -X- _ O
attention -X- _ O
mask -X- _ O
in -X- _ O
Transformers -X- _ O
to -X- _ O
achieve -X- _ O
permutation -X- _ O
of -X- _ O
the -X- _ O
factorization -X- _ O
order -X- _ O
. -X- _ O

Remark -X- _ O
on -X- _ O
Permutation -X- _ O
The -X- _ O
proposed -X- _ O
objective -X- _ O
only -X- _ O
permutes -X- _ O
the -X- _ O
factorization -X- _ O
order -X- _ O
, -X- _ O
not -X- _ O
the -X- _ O
sequence -X- _ O
order -X- _ O
. -X- _ O

Moreover -X- _ O
, -X- _ O
as -X- _ O
this -X- _ O
objective -X- _ O
fits -X- _ O
into -X- _ O
the -X- _ O
AR -X- _ O
framework -X- _ O
, -X- _ O
it -X- _ O
naturally -X- _ O
avoids -X- _ O
the -X- _ O
independence -X- _ O
assumption -X- _ O
and -X- _ O
the -X- _ O
pretrain -X- _ O
- -X- _ O
finetune -X- _ O
discrepancy -X- _ O
discussed -X- _ O
in -X- _ O
Section -X- _ O
2.1 -X- _ O
. -X- _ O

( -X- _ O
3 -X- _ O
) -X- _ O
θ -X- _ O
t=1 -X- _ O
Essentially -X- _ O
, -X- _ O
for -X- _ O
a -X- _ O
text -X- _ O
sequence -X- _ O
x -X- _ O
, -X- _ O
we -X- _ O
sample -X- _ O
a -X- _ O
factorization -X- _ O
order -X- _ O
z -X- _ O
at -X- _ O
a -X- _ O
time -X- _ O
and -X- _ O
decompose -X- _ O
the -X- _ O
likelihood -X- _ O
pθ -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
according -X- _ O
to -X- _ O
factorization -X- _ O
order -X- _ O
. -X- _ O

Ez∼ZT -X- _ O
log -X- _ O
pθ -X- _ O
( -X- _ O
xzt -X- _ O
| -X- _ O
xz -X- _ O
< -X- _ O
t -X- _ O
) -X- _ O
. -X- _ O

We -X- _ O
use -X- _ O
zt -X- _ O
and -X- _ O
z -X- _ O
< -X- _ O
t -X- _ O
to -X- _ O
denote -X- _ O
the -X- _ O
t -X- _ O
- -X- _ O
th -X- _ O
element -X- _ O
and -X- _ O
the -X- _ O
first -X- _ O
t−1 -X- _ O
elements -X- _ O
of -X- _ O
a -X- _ O
permutation -X- _ O
z -X- _ O
∈ -X- _ O
ZT -X- _ O
. -X- _ O

, -X- _ O
T -X- _ O
] -X- _ O
. -X- _ O

To -X- _ O
formalize -X- _ O
the -X- _ O
idea -X- _ O
, -X- _ O
let -X- _ O
ZT -X- _ O
be -X- _ O
the -X- _ O
set -X- _ O
of -X- _ O
all -X- _ O
possible -X- _ O
permutations -X- _ O
of -X- _ O
the -X- _ O
length -X- _ O
- -X- _ O
T -X- _ O
index -X- _ O
sequence -X- _ O
[ -X- _ O
1 -X- _ O
, -X- _ O
2 -X- _ O
, -X- _ O
. -X- _ O
. -X- _ O
. -X- _ O

Intuitively -X- _ O
, -X- _ O
if -X- _ O
model -X- _ O
parameters -X- _ O
are -X- _ O
shared -X- _ O
across -X- _ O
all -X- _ O
factorization -X- _ O
orders -X- _ O
, -X- _ O
in -X- _ O
expectation -X- _ O
, -X- _ O
the -X- _ O
model -X- _ O
will -X- _ O
learn -X- _ O
to -X- _ O
gather -X- _ O
information -X- _ O
from -X- _ O
all -X- _ O
positions -X- _ O
on -X- _ O
both -X- _ O
sides -X- _ O
. -X- _ O

different -X- _ O
orders -X- _ O
to -X- _ O
perform -X- _ O
a -X- _ O
valid -X- _ O
autoregressive -X- _ O
factorization -X- _ O
. -X- _ O

Specifically -X- _ O
, -X- _ O
for -X- _ O
a -X- _ O
sequence -X- _ O
x -X- _ O
of -X- _ O
length -X- _ O
T -X- _ O
, -X- _ O
there -X- _ O
are -X- _ O
T -X- _ O
! -X- _ O

A -X- _ O
natural -X- _ O
question -X- _ O
to -X- _ O
ask -X- _ O
is -X- _ O
whether -X- _ O
there -X- _ O
exists -X- _ O
a -X- _ O
pretraining -X- _ O
objective -X- _ O
that -X- _ O
brings -X- _ O
the -X- _ O
advantages -X- _ O
of -X- _ O
both -X- _ O
while -X- _ O
avoiding -X- _ O
their -X- _ O
weaknesses -X- _ O
. -X- _ O

As -X- _ O
a -X- _ O
result -X- _ O
, -X- _ O
the -X- _ O
BERT -X- _ B-MethodName
objective -X- _ O
allows -X- _ O
the -X- _ O
model -X- _ O
to -X- _ O
be -X- _ O
pretrained -X- _ O
to -X- _ O
better -X- _ O
capture -X- _ O
bidirectional -X- _ O
context -X- _ O
. -X- _ O

Replacing -X- _ O
[ -X- _ O
MASK -X- _ O
] -X- _ O
with -X- _ O
original -X- _ O
tokens -X- _ O
as -X- _ O
in -X- _ O
[ -X- _ O
10 -X- _ O
] -X- _ O
does -X- _ O
not -X- _ O
solve -X- _ O
the -X- _ O
problem -X- _ O
because -X- _ O
original -X- _ O
tokens -X- _ O
can -X- _ O
be -X- _ O
only -X- _ O
used -X- _ O
with -X- _ O
a -X- _ O
small -X- _ O
probability -X- _ O
— -X- _ O
otherwise -X- _ O
Eq -X- _ O
. -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
will -X- _ O
be -X- _ O
trivial -X- _ O
to -X- _ O
optimize -X- _ O
. -X- _ O

[ -X- _ O
MASK -X- _ O
] -X- _ O
that -X- _ O
never -X- _ O
occur -X- _ O
in -X- _ O
downstream -X- _ O
tasks -X- _ O
, -X- _ O
which -X- _ O
creates -X- _ O
a -X- _ O
pretrain -X- _ O
- -X- _ O
finetune -X- _ O
discrepancy -X- _ O
. -X- _ O

• -X- _ O
Input -X- _ O
noise -X- _ O
: -X- _ O
The -X- _ O
input -X- _ O
to -X- _ O
BERT -X- _ B-MethodName
contains -X- _ O
artificial -X- _ O
symbols -X- _ O
like -X- _ O

The -X- _ O
pros -X- _ O
and -X- _ O
cons -X- _ O
of -X- _ O
the -X- _ O
two -X- _ O
pretraining -X- _ O
objectives -X- _ O
are -X- _ O
compared -X- _ O
in -X- _ O
the -X- _ O
following -X- _ O
aspects -X- _ O
: -X- _ O
• -X- _ O
Independence -X- _ O
Assumption -X- _ O
: -X- _ O
As -X- _ O
emphasized -X- _ O
by -X- _ O
the -X- _ O
≈ -X- _ O
sign -X- _ O
in -X- _ O
Eq -X- _ O
. -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
factorizes -X- _ O
the -X- _ O
joint -X- _ O
conditional -X- _ O
probability -X- _ O
p(x̄ -X- _ O
| -X- _ O
x̂ -X- _ O
) -X- _ O
based -X- _ O
on -X- _ O
an -X- _ O
independence -X- _ O
assumption -X- _ O
that -X- _ O
all -X- _ O
masked -X- _ O
tokens -X- _ O
x̄ -X- _ O
are -X- _ O
separately -X- _ O
reconstructed -X- _ O
. -X- _ O

[ -X- _ O
Hθ -X- _ O
( -X- _ O
x)1 -X- _ O
, -X- _ O
Hθ -X- _ O
( -X- _ O
x)2 -X- _ O
, -X- _ O
· -X- _ O
· -X- _ O
· -X- _ O
, -X- _ O
Hθ -X- _ O
( -X- _ O
x)T -X- _ O
] -X- _ O
. -X- _ O

Hθ -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
= -X- _ O

Specifically -X- _ O
, -X- _ O
for -X- _ O
a -X- _ O
text -X- _ O
sequence -X- _ O
x -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
first -X- _ O
constructs -X- _ O
a -X- _ O
corrupted -X- _ O
version -X- _ O
x̂ -X- _ O
by -X- _ O
randomly -X- _ O
setting -X- _ O
a -X- _ O
portion -X- _ O
( -X- _ O
e.g. -X- _ O
15 -X- _ O
% -X- _ O
) -X- _ O
of -X- _ O
tokens -X- _ O
in -X- _ O
x -X- _ O
to -X- _ O
a -X- _ O
special -X- _ O
symbol -X- _ O

log -X- _ O
, -X- _ O
> -X- _ O
0 -X- _ O
x0 -X- _ O
exp -X- _ O
( -X- _ O
hθ -X- _ O
( -X- _ O
x1 -X- _ O
: -X- _ O
t−1 -X- _ O
) -X- _ O
e(x -X- _ O
) -X- _ O
) -X- _ O
t=1 -X- _ O
t=1 -X- _ O
2 -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O

max -X- _ O
θ -X- _ O
T -X- _ O
X -X- _ O
T -X- _ O
X -X- _ O
 -X- _ O
exp -X- _ O
hθ -X- _ O
( -X- _ O
x1 -X- _ O
: -X- _ O
t−1 -X- _ O
) -X- _ O
> -X- _ O
e(xt -X- _ O
) -X- _ O
P -X- _ O
log -X- _ O
pθ -X- _ O
( -X- _ O
x -X- _ O
) -X- _ O
= -X- _ O
log -X- _ O
pθ -X- _ O
( -X- _ O
xt -X- _ O
| -X- _ O
x -X- _ O
< -X- _ O
t -X- _ O
) -X- _ O
= -X- _ O

[ -X- _ O
11 -X- _ O
] -X- _ O
, -X- _ O
which -X- _ O
only -X- _ O
considers -X- _ O
a -X- _ O
fixed -X- _ O
order -X- _ O
though -X- _ O
. -X- _ O

Finally -X- _ O
, -X- _ O
for -X- _ O
both -X- _ O
orderless -X- _ O
NADE -X- _ B-MethodName
and -X- _ O
XLNet -X- _ B-MethodName
, -X- _ O
we -X- _ O
would -X- _ O
like -X- _ O
to -X- _ O
emphasize -X- _ O
that -X- _ O
“ -X- _ O
orderless -X- _ O
” -X- _ O
does -X- _ O
not -X- _ O
mean -X- _ O
that -X- _ O
the -X- _ O
input -X- _ O
sequence -X- _ O
can -X- _ O
be -X- _ O
randomly -X- _ O
permuted -X- _ O
but -X- _ O
that -X- _ O
the -X- _ O
model -X- _ O
allows -X- _ O
for -X- _ O
different -X- _ O
factorization -X- _ O
orders -X- _ O
of -X- _ O
the -X- _ O
distribution -X- _ O
. -X- _ O

Related -X- _ O
Work -X- _ O

As -X- _ O
a -X- _ O
solution -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
to -X- _ O
reparameterize -X- _ O
the -X- _ O
Transformer(-XL -X- _ B-MethodName
) -X- _ I-MethodName
network -X- _ O
to -X- _ O
remove -X- _ O
the -X- _ O
ambiguity -X- _ O
. -X- _ O

In -X- _ O
addition -X- _ O
to -X- _ O
a -X- _ O
novel -X- _ O
pretraining -X- _ O
objective -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
improves -X- _ O
architectural -X- _ O
designs -X- _ O
for -X- _ O
pretraining -X- _ O
. -X- _ O

Meanwhile -X- _ O
, -X- _ O
the -X- _ O
autoregressive -X- _ O
objective -X- _ O
also -X- _ O
provides -X- _ O
a -X- _ O
natural -X- _ O
way -X- _ O
to -X- _ O
use -X- _ O
the -X- _ O
product -X- _ O
rule -X- _ O
for -X- _ O
factorizing -X- _ O
the -X- _ O
joint -X- _ O
probability -X- _ O
of -X- _ O
the -X- _ O
predicted -X- _ O
tokens -X- _ O
, -X- _ O
eliminating -X- _ O
the -X- _ O
independence -X- _ O
assumption -X- _ O
made -X- _ O
in -X- _ O
BERT -X- _ B-MethodName
. -X- _ O

Hence -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
does -X- _ O
not -X- _ O
suffer -X- _ O
from -X- _ O
the -X- _ O
pretrain -X- _ O
- -X- _ O
finetune -X- _ O
discrepancy -X- _ O
that -X- _ O
BERT -X- _ B-MethodName
is -X- _ O
subject -X- _ O
to -X- _ O
. -X- _ O

• -X- _ O
Secondly -X- _ O
, -X- _ O
as -X- _ O
a -X- _ O
generalized -X- _ O
AR -X- _ O
language -X- _ O
model -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
does -X- _ O
not -X- _ O
rely -X- _ O
on -X- _ O
data -X- _ O
corruption -X- _ O
. -X- _ O

In -X- _ O
expectation -X- _ O
, -X- _ O
each -X- _ O
position -X- _ O
learns -X- _ O
to -X- _ O
utilize -X- _ O
contextual -X- _ O
information -X- _ O
from -X- _ O
all -X- _ O
positions -X- _ O
, -X- _ O
i.e. -X- _ O
, -X- _ O
capturing -X- _ O
bidirectional -X- _ O
context -X- _ O
. -X- _ O

Thanks -X- _ O
to -X- _ O
the -X- _ O
permutation -X- _ O
operation -X- _ O
, -X- _ O
the -X- _ O
context -X- _ O
for -X- _ O
each -X- _ O
position -X- _ O
can -X- _ O
consist -X- _ O
of -X- _ O
tokens -X- _ O
from -X- _ O
both -X- _ O
left -X- _ O
and -X- _ O
right -X- _ O
. -X- _ O

all -X- _ O
possible -X- _ O
permutations -X- _ O
of -X- _ O
the -X- _ O
factorization -X- _ O
order -X- _ O
. -X- _ O

Firstly -X- _ O
, -X- _ O
instead -X- _ O
of -X- _ O
using -X- _ O
a -X- _ O
fixed -X- _ O
forward -X- _ O
or -X- _ O
backward -X- _ O
factorization -X- _ O
order -X- _ O
as -X- _ O
in -X- _ O
conventional -X- _ O
AR -X- _ O
models -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
maximizes -X- _ O
the -X- _ O
expected -X- _ O
log -X- _ O
likelihood -X- _ O
of -X- _ O
a -X- _ O
sequence -X- _ O
w.r.t -X- _ O
. -X- _ O

• -X- _ O

However -X- _ O
, -X- _ O
the -X- _ O
artificial -X- _ O
symbols -X- _ O
like -X- _ O
[ -X- _ O
MASK -X- _ O
] -X- _ O
used -X- _ O
by -X- _ O
BERT -X- _ B-MethodName
during -X- _ O
pretraining -X- _ O
are -X- _ O
absent -X- _ O
from -X- _ O
real -X- _ O
data -X- _ O
at -X- _ O
finetuning -X- _ O
time -X- _ O
, -X- _ O
resulting -X- _ O
in -X- _ O
a -X- _ O
pretrain -X- _ O
- -X- _ O
finetune -X- _ O
discrepancy -X- _ O
. -X- _ O

Pretrained -X- _ O
models -X- _ O
and -X- _ O
code -X- _ O
are -X- _ O
available -X- _ O
at -X- _ O
https://github.com/zihangdai/xlnet -X- _ O
33rd -X- _ O
Conference -X- _ O
on -X- _ O
Neural -X- _ O
Information -X- _ O
Processing -X- _ O
Systems -X- _ O
( -X- _ O
NeurIPS -X- _ O
2019 -X- _ O
) -X- _ O
, -X- _ O
Vancouver -X- _ O
, -X- _ O
Canada -X- _ O
. -X- _ O

Order -X- _ O
determined -X- _ O
by -X- _ O
swapping -X- _ O
the -X- _ O
one -X- _ O
in -X- _ O
[ -X- _ O
9 -X- _ O
] -X- _ O
. -X- _ O

Given -X- _ O
the -X- _ O
input -X- _ O
token -X- _ O
sequence -X- _ O
, -X- _ O
a -X- _ O
certain -X- _ O
portion -X- _ O
of -X- _ O
tokens -X- _ O
are -X- _ O
replaced -X- _ O
by -X- _ O
a -X- _ O
special -X- _ O
symbol -X- _ O
[ -X- _ O
MASK -X- _ O
] -X- _ O
, -X- _ O
and -X- _ O
the -X- _ O
model -X- _ O
is -X- _ O
trained -X- _ O
to -X- _ O
recover -X- _ O
the -X- _ O
original -X- _ O
tokens -X- _ O
from -X- _ O
the -X- _ O
corrupted -X- _ O
version -X- _ O
. -X- _ O

[ -X- _ O
10 -X- _ O
] -X- _ O
, -X- _ O
which -X- _ O
has -X- _ O
been -X- _ O
the -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
pretraining -X- _ O
approach -X- _ O
. -X- _ O

A -X- _ O
notable -X- _ O
example -X- _ O
is -X- _ O
BERT -X- _ B-MethodName

A -X- _ O
parametric -X- _ O
model -X- _ O
( -X- _ O
e.g. -X- _ O
a -X- _ O
neural -X- _ O
network -X- _ O
) -X- _ O
is -X- _ O
trained -X- _ O
to -X- _ O
model -X- _ O
each -X- _ O
conditional -X- _ O
distribution -X- _ O
. -X- _ O

t -X- _ O
= -X- _ O
T -X- _ O
p(xt -X- _ O
| -X- _ O
x -X- _ O
> -X- _ O
t -X- _ O
) -X- _ O
. -X- _ O

Under -X- _ O
this -X- _ O
shared -X- _ O
high -X- _ O
- -X- _ O
level -X- _ O
idea -X- _ O
, -X- _ O
different -X- _ O
unsupervised -X- _ O
pretraining -X- _ O
objectives -X- _ O
have -X- _ O
been -X- _ O
explored -X- _ O
in -X- _ O
literature -X- _ O
. -X- _ O

Typically -X- _ O
, -X- _ O
these -X- _ O
methods -X- _ O
first -X- _ O
pretrain -X- _ O
neural -X- _ O
networks -X- _ O
on -X- _ O
large -X- _ O
- -X- _ O
scale -X- _ O
unlabeled -X- _ O
text -X- _ O
corpora -X- _ O
, -X- _ O
and -X- _ O
then -X- _ O
finetune -X- _ O
the -X- _ O
models -X- _ O
or -X- _ O
representations -X- _ O
on -X- _ O
downstream -X- _ O
tasks -X- _ O
. -X- _ O

Empirically -X- _ O
, -X- _ O
under -X- _ O
comparable -X- _ O
experiment -X- _ O
settings -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
outperforms -X- _ O
BERT -X- _ B-TaskName
on -X- _ O
20 -X- _ O
tasks -X- _ O
, -X- _ O
often -X- _ O
by -X- _ O
a -X- _ O
large -X- _ O
margin -X- _ O
, -X- _ O
including -X- _ O
question -X- _ B-TaskName
answering -X- _ I-TaskName
, -X- _ O
natural -X- _ B-TaskName
language -X- _ I-TaskName
inference -X- _ I-TaskName
, -X- _ O
sentiment -X- _ B-TaskName
analysis -X- _ I-TaskName
, -X- _ O
and -X- _ O
document -X- _ B-TaskName
ranking.1 -X- _ I-TaskName
. -X- _ O

Furthermore -X- _ O
, -X- _ O
XLNet -X- _ B-MethodName
integrates -X- _ O
ideas -X- _ O
from -X- _ O
Transformer -X- _ B-MethodName
- -X- _ I-MethodName
XL -X- _ I-MethodName
, -X- _ O
the -X- _ O
state -X- _ O
- -X- _ O
of -X- _ O
- -X- _ O
the -X- _ O
- -X- _ O
art -X- _ O
autoregressive -X- _ O
model -X- _ O
, -X- _ O
into -X- _ O
pretraining -X- _ O
. -X- _ O

In -X- _ O
light -X- _ O
of -X- _ O
these -X- _ O
pros -X- _ O
and -X- _ O
cons -X- _ O
, -X- _ O
we -X- _ O
propose -X- _ O
XLNet -X- _ B-MethodName
, -X- _ O
a -X- _ O
generalized -X- _ O
autoregressive -X- _ O
pretraining -X- _ O
method -X- _ O
that -X- _ O
( -X- _ O
1 -X- _ O
) -X- _ O
enables -X- _ O
learning -X- _ O
bidirectional -X- _ O
contexts -X- _ O
by -X- _ O
maximizing -X- _ O
the -X- _ O
expected -X- _ O
likelihood -X- _ O
over -X- _ O
all -X- _ O
permutations -X- _ O
of -X- _ O
the -X- _ O
factorization -X- _ O
order -X- _ O
and -X- _ O
( -X- _ O
2 -X- _ O
) -X- _ O
overcomes -X- _ O
the -X- _ O
limitations -X- _ O
of -X- _ O
BERT -X- _ B-MethodName
thanks -X- _ O
to -X- _ O
its -X- _ O
autoregressive -X- _ O
formulation -X- _ O
. -X- _ O

However -X- _ O
, -X- _ O
relying -X- _ O
on -X- _ O
corrupting -X- _ O
the -X- _ O
input -X- _ O
with -X- _ O
masks -X- _ O
, -X- _ O
BERT -X- _ B-MethodName
neglects -X- _ O
dependency -X- _ O
between -X- _ O
the -X- _ O
masked -X- _ O
positions -X- _ O
and -X- _ O
suffers -X- _ O
from -X- _ O
a -X- _ O
pretrain -X- _ O
- -X- _ O
finetune -X- _ O
discrepancy -X- _ O
. -X- _ O

Yiming -X- _ O
Yang1 -X- _ O
, -X- _ O
Jaime -X- _ O
Carbonell1 -X- _ O
, -X- _ O
Ruslan -X- _ O
Salakhutdinov1 -X- _ O
, -X- _ O
Quoc -X- _ O
V. -X- _ O
Le2 -X- _ O
1 -X- _ O
Carnegie -X- _ O
Mellon -X- _ O
University -X- _ O
, -X- _ O
2 -X- _ O
Google -X- _ O
AI -X- _ O
Brain -X- _ O
Team -X- _ O
{ -X- _ O
zhiliny,dzihang,yiming,jgc,rsalakhu}@cs.cmu.edu -X- _ O
, -X- _ O
qvl@google.com -X- _ O

[ -X- _ O
cs -X- _ O
. -X- _ O
CL -X- _ O
] -X- _ O
2 -X- _ O
Jan -X- _ O
2020 -X- _ O
Zhilin -X- _ O
Yang∗1 -X- _ O
, -X- _ O
Zihang -X- _ O
Dai∗12 -X- _ O
, -X- _ O

