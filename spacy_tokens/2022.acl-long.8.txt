[ CASPI ] Causal - aware Safe Policy Improvement for Task - oriented Dialogue Govardana Sachithanandam Ramachandran , Kazuma Hashimoto∗ , Caiming Xiong
Salesforce Research
gramachandran@salesforce.com
hassy@logos.t.u-tokyo.ac.jp
cxiong@salesforce.com
Abstract efficiency is key for learning offline task - oriented dialogue system , as access to data are finite and expensive .
Recent advancements in off - policy reinforcement learning methods that uses offline data as against a simulator has proven to be sample efficient ( Thomas and Brunskill , 2016 ) .
The effective use of these techniques are hindered by the nature of ToD.
For instance , bias correction in off - policy based methods usually requires estimation of behaviour policy for a given state of Markov Decision Process ( MDP ) .
In ToD , per - turn annotated belief - state does not capture the true state of the MDP .
Example of such annotated belief - state are shown in Fig:1 .
Latent state information such as prosody , richness of natural language and among others induces stochasticity in the agents response .
In addition to these short comings , the direct use of automatic evaluation metric as reward for policy learning is not desirable , since these automatic evaluation metrics are often for the entire dialogue and not per turn .
Hence such rewards are sparse and under - specified ( Wang et al. , 2020 ) .
Use of under - specified reward will often lead to policy that suffers from high variance ( Agarwal et al. , 2019 ) .
Alternatively use of imitation learning based methods falls short of reasoning on the outcome .
This is demonstrated in Fig:1 . Turns#3
and # 2 are rich in semantic information and Turn#3 is key to success of the booking process .
While Turn#4 contributes least to successful outcome .
Though the turns have varying levels of importance , each of the turns are treated equally in imitation learning .
In worst case , turns like Turn#4 will appear more often than turns Turn#2 and # 3 in a ToD dataset , there by taking greater share of the gradient budget .
We address aforementioned shortcomings with following key contributions : 1.We introduce pairwise causal reward learning to learn fine grained per turn reward that reason the intention of human utterance .
2.We propose a safe policy improvement method The recent success of reinforcement learning ( RL ) in solving complex tasks is often attributed to its capacity to explore and exploit an environment .
Sample efficiency is usually not an issue for tasks with cheap simulators to sample data online .
On the other hand , Taskoriented Dialogues ( ToD ) are usually learnt from offline data collected using human demonstrations .
Collecting diverse demonstrations and annotating them is expensive .
Unfortunately , RL policy trained on off - policy data are prone to issues of bias and generalization , which are further exacerbated by stochasticity in human response and non - markovian nature of annotated belief state of a dialogue management system .
To this end , we propose a batch - RL framework for ToD policy learning : Causal - aware Safe Policy Improvement ( CASPI ) .
CASPI includes a mechanism to learn fine - grained reward that captures intention behind human response and also offers guarantee on dialogue policy ’s performance against a baseline .
We demonstrate the effectiveness of this framework on end - to - end dialogue task of the Multiwoz2.0 dataset .
The proposed method outperforms the current state of the art .
Further more we demonstrate sample efficiency , where our method trained only on 20 % of the data , are comparable to current state of the art method trained on 100 % data on two out of there evaluation metrics .
1 Introduction Offline task - oriented dialogue ( ToD ) systems involves solving disparate tasks of belief states tracking , dialogue policy management , and response generation .
Of these tasks , in this work we focus on dialogue policy management to improve the endto - end performance of ToD.
The need for sample ∗ Contributed to this work during his time at Salesforce Research Code : https://github.com/salesforce/CASPI 92 Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1 : Long Papers , pages 92 - 102 May 22 - 27 , 2022 c 2022 Association for Computational Linguistics  trary roll - outs .
This method requires a normalizing constant that integrates across rollouts , which is challenging .
Christiano et al. ( 2017 ) and Thananjeyan et al. ( 2020 ) propose to do relative comparison of two roll - outs there by eliminating the need for normalization constant
and they demonstrate in online setting .
Method 3.1 Preliminaries We model task - oriented dialogue as a Markov decision process ( MDP ) ( Sutton and Barto , 2018 ) with set of states S and actions A.
The agent at time step t with state st performs a composite action at as per a target policy πe ( at |st ) on the environment .
The environment is defined by transition probabilities P ( st+1 |st , at ) , a latent reward function , R(st , at , g ) , discount factor γ ∈
[ 0 , 1 ] and goal of dialogue g.
Then the objective of the target policy πe , is to maximizes the discounted sum of future reward on the MDP , given by the state - action function Qπe ( at , st ) =
PT value ′ Eat ∼πe , st ∼P [ t′ = t γ t−t R(st′ , at′ , g ) ] .
In offline Batch - RL .
The agent does not get to interact with the environment , instead we are provided with offline data D logged by human agents performing actions based on a latent stochastic behaviour policy πb .
Rollout of a dialogue τ i ∈ D is composed of τ i = ( ( oi0 , ai0 ) , ... , ( oiT −1 , aiT −1 ) ) .
Here ot is the observation at turn t , composing of ot = ( bt , uut , uat−1 ) , where bt is the belief state of the agent at turn t , uut and uat−1 are the user and agent utterance at time t and t − 1 respectively .
Figure 1 : A typical Task oriented dialogue conversation in MultiWoz2.0 dataset for task oriented dialogue setting that guarantees performance against a baseline .
By use of these two methods , we demonstrate performance and sample efficiency .
2 3 Related Works With the release of multi - domain , multi - turn MultiWoz2.0 dataset ( Budzianowski et al. , 2018a ) , there has been flurry of recent works , of which Zhang et al. ( 2019 ) uses data augmentation .
Rastogi et al. ( 2019 ) and Hosseini - Asl et al. ( 2020 ) frame dialogue policy learning as language modeling task .
Among the works that uses reinforcement learning .
Mehri et al. ( 2019 ) uses supervised learning to bootstrap followed by RL fine tuning , whereas Zhao et al. ( 2019 ) uses policy gradient on latent action space as against handcrafted ones .
Jaques et al. ( 2019 ) and Wang et al. ( 2020 ) uses Batch - RL for dialogue policy learning .
( Wang et al. , 2020 ) is first to argue the use of automated evaluation metrics directly as reward is under - specified for ToD policy learning .
Recently there ’s has been proliferation in use of large pretrained language model based systems like Hosseini - Asl et al. ( 2020 ) , Lin et al. ( 2020 ) , Chen et al. ( 2019 ) etc .
More details on contrasting the merits and limitations of these methods can be found in Sec : A.1 The line of inverse RL used in this work can be traced back to Ziebart et al. ( 2008 ) , which proposes roll - outs from expert demonstration should have rewards exponentially higher than any other arbi 3.2 Safe policy improvement Batch - RL entails training target policy πe on rollout generated by a latent behaviour policy πb .
Directly optimizing on the rollouts generated by policy other than the target policy , will lead to large bias in the value function estimation , poor generalization characteristic , and sample inefficiency ( Thomas and Brunskill , 2016 ) .
Safe policy improvement ensures the new policy performance is bounded by performance against a baseline policy .
This is expressed as : P r(V πe ≥ V πb − ζ ) ≥ 1 − δ , where V πe and V πb are value functions of the target and behaviour policy respectively .
Here 1 − δ and ζ are the high probability and approximation metaparameters respectively .
Schulman et al. ( 2015 ) 93  dence of the observation ot , ( beside bt ) the mode of the policy collapse to a near deterministic action .
To factor this into the policy learning , we have an additional loss : Ldet ( θ ) = min − E [ G(τ , t ) log πe ( at |ot ; θ ) ] ( ot , at ) ∼D ( 2 ) t′
−t R(s ′ , a ′ , g ) is t t t′ = t γ PT where return G(τ , t ) = the discounted sum of future reward for rollout τ with goal g.
Hence policy optimization loss function is given by : L(θ ) = αLsto ( θ ) + ( 1 − α)Ldet ( θ ) We achieve this by doing two forward passes of the policy network πe ( at |ot ; θ ) , first with only the belief state , bt as the input and second pass with entire observation i , e ot : = ( bt , uut , uat ) as input to the policy network .
We then use the corresponding action distribution πe ( at |bt ; θ ) and πe ( at |ot ; θ ) in loss functions ( 1 ) and ( 2 ) respectively .
Figure 2 : Shows stochacity i.e number of different dialogue act against each delexicalized belief state in MultiWoz2.0 dataset provide such update mechanism , ( 1 ) , whose errors are bounded as long as the constraints of ( 1 ) are met , where DKL ( .|| . ) is the KL divergence and η is a hyper - parameter .
3.3  πe ( at |st ; θ ) πbs Lsto ( θ ) = minπ −E Q ( st , at ) st ∼P bs πbs ( at |st ) at ∼πbs E st ∼P πbs
Pairwise causal reward learning Algorithm 1 CASPI Input : Dialogue dataset D and evaluation metric M ( . )
Sub - sample K - folds of train and val set { ( DT , DV ) 1 , ... , ( DT , DV ) k |(DT , DV ) ∼ D }  s.t .
( 3 ) [ DKL ( πbs ( .|st ) ||πe ( .|st ) ) ]
≤ η for ∀(DT , DV ) do Learn ToD in supervised setting by optimizing for objective : − min Eat , st ∼DT log(πm ( at |st ) ) for ∀ epoch do Using πm ( at |st ) predict actions on the valset DV and add it to the dataset , DP along with corresponding metric score M ( τ ) for pairwise causal reward learning DP = DP ∪ ( τ , M ( τ ) ) |τ
∼ πm end for end for repeat Sample pair of rollouts ( τ 1 , τ 2 ) ∼ DP Learn for R ( . )
by optimizing for objective ( 4 ) until Convergence using data DP repeat Optimize for policy πe using objective ( 3 ) until Convergence using data D ( 1 ) ( Schulman et al. , 2015 ) originally formulated ( 1 ) for online learning as trust region for policy updates and uses policy before gradient update as the baseline policy , πbs ( at |bt ; θold ) .
In this work we adapt it to offline setting and use behaviour policy πb as the baseline policy .
Use of this update rule requires access to the behavior policy πb ( at |st ) which is intractable to estimate and the learnt ones might have bias .
Use of such behavior policy to perform bias correction by Important Sampling ( Precup , 2000 ) might lead to worse policy .
Instead we estimate the behaviour policy conditioned only the annotated belief - state bt as against true state st in ( 1 ) , which result in a stochastic behavior policy .
This stochasticity of dialogue act vis - à - vis annotated belief state can observed in Fig:2 .
We also estimate the Q - function of the behavior policy , Qπb ( bt , at ) using learnt reward R(st , at , g ) .
More on learnt reward in Sec : 3.3 .
The belief state bt is part of the observation ot , hence we purport that , on availability of more evi The policy optimization objective introduced in the previous section requires access to per timestep reward R(st , at , g ) .
To this end , we provide a 94  Figure 3 : Process flow of pairwise causal reward learning mechanism to learn a reward that is causally reasoned on the intention of the human demonstrator .
Usually ToD are evaluated using dialogue level automatic evaluation metrics M ( . ) .
Given the large state - action space of the dialogue management system , these dialogue level feedback are under - specified for for effective policy learning ( Wang et al. , 2020 ) .
Details about the the choice of evaluation metric M ( . ) are covered in Sec:4.4.2 .
To address this under - specified feedback , we adapt preference learning introduced by ( Christiano et al. , 2017 ) from an online to an offline setting , to learn fine grained per dialogue turn ( ie . per timestep t ) reward , R(st , at , g ) .
Given a pair of rollouts τ 1 , τ 2 ∈ D with actions for each state in the rollout is sampled from a pair 1 and π 2 respectively .
Let of different policies πm m 1 2 τ ≻
τ represent preference of rollout τ 1 over rollout τ 2 .
This preference is true when sum of rewards of each PTdialogue turn of the two 1 rollouts satisfies : t=0 R(st , at , g|(st , at )
∈ τ ) >
PT 2
For brevity , t , at ) ∈
τ ) .
t=0 R(st , at , g|(sP T henceforth we refer t=0 R(st , at , g|(sT , at )
∈ τ ) as R(τ ) .
Then preferential probability of one rollout over an another , can be represented by : P
[ τ 1 ≻ τ 2 ] = where , µ(τ 1 ) =
We formalize this insight into a method depicted in Fig:3 and Algo:1 .
The ( train ) dataset is subsampled into K - fold train & val sets .
K - baseline policies are trained to fit the data distribution generated by experts using cross entropy loss , i.e supervised learning .
During the process of fitting the data distribution , the still learning K - policies are used to predict on their corresponding K - fold valset at every epoch of the training .
Each of these predictions are the scored by a chosen dialogue level metric , M ( . ) .
On convergence of this supervised learning process , pairs of dialogue predictions generated by the above process , along with their corresponding metric score are used to train for fine grained reward R(at , st , g ) using objective ( 4 ) .
ϕ(R(τ 1 ) )
ϕ(R(τ 1 ) )
+ ϕ(R(τ 2 ) )
E ( 5 )
We observe that the dialogue roll - outs are generated by expert latent policy .
The data ( dialogue rollouts ) are distributed as per the optimal latent policy and transition probability .
We propose that predictions made by a policy while in the process of learning to maximize the likelihood of the data is a good curriculum for exploring the state - action space for pairwise reward learning .
This is a key insight of this work .
Here ϕ ( . ) could either be exp ( . ) or identity 1 ( . ) .
In our experiments , the later works best .
We optimize for reward , R(st , at , g ) by minimizing binary cross - entropy loss between the preference probability and the normalized metrics score , µ(τ ) between a pair of rollout .
L(θ ) = min − M ( τ 1 ) M ( τ 1 ) +
M ( τ 2 )
The use of K - fold subsampling , K - baseline poli .
and actions sampled from these K - policies cies , πm that are still in the process of learning help generate counter factual examples in the action space .
These counter factual actions close to optimal policy , along with the goal of the dialogue helps us to learn subtle nuance of fine grained reward function R(at , st , g ) in the region of action space that matters the most .
[ µ(τ 1 ) log P
[ τ 1 ≻ τ 2 ] 1 , τ 2 ∼π 2 τ 1 ∼πm m + µ(τ 2 ) log P
[ τ 2 ≻ τ 1 ] ] ( 4 ) 95  4 Experimental Settings MinTL does n’t explicitly predict dialogue act .
Hence we only use the deterministic loss , Ldet directly on the generated response and for DST we retain the loss as is from MintTL ( Lin et al. , 2020 ) .
4.1 Model 4.1.1 CASPI ( . )
The learnt reward using CASPI R(st , at , g ) is akin to sample weights for each dialogue turn , that helps to redistribute the gradient budget among dialogue turns based of their contribution to the overall success of the ToD. θ : = θ − R(st , at , g)∇πblackbox ( at |st ;
θ ) 4.1.4 Pairwise Causal Learning Network For k - model training of pairwise casual reward learning illustrated in Fig:3
, we chose DAMD ( Zhang et al. , 2019 ) model for it ’s light weight model architecture .
In all our experiments , we use K = 10 .
For the pairwise casual reward learning network , we use three single bi - LSTM layers , one each to encode goal , belief state and either dialogue act or response sequences at each dialogue turn on each of the sampled roll - outs pairs , τ 1 and τ 2 .
The three encoded representations are concatenate and are fed through a couple of feed - forward layers before making a bounded reward prediction R(st , at , g ) ∈
[ 0 , 1 ] for each turn using a sigmoid function .
The per turn rewards are summed to form a global reward R(τ ) for the roll - out τ .
Using a pair of dialogue rewards R(τ 1 ) and R(τ 2 ) , we compute the probabilistic preference between the roll - outs P
[ τ 1 ≻ τ 2 ] either by standard normalization or a softmax function .
The output of this optimized using binary crossentopy loss described in Eqn:4 .
The above described architecture is illustrated in Fig:10 .
( 6 )
Hence we believe our pairwise casual reward learning and associated improvement in sample efficiency are independent of model architecture .
To this end we choose two ToD methods that are at the extremes of model architecture spectrum 1 ) One uses a light weight custom model and 2 ) Other uses a large standard pre - trained out - of - the box universal language model .
4.1.2 CASPI(DAMD )
In this setting , we use the neural model proposed by Zhang et al. ( 2019 ) .
DAMD is composed of three seq2seq generative model using GRUs .
The three seq2seq models are one each for belief state , dialogue act and response generation modules .
An attention layers is used to attend the outputs of the seq2seq models with the context vector of previous turn for copy over mechanism .
The outputs of these attention layer are used as representation for predicting series of tokens for their respective modules .
For more details on the model architecture and parameter setting refer Zhang et al. ( 2019 ) .
In this setting we use both stochastic , Lsto and deterministic , Ldet loss functions on dialogue act .
For DST and response generation , we retain the cross entropy loss as is from DAMD ( Zhang et al. , 2019 ) .
4.2 Dataset To evaluate our proposed method on Multi - domain Wizard - of - Oz ( MultiWoz ) ( Budzianowski et al. , 2018a ) dataset .
It is a large scale multidomain , task oriented dataset generated by human - to - human conversation , where one participant plays the role of a user while the other plays the agent .
The conversations are between a tourist and a clerk at an information center .
The conversations span across 7 domains including attraction , hospital , hotel , police , restaurant , taxi and train .
Each dialogue is generated by users with a defined goal which may cover 1 - 5 domains with a maximum of 13 turns in a conversation .
The dataset has 10438 dialogues split into 8438 dialogues for training set and 1000 dialogues each for validation and test set .
4.1.3 CASPI(MinTL )
On the other extreme of model complexity , we use the Task oriented Dialogue model , MinTL(Lin et al. , 2020 ) .
MinTL uses a large pretrained language model BART ( Lewis et al. , 2019 ) .
BART use as a standard encoder decoder transformer architecture with a bidirectional encoder and an autoregressive decoder .
It is pre - trained on the task of denoising corrupt documents .
BART is trained using cross - entropy loss between the decoder output and the original document .
For more details of the model architecture and parameter setting , we suggest referring to ( Lin et al. , 2020 ) ( Lewis et al. , 2019 ) .
4.3 Prepossessing We represent DB results as one - hot vectors as proposed by Budzianowski et al. ( 2018b ) .
To reduce surface - level variability in the responses , we use domain - adaptive delexicalization preprocess96  ing proposed in Wen et al. ( 2016 ) .
As proposed in Zhang et al. ( 2019 ) , We generate delexicalized responses with placeholders for specific values which can be filled with information in DST and database .
( Budzianowski et al. , 2018a ) .
The results are tabulated at Table:1 .
CASPI(DAMD )
with its light weight model architecture and no pretraining on any external corpus , except for ( Lubis et al. , 2020 ) , out perform all other previous methods , these includes methods that use large pretrained language models such as Hosseini - Asl et al. ( 2020 ) , Peng et al. ( 2020 ) and Lin et al. ( 2020 ) .
This show using CASPI to shepard the gradient update process as sample weights for each dialogue turn leads to a model that ’s well aligned with true objective of the task .
CASPI(MinTL ) with its robust pretrained model out performs CASPI(DAMD ) and LAVA ( Lubis et al. , 2020 ) by a large margin .
This demonstrates the ease of adaptation of existing methods with CASPI .
4.4 Metrics 4.4.1 Evaluation We evaluate performance of our method on end - to - end dialogue modeling task of Multiwoz2.0 ( Budzianowski et al. , 2018a ) .
We uses three evaluations metrics proposed by ( Budzianowski et al. , 2018a ) .
These include : 1 ) inform rate measures the fraction of dialogue , the system has provided the correct entity , 2 ) success rate - fraction of dialogues , the system has answered all the requested information and 3 ) BLEU ( Papineni et al. , 2002 ) - measures the fluency of the generated response .
We also report the combined score ( Inf orm + Success ) × 0.5 + BLEU proposed by Mehri et al. ( 2019 ) .
All the numbers of CASPI reported in this work are median of 5 runs with different seeds .
5.1 Inverse reinforcement learning , coupled with offpolicy policy learning and evaluation are proven to be sample efficient ( Thomas and Brunskill , 2016 ) .
We argue CASPI is competitive with other sample efficiency techniques , such as data augmentation and transfer learning as performed by Zhang et al. ( 2019 ) and Lin et al. ( 2020 ) respectively .
To demonstrate the hypothesis , we test our method against baseline in a low sample complexity regime .
For experimental setup , we adopt the low resource testing strategy from Lin et al. ( 2020 ) .
We train our model on 5 % , 10 % , and 20 % of the training data and compared with other baselines on end - to - end dialogue task , Table 2 list the results .
CASPI(MinTL ) trained only on 20 % of data was able to out perform previous state of the art method , LAVA ( Lubis et al. , 2020 ) and MINTL ( Lin et al. , 2020 ) trained on 100 % data on two of the three performance metrics .
This goes to show that having the right reward function to guide the budget of the gradient update process to reach the true objective is important in extremely low resource setting .
4.4.2 Training For the metric M used in pairwise causal reward learning , we use the following : M : =
Inf orm + Success + λ × BLEU ( 7 )
This is very similar to combined score used in evaluation and both are equivalent when λ = 2 .
We introduced hyperparamter λ to normalize the achievable scale of BLEU .
We observe that success rate , if used as is , will result in non - markovian and stochastic per turn reward function .
This is because the reward of current state will depend on the performance of future states .
Hence , we also use a soft version of the metric Msof t , where the success rate measures a fraction of requested information provided in a dialogue .
We refer the original metric that uses the discrete variant of success rate as Mhard .
The choice of action in reward function R(st , at , g ) can either be dialogue act or generate response , we refer corresponding variants of metrics as M ( act ) and M ( resp ) .
To demonstrate the versatility of our method to adapt to different metrics , we use all the discussed variants of the metric .
5 Sample Efficiency 5.2 Human Evaluation Automatic evaluation metrics have their own biases .
True objective of ToD is human experience while interacting with the dialogue systems , which automatic evaluation metrics might fall short to capture .
To this end we conduct human evaluation on the quality of the generated response .
We define quality by the following criterias : 1 ) Appropriateness : Are the generated responses appropriate for the given context in the dialogue turn ?
Result We compare both adaptation of our methods CASPI(DAMD ) and CASPI(MinTL ) on the endto - end dialogue tasks defined by MultiWoz2.0 97  Model DAMD DAMD + multi - action SimpleTOD SOLOIST MinTL - BART LAVA CASPI(DAMD ) , Msof t ( act ) CASPI(MinTL ) , Msof t ( act ) CASPI(MinTL ) , Mhard ( act ) Pre - trained model Inform % Success % BLEU
Combined Score
No 72.79 60.45 16.93 83.55 No 76.33 64.35 17.96 88.30 Yes 84.4 70.10 15.01 92.26 Yes 85.5 72.90 16.54 95.74
Yes 84.88 74.91 17.89 97.79 Yes 91.80 81.80 12.03 98.47 No 89.1 76.1 18.08 100.68
Yes 94.59 85.59 17.96 108.05 Yes 93.79 84.88 17.47 106.81 Table 1 : Comparison of results for end - to - end task of Multiwoz2.0 .
Model 5 % 10 % 20 % Inform Success BLEU
Inform
Success BLEU Inform Success BLEU MD - Sequicity DAMD MinTL CASPI(MinTL ) , Msof t ( resp ) CASPI(MinTL ) , Mhard ( resp ) 49.40
19.70 10.30 58.10 34.70 11.40 64.40 42.10 13.00 56.60 24.50 10.60 62.00 39.40 14.50 68.30 42.90 11.80 75.48 60.96 13.98 78.08 66.87 15.46 82.48 68.57 13.00 87.69 71.17 13.51 82.08 72.27 14.10 89.39 78.58 15.16 89.69 69.47 13.33 92.59 78.58 14.48 94.19 83.28 13.65 Table 2 : Comparison of results for end - to - end of Multiwoz2.0 .
in low resource setting 2 ) Fluency : Are the generated responses coherent and comprehensible ?
function .
In our analysis , low BLEU score is good indicator if the learnt policy indulges in reward hacking , which LAVA ( Lubis et al. , 2020 ) exhibits .
More on reward hacking in Sec:5.4.2 .
A dialogue turn in the test set is randomly picked .
The human evaluators were shown context leading up to the turn .
The predictions for the turn by different methods were anonymized and displayed to the evaluators .
This is illustrated in Fig:4 .
The human evaluators were asked to give a score between 1 and 5 for appropriateness and fluency , with score of 5 being best and 1 being the worst .
100 randomly selected dialogue turns were presented to 10 participants .We report the mean and variance of the score .
We compare our model performance against MinTL ( Lin et al. , 2020 ) , SimpleTOD ( HosseiniAsl et al. , 2020 ) , LAVA ( Lubis et al. , 2020 ) and DAMD ( Zhang et al. , 2019 ) .
Fig:5 shows the results of the evaluation .
CASPI(MinTL ) outperforms all other models in appropriateness score .
While fluency score of CASPI(MinTL ) , MinTL and SimpleTOD are comparable to each other .
It is worth noting that though LAVA ( Lubis et al. , 2020 ) performs well on automatic evaluation metrics , it performs poorly on human evaluation .
We suspect the policy learnt by ( Lubis et al. , 2020 ) exploits gaps in the reward function .
In case of LAVA ( Lubis et al. , 2020 ) , success rate is used as the reward 5.3 Human in the loop training In the previous section we argued that automatic dialogue evaluation metrics are biased and does n’t truly reflect the human objective , but in our method we use these very same dialogue evaluation metrics to learn reward R(st , at , g ) .
To bridge this gap , we performed the following human - in - theloop ( HITL ) experiment .
We first trained a pair CASPI(MINTL ) models with different seeds , on 5 % of Multiwoz2.0 dataset .
We then used these pair of models to predict on 0.5 % of Multiwoz2.0 train data ( 40 dialogues ) and had a human score these pairs of generated response relative to each other .
We then trained for reward R(st , at , g ) using pairwise causal reward learning as described in Sec:3.3 , where examples of the mini batch are randomly sampled either from human scored examples or the ones scored by the automatic evaluation metric as show in Fig:6 .
We then trained a fresh CASPI(MINTL ) model on the original 5 % of data and the learnt R(st , at , g ) .
We perform human evaluation on 24 dialogues using 3 participants .
Fig:7 98  Figure 4 : Example of generated responses by different ToD models Figure 6 : Mixed Human - in - the - loop and automatic evaluation metric scores for pairwise causal reward learning Figure 5 : Human evaluation rias :
Appropriateness and Fluency Figure 7 : Human evaluation of Human in the loop training of CASPI(MinTL ) on 5 % of Multiwoz2.0 dataset on crite action happens in this turn , which is crucial and has to be risk averse for the success of the dialogue .
Turn#2 gets the next best reward which captures crucial information needed for transaction to happen in Turn#3 .
Turn#4 gets reward an order lower than Turn#3 & 2 because other than nicety , it does n’t contribute much to the success of the conversation .
It should be noted that responses like Turn#4 will appear in almost all conversations and in supervised learning , these turns will be receiving the highest share of the gradient budget .
The learnt reward redistributes the gradient budget based on the turns contribution to the success of the dialogue objective .
shows the performance .
Though CASPI(MINTL ) using just 5 % of the data outperforms DAMD trained on 100 % of data in 2 out of the 3 automatic evaluation metrics shown in Table:1 and 2 , performs poorly in human appropriateness score .
With the HITL score in the reward learning , we see a boost in performance in both the human evaluation criteria : appropriateness and fluency .
The 5 % data CASPI(MINTL ) ’s human approriateness score is now comparable to 100 % data DAMD .
This goes to show the versatility of the pairwise causal reward learning .
With enough expressiveness of the neural network used , the pairwise causal reward learning can generalize to unknown dialogue evaluation criteria .
5.4.2 Type of agents In this section we analyze the type of behaviour CASPI agents sometime exhibit , especially when trained in low sample regime .
Greedy agent : In certain domains , the agents has a tendency to book a service before it has gathered all the required information or before the user requested or agreed for booking a service .
The first example in Fig:9 demonstrate this behaviour .
Here the user has requested for a taxi , before enough information such as destination or time of departure 5.4 Analysis 5.4.1 Rewards
In this section we qualitatively analyze the results of pairwise causal reward learning .
Fig:8 is the same conversation between a tourist and information center agents that we introduced earlier , now we have learnt reward R(st , at , g ) , against each turn .
We observe that Turn#3 has received the highest reward , retrospectively we realize the trans99  generic and can be extend to other NLP tasks .
References Rishabh Agarwal , Chen Liang , Dale Schuurmans , and Mohammad Norouzi . 2019 .
Learning to generalize from sparse and underspecified rewards .
arXiv preprint arXiv:1902.07198 .
Paweł Budzianowski , Tsung - Hsien Wen , Bo - Hsiang Tseng , Inigo Casanueva , Stefan Ultes , Osman Ramadan , and Milica Gašić. 2018a .
Multiwoz - a large - scale multi - domain wizard - of - oz dataset for task - oriented dialogue modelling .
arXiv preprint arXiv:1810.00278 .
Paweł Budzianowski , Tsung - Hsien Wen , Bo - Hsiang Tseng , Inigo Casanueva , Stefan Ultes , Osman Ramadan , and Milica Gašić. 2018b .
Multiwoz - a large - scale multi - domain wizard - of - oz dataset for task - oriented dialogue modelling .
arXiv preprint arXiv:1810.00278 .
Figure 8 : Example of learnt reward Wenhu Chen , Jianshu Chen , Pengda Qin , Xifeng Yan , and William Yang Wang .
2019 .
Semantically conditioned dialog response generation via hierarchical disentangled self - attention .
arXiv preprint arXiv:1905.12866 .
Figure 9 : Example of agent behaviour in low sample regime .
are gathered , the agent books the taxi .
This happens because there are gaps in automatic evaluation metrics .
A low BLEU score and relatively high inform and success rate might indicate greedy agent behaviour .
Other reasons for low BLEU score includes : lack of diversity in the responses or malformation of response .
Cautious agent : The agent tends to be cautious by providing long winded replies packed with more information than needed .
Agent tend to do this to prevent the risk of loosing rewards by missing out any requested information .
This behaviour is demonstrated in the second example in Fig:9
These subtle behaviour demonstrates gap in automatic evaluation metrics , which could be weeded out using Human in the loop learning described in Sec:5.3 .
6 Conclusion In this work we introduced a fine grained reward learning process using an under - specified metrics and expert demonstrations for efficiently learn task oriented dialogue .
We demonstrated the efficacy of our method on MultiWoz2.0 dataset with results comparable to the existing state of the art method with only 20 % of data .
We believe the methods is Paul F Christiano , Jan Leike , Tom Brown , Miljan Martic , Shane Legg , and Dario Amodei . 2017 .
Deep reinforcement learning from human preferences .
In Advances in Neural Information Processing Systems , pages 4299–4307 .
Ehsan Hosseini - Asl , Bryan McCann , Chien - Sheng Wu , Semih Yavuz , and Richard Socher .
2020 .
A simple language model for task - oriented dialogue .
arXiv preprint arXiv:2005.00796 .
Natasha Jaques , Asma Ghandeharioun , Judy Hanwen Shen , Craig Ferguson , Agata Lapedriza , Noah Jones , Shixiang Gu , and Rosalind Picard . 2019 .
Way off - policy batch deep reinforcement learning of implicit human preferences in dialog .
arXiv preprint arXiv:1907.00456 .
Mike Lewis , Yinhan Liu , Naman Goyal , Marjan Ghazvininejad , Abdelrahman Mohamed , Omer Levy , Ves Stoyanov , and Luke Zettlemoyer .
2019 .
Bart :
Denoising sequence - to - sequence pre - training for natural language generation , translation , and comprehension .
arXiv preprint arXiv:1910.13461 .
Zhaojiang Lin , Andrea Madotto , Genta Indra Winata , and Pascale Fung . 2020 .
Mintl :
Minimalist transfer learning for task - oriented dialogue systems .
arXiv preprint arXiv:2009.12005 .
Nurul Lubis , Christian Geishauser , Michael Heck , Hsien - chin Lin , Marco Moresi , Carel van Niekerk , and Milica Gašić. 2020 .
Lava : Latent action spaces via variational auto - encoding for dialogue policy optimization .
arXiv preprint arXiv:2011.09378 .
100  Shikib Mehri , Tejas Srinivasan , and Maxine Eskenazi . 2019 .
Structured fusion networks for dialog .
arXiv preprint arXiv:1907.10016 .
Kishore Papineni , Salim Roukos , Todd Ward , and WeiJing Zhu . 2002 .
Bleu : a method for automatic evaluation of machine translation .
In Proceedings of the 40th annual meeting of the Association for Computational Linguistics , pages 311–318 .
Baolin Peng , Chunyuan Li , Jinchao Li , Shahin Shayandeh , Lars Liden , and Jianfeng Gao .
2020 .
Soloist : Few - shot task - oriented dialog with a single pretrained auto - regressive model .
arXiv preprint arXiv:2005.05298 .
Doina Precup . 2000 .
Eligibility traces for off - policy policy evaluation .
Computer Science Department Faculty Publication Series , page 80 .
Abhinav Rastogi , Xiaoxue Zang , Srinivas Sunkara , Raghav Gupta , and Pranav Khaitan .
2019 .
Towards scalable multi - domain conversational agents : The schema - guided dialogue dataset .
arXiv preprint arXiv:1909.05855 .
John Schulman , Sergey Levine , Pieter Abbeel , Michael Jordan , and Philipp Moritz .
2015 .
Trust region policy optimization .
In International conference on machine learning , pages 1889–1897 .
Richard S Sutton and Andrew G Barto .
2018 .
Reinforcement learning : An introduction .
MIT press .
Brijen Thananjeyan , Ashwin Balakrishna , Ugo Rosolia , Felix Li , Rowan McAllister , Joseph E Gonzalez , Sergey Levine , Francesco Borrelli , and Ken Goldberg . 2020 .
Safety augmented value estimation from demonstrations ( saved ): Safe deep model - based rl for sparse cost robotic tasks .
IEEE Robotics and Automation Letters , 5(2):3612–3619 .
Philip Thomas and Emma Brunskill . 2016 .
Dataefficient off - policy policy evaluation for reinforcement learning .
In International Conference on Machine Learning , pages 2139–2148 .
Jianhong Wang , Yuan Zhang , Tae - Kyun Kim , and Yunjie Gu . 2020 .
Modelling hierarchical structure between dialogue policy and natural language generator with option framework for task - oriented dialogue system .
arXiv preprint arXiv:2006.06814 .
Tsung - Hsien Wen , David Vandyke , Nikola Mrksic , Milica Gasic , Lina M Rojas - Barahona , Pei - Hao Su , Stefan Ultes , and Steve Young .
2016 .
A network - based end - to - end trainable task - oriented dialogue system .
arXiv preprint arXiv:1604.04562 .
Yichi Zhang , Zhijian Ou , and Zhou Yu . 2019 .
Taskoriented dialog systems that consider multiple appropriate responses under the same context .
arXiv preprint arXiv:1911.10484 .
Tiancheng Zhao , Kaige Xie , and Maxine Eskenazi .
2019 .
Rethinking action spaces for reinforcement learning in end - to - end dialog agents with latent variable models .
arXiv preprint arXiv:1902.08858 .
Brian D Ziebart , Andrew L Maas , J Andrew Bagnell , and Anind K Dey . 2008 .
Maximum entropy inverse reinforcement learning .
A Appendix A.1 Baselines DAMD : Introduced by ( Zhang et al. , 2019)is a domain - aware multi - decoder network .
The method also exploits stochastic nature of the dialogue act by using a data - augmentation technique called the multi - action data augmentation .
DAMD with data augmentation is denoted here as DAMD + multiaction .
HDSA by ( Chen et al. , 2019 ) proposes to use hierarchical graph representation for dialogue act .
It uses a pre - trained 12 - layer BERT model ( Devlin et al. , 2019 ) to represent dialogue act .
The predicted dialogue act is transformed to the hierarchical graph structure using disentangled self - attention model , a 3 - layer self - attention model ( Vaswani et al. , 2017 ) SOLOIST ( Peng et al. , 2020 ) and SimpleTOD ( Hosseini - Asl et al. , 2020 ) uses pretrained GPT2 - based methods .
These method are trained on turn - level data without generated belief state and system act in dialog history .
MinTL - BART ( Lin et al. , 2020 ) , introduced Levenshtein belief spans framework that predicts only the incremental change in dialogue state per turn .
It leverages the pretrained T5 and BART ( Lewis et al. , 2019 ) as backbone for model architecture .
LAVA ( Lubis et al. , 2020 ) , reduces the action space of policy in end - to - end ToD , by using the latent space of a variational model with an informed prior .
The work use variable distribution : via pretraining , to obtain an informed prior , and uses autoencoding as the auxiliary task , to capture generative factors of dialogue responses .
HDNO proposed by ( Wang et al. , 2020 ) is a dialogue policy learning method to solve context - to - response generation task of Multiwoz2.0
( Budzianowski et al. , 2018a ) .
It exploits the hierarchical nature of dialogue act and response generation task by proposing an option based framework of Hierarchical RL and variational model to learn a latent dialogue act that corresponds to natural language response .
Unlike our 101  method , HDNO though highlights the risk of sparsity of metric function such as success rate as reward function , resorts to shaping a proxy reward function .
It uses markov language model as a proxy reward function .
The language model is learnt independent of the metric function .
Our method refrains from reward shaping and is independent of the nature of any underspecified metric function .
Since we learn fine grained turn specific credit assignment , our solution can adapt to other metric function as long as the pairwise reward network is rich enough to factorize them .
A.2
Pairwise causal reward learning network architecture Figure 10 :
Pairwise causal reward learning network architecture 102 
