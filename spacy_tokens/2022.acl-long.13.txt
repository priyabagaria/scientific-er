TopWORDS - Seg :
Simultaneous Text Segmentation and Word Discovery for Open - Domain Chinese Texts via Bayesian Inference Changzai Pan Center for Statistical Science & Department of Industrial Engineering , Tsinghua University pcz18@mails.tsinghua.edu.cn Maosong Sun Department of Computer Science and Technology & Guo Qiang Institute for Artificial Intelligence , Tsinghua University Ke Deng‚àó Center for Statistical Science & Department of Industrial Engineering , Tsinghua University kdeng@tsinghua.edu.cn sms@tsinghua.edu.cn
Abstract Processing open - domain Chinese texts has been a critical bottleneck in computational linguistics for decades , partially because text segmentation and word discovery often entangle with each other in this challenging scenario .
No existing methods yet can achieve effective text segmentation and word discovery simultaneously in open domain .
This study fills in this gap by proposing a novel method called TopWORDS - Seg based on Bayesian inference , which enjoys robust performance and transparent interpretation when no training corpus and domain vocabulary are available .
Advantages of TopWORDS - Seg are demonstrated by a series of experimental studies .
1 Introduction Due to absence of word boundaries in Chinese , Chinese natural language processing ( CNLP ) faces a few unique challenges , including text segmentation and word discovery .
When processing opendomain Chinese corpus containing many unregistered words and named entities , these challenges become more critical as they often entangle with each other : we usually can not segment Chinese texts correctly without knowing the underlying vocabulary ; on the other hand , it is often difficult to precisely discover unregistered words and named entities from open - domain corpus without guidance on text segmentation .
Most methods for CNLP in the literature assume that the underlying vocabulary is known and focus on improving performance of text segmentation in closed test .
The first category of methods along this research line are simple methods based on Word Matching ( Chen and Liu , 1992 ; Geutner , 1996 ; Chen , 2003 ; Shu et al. , 2017 ) , which segment a Chinese sentence by matching sub - strings in the sentence to a pre - given vocabulary in a forward or * Corresponding author .
reserve order .
The second category of methods utilize manually segmented corpus or large - scale pretraining corpus to train statistical models such as Maximum Entropy ( Berger et al. , 1996 ; McCallum et al. ; Low et al. , 2005 ) , HMM ( Sproat et al. , 1994 ; Zhang et al. , 2003 ) and CRF ( Lafferty et al. , 2001 ; Xue , 2003 ; Peng et al. , 2004 ; Luo et al. , 2019 ) , or deep learning models including CNN ( Wang and Xu ) , LSTM ( Chen et al. , 2015 ) , Bi - LSTM ( Ma et al. , 2018 ) and BERT ( Yang , 2019 ) , or hybrid models like Bi - LSTM - CRF ( Huang et al. , 2015 ) and LSTM - CNNs - CRF ( Ma and Hovy , 2016 ) , to achieve text segmentation directly or indirectly .
Methods of this category have led to popular toolkits for processing Chinese texts , including Jieba ( Sun , 2012 ) , StanfordNLP ( Manning et al. , 2014 ) , THULAC ( Sun et al. , 2016 ) , PKUSEG ( Luo et al. , 2019 ) , and LTP ( Che et al. , 2021 ) .
A popular strategy adopted by some of these toolkits is to segment the target texts into sequences of basic words first , and capture unregistered words and named entities , which are often word compounds consisting of basic words , later via chunking and syntactic analysis .
Although such a strategy can equip these toolkits with some ability on word discovery , it is apparently sub - optimal , because we may mis - segment basic words at the first place without realizing the existence of potential technical words , making it impossible to discover technical word compounds correctly in post analysis such as chunking and syntactic analysis .
On the other hand , unsupervised methods are also developed to achieve text segmentation when no pre - given vocabulary and manually segmented training corpus are available .
Some methods of this research line segment texts based on local statistics of the target texts , including Description Length Gain ( Kit and Wilks , 1999 ) , Mutual Information ( Chang and Lin , 2003 ) , Accessor Variety ( Feng et al. , 2004 ) , Evaluation - Selection - Adjustment Process ( Wang et al. , 2011 ) , and Normalized Variation 158 Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 1 : Long Papers , pages 158 - 169 May 22 - 27 , 2022 c 2022 Association for Computational Linguistics  of Branching Entropy ( Magistry and Sagot , 2012 ) .
The others , however , rely on generative statistical models whose parameters can be estimated from the target texts only , including Hierarchical Dirichlet Process ( Goldwater et al. , 2009 ) , Nested PitmanYor Process ( Mochihashi et al. , 2009 ) , Bayesian HMM ( Chen et al. , 2014 ) , TopWORDS ( Deng et al. , 2016 ) and GTS ( Yuan et al. , 2020 ) .
In general , methods based on word matching and unsupervised learning can not produce high - quality text segmentation ( Zhao and Kit , 2011 ) , although some unsupervised methods are successful on word discovery ( Deng et al. , 2016 ) .
Methods based on supervised learning can achieve excellent performance in closed test ( Emerson , 2005 ) , but often suffer from dramatic performance degradation when applied to open - domain Chinese corpus containing many unregistered words and named entities ( Liu and Zhang , 2012 ; Wang et al. , 2019 ) .
Methods based on deep learning are usually more robust under the ‚Äú pre - training and fine - tuning ‚Äù framework , but still suffer from unstable performance and often fail to correctly segment technical words , which play a key role in deciphering the meaning of domain - specific texts , when applied to opendomain texts ( Zhao et al. , 2018 ; Fu et al. , 2020 ) .
There are also some efforts in the literature to integrate supervised and unsupervised methods for improved performance ( Zhao and Kit , 2007 , 2008 , 2011 ; Wang et al. , 2019 ; Yang et al. , 2019 ) .
But , these methods either heavily depend on manually labelled corpus for model training , or suffer from unbalanced emphasis on text segmentation and word discovery , resulting in limited improvement for CNLP in open domain .
These facts make processing open - domain Chinese texts a critical bottleneck in computational linguistics even for today .
Many factors contribute to the stagnation on development of efficient tools for processing opendomain Chinese texts .
From the methodology point of view , we do not have a proper learning framework yet to connect the text segmentation problem to the word discovery problem and deal with them at the same time effectively .
From the practical point of view , the lack of proper evaluation criterion in open domain places a critical barrier for fair comparison of different methods and discourages researchers from looking for potential solutions .
This study tries to provide solutions to these critical issues .
First , we propose a novel Bayesian framework to integrate TopWORDS , an effective word discoverer ( Deng et al. , 2016 ) , and PKUSEG , a strong text segmenter , leading to a more efficient text segmenter called TopWORDS - Seg , which can achieve effective text segmentation and word discovery simultaneously in open domain .
Next , we design a cocktail strategy for method evaluation and comparison by measuring the overall performance of a target method on both text segmentation in benchmark corpus and technical word discovery and segmentation in open - domain corpus .
Experimental studies demonstrate that the proposed TopWORDS - Seg outperforms existing methods with a significant margin for CNLP in open domain .
2 TopWORDS - Seg Proposed by Deng et al. ( 2016 ) , TopWORDS is a general approach for offline natural language processing based on unsupervised statistical learning .
Assuming that sentences are generated by randomly sampling and concatenating words from an underlying word dictionary ( i.e. , unigram language model ) , TopWORDS starts with an over - complete initial word dictionary D containing all plausible word candidates in the target texts , and gradually simplifies the model by removing non - significant word candidates from D based on statistical model selection principles , with the unknown word usage frequencies estimated by EM algorithm ( Dempster et al. , 1977 ) .
TopWORDS is closely related to methods widely used in neural machine translation for constructing sub - word dictionary , and can be viewed as an advanced version of WordPiece ( Schuster and Nakajima , 2012 ) , Byte Pair Encoding ( Sennrich et al. , 2016 ) and Unigram Language Model ( Kudo , 2018 ) .
In practice , TopWORDS is particularly effective on discovering words , technical terms and phrases from open - domain Chinese texts , but tends to segment texts with coarser granularity at phrase instead of word level .
In this section , we upgrade TopWORDS from a weak text segmenter with strong ability on word discovery to a more powerful tool enjoying balanced ability on both dimensions via Bayesian inference .
2.1
The Bayesian Framework Following the setting in Deng et al. ( 2016 ) , let T = { T1 , ¬∑ ¬∑ ¬∑ , Tn } be a collection of unsegmented Chinese text sequences to process , A = { a1 , a2 , ¬∑ ¬∑ ¬∑ , aM } be the set of Chinese characters 159  involved in T , and DT be the underlying vocabulary behind T unknown to the investigator .
We aim to discover DT from T , and predict the invisible word boundary profile
Bj = ( bj1 , ¬∑ ¬∑ ¬∑ , bjLj ) for each piece of unsegmented Chinese text Tj = aj1 aj2 ¬∑ ¬∑ ¬∑ ajLj e , where bjl = 1 if there is a word boundary behind the l - th position of Tj and 0 otherwise , and e is a special end mark indicating the end of text sequence .
To learn DT , we starts with an over - complete initial word dictionary D = { w1 , w2 , . . .
, wN , e } covering all plausible word candidates in T ( i.e. , all sub - strings in T whose length ‚â§ œÑL and frequency ‚â• œÑF ) and the end mark e.
For simplicity , we always assume that DT ‚äÇ D and all characters in A are covered by D. Under the unigram language model , we have the following likelihood function for a piece of unsegmented text Tj given Bj and D : Y P(Tj |
D , Œ∏ , Bj ) = ( Œ∏w ) nw ( Bj ) , ( 1 ) Based on P(Œ∏ | T , D ) , model parameters Œ∏ can be estimated by the posterior mode , i.e , Œ∏ÃÇ = arg max P(Œ∏ | T , D ) .
Œ∏ Given Œ∏ÃÇ , we can further infer B according to P(B | T , D , Œ∏ÃÇ ) to achieve text segmentation .
2.2 Specification of Prior Distribution There are various ways to specify the prior distributions œÄ(Œ∏ , B ) .
In this study , we choose to use the independent conjugate prior below for conceptual and computational convenience : œÄ(Œ∏ , B ) = œÄ(Œ∏ ) ¬∑ œÄ(B ) , where œÄ(Œ∏ ) = Dirichlet(Œ∏ | Œ± ) , œÄ(B )
= P(T | D , Œ∏ , B ) = n Y ( Œ∏w ) nw ( B ) , ( 2 ) w‚ààD where nw ( B ) = n X œÄ(bjl ) , j=1 l=1 œÄ(bjl ) = Binary(bjl
| œÅjl ) , with Œ± = { Œ±w } w‚ààD and œÅ = { œÅjl } being the hyper - parameters controlling the strength of prior information .
In this study , we choose to specify ( 4 ) leading to a flat prior distribution for Œ∏ , but adopt a non - flat prior distribution for œÅ by smoothing the word boundary profiles B ‚àó = { Bj‚àó } 1‚â§j‚â§n predicted by a pre - given text segmenter S ‚àó : ( ( 1 ‚àí Œ∫ ) ¬∑ b‚àójl + Œ∫ ¬∑ œÅ , l < Lj , œÅjl = ( 5 ) 1 , l = Lj , j=1 = œÄ(Bj ) =
Lj n Y Y Œ±w = 1 , ‚àÄ w ‚àà D , P(Tj | D , Œ∏ , Bj ) Y n Y j=1 w‚ààD where Œ∏ = { Œ∏w } w‚ààD with Œ∏w being the usage frequency of word w in T , and nw ( Bj ) counts the number of occurrences of word w in the segmented version of Tj based on Bj .
Let B = { B1 , ¬∑ ¬∑ ¬∑ , Bn } being the word boundary profiles of the n text sequences in T .
We have ( 3 ) nw ( Bj ) .
j=1
In this study , we propose to specify a joint prior distribution œÄ(Œ∏ , B ) for ( Œ∏ , B ) to integrate prior preference on word usage and text segmentation into the learning procedure .
According to the Bayes Theorem , we have the following posterior distribution of ( Œ∏ , B ) given T and D : P(Œ∏ , B | T , D ) ‚àù œÄ(Œ∏ , B ) ¬∑ P(T | D , Œ∏ , B ) , which leads to the following marginal and conditional posterior distributions : Z P(Œ∏ | T , D ) = P(Œ∏ , B | T , D)dB , where b‚àójl is the location - specific binary segmentation indicator predicted by S ‚àó , Œ∫ ‚àà ( 0 , 1 ) is the smoothing parameter , and œÅ > 0 highlights the probability to place a word boundary at each location by a pseudo segmenter that places boundaries randomly in the text sequence .
Here , we set œÅ = 0.5 by default , and leave Œ∫ as a hyper - parameter that can be tuned to fit different application scenarios , leading to the following joint prior distribution : P(B | T , D , Œ∏ ) ‚àù P(Œ∏ , B | T , D ) .
œÄŒ∫ ( Œ∏ , B ) ‚àù
Lj n Y Y j=1 l=1 160 ( œÅjl ) bjl ( 1 ‚àí œÅjl ) 1‚àíbjl .
( 6 )  2.3 Word Discovery where Given the prior distribution œÄŒ∫ ( Œ∏ , B ) specified previously , the posterior distribution becomes : rwj P(Œ∏ , B | T , D ) ‚àù œÄŒ∫ ( Œ∏ , B ) ¬∑ P(T | D , Œ∏ , B ) "
‚àù n Y œÄŒ∫ ( Bj ) ¬∑ j=1 Y ( Œ∏w ) , ( 7 ) 2.4 ( œÅjl ) bjl ( 1 ‚àí œÅjl )
1‚àíbjl l=1 is a deterministic function of Œ∫ , as œÅjl ‚Äôs degenerate to constants for fixed Œ∫ based on ( 5 ) .
Under such a Bayesian model , the problem of word discovery can be naturally converted into a statistical model selection problem , as only word candidates whose usage frequency Œ∏w is significantly larger than 0 could be meaningful words .
We estimate Œ∏ by the posterior mode Œ∏ÃÇ as defined in ( 3 ) , which can by obtained via the EM algorithm ( Dempster et al. , 1977 ) with B as the missing data .
Details of the EM algorithm are described in Appendix A.
Once the EM algorithm gets converged , we can evaluate the statistical significance of a word candidate w by the likelihood - ratio statistics between the full model and a reduced model with w removed : !
P ( T | D , Œ∏ÃÇ ) œàw = log , ( 8) P(T | D , Œ∏ÃÇ[w=0 ] ) where Œ∏ÃÇ [ w=0 ] is the modification of Œ∏ÃÇ by setting Œ∏ÃÇw = 0 with other elements unchanged .
Apparently , a larger œàw suggests that word candidate w is more important for fitting the observed texts , and thus is more likely to be a meaningful word .
Because ‚àí2œàw ‚àº œá2 asymptotically under the null hypothesis that the reduced model with w removed is the true model , we can filter out word candidates whose œàw < œÑœà , where threshold œÑœà is the 2 ( 1 ‚àí 0.05 N ) -quantile of the œá distribution , following the Bonferroni correction principle for multiple hypothesis testing .
As demonstrated by Deng et al. ( 2016 ) , such a model selection strategy can effectively filter out most meaningless word candidates and results in a concise final dictionary containing meaningful words and phrases only .
Considering that œàw = ‚àí n X j=1 log ( 1 ‚àí rwj ) ,  with notation ‚Äú w ‚àº Bj ‚Äù meaning that word candidate w appears in the segmented version of Tj based on Bj , we can get œàw by calculating rwj for each Tj .
# nw ( Bj ) w‚ààD
Lj Y  Bj ‚ààBj where œÄŒ∫ ( Bj ) =
PŒ∫ w ‚àº
Bj | Tj , D , Œ∏ÃÇ ( 9 ) X =
I ( w ‚àº Bj ) ¬∑
PŒ∫ ( Bj | Tj , D , Œ∏ÃÇ ) , = Text Segmentation Given Œ∏ÃÇ , plausible text segmentation of Tj can be obtained by optimizing Bj according to PŒ∫ ( Bj | Tj , D , Œ∏ÃÇ ) , i.e. , segment Tj according to BÃÇj = max PŒ∫ ( B | Tj , D , Œ∏ÃÇ ) .
B‚ààBj ( 10 )
Alternatively , we can also calculate the posterior probability of existing a word boundary at position ( j , l ) as X Œ≥jl = bjl ¬∑ PŒ∫ ( Bj | Tj , D , Œ∏ÃÇ ) , ( 11 ) B‚ààBj and segment Tj based on BÃÉj = I(Œ≥ j ‚â• œÑS ) , ( 12 ) where Œ≥ j = ( Œ≥j1 , ¬∑ ¬∑ ¬∑ , Œ≥jLj ) and œÑS is a pre - given threshold with 0.5 as the default value .
Here , we choose to use the second segmentation strategy , because it leads to more robust results in practice .
2.5 TopWORDS - Seg Algorithm Integrating the dictionary initialization stage via sub - string enumeration , the prior construction stage guided by a pre - given segmenter S ‚àó ( i.e. , PKUSEG by default ) , the word discovery stage empowered by EM algorithm and likelihood - ratio tests , and the text segmentation stage based on conditional probability inference , into a united framework , we come up with the TopWORDS - Seg algorithm as demonstrated in Figure 1 .
Computation issues involved in the algorithm are detailed in Appendix B. A collection of hyper - parameters , including œÑL , œÑF , Œ∫ , œÅ and œÑS , are associated with the TopWORDS - Seg algorithm , and need be specified to initiate the algorithm .
We recommend to set œÑL = 15 , œÑF
= 2 and œÅ = œÑS = 0.5 by default .
The specification of hyper - parameter Œ∫ is a bit complicated .
To capture unregistered words from opendomain texts more efficiently , we would like to 161  Target text ùíØ ùúèùêø ùúèùêπ
Pregiven segmenter ùíÆ ‚àó Dictionary Initialization Stage Sub - string enumeration Initial dictionary ùíü Unigram language model Prior for word discovery œÄùúÖùëë ( ùúΩ , ùë© )
Bayesian framework ùëÉ ùúΩ , ùë© ùíØ , ùíü ‚àù œÄ
ùúΩ , ùë© ùëÉ(ùíØ|ùíü , ùúΩ , ùë© )
Parameter estimation ‡∑° Estimated parameters ùúΩ Final dictionary ùíüùêπ Model selection with ùúìùë§ Posterior distribution ùúÖùëë
Word boundary profiles ùë© Prior specification Prior distribution ùúã(ùúΩ )
Prior for text segmentation œÄŒ∫ùë† ( ùúΩ , ùë© ) ùúÖùë† Word Discovery Stage
Prior Specification Stage ùúèùëÜ Text segmentation
Segmented text ùë© Text Segmentation Stage Figure 1 : Flow chart of the TopWORDS - Seg choose a larger Œ∫ to encourage word discovery .
To segment regular texts more precisely , however , we would like to choose a smaller Œ∫ instead to better utilize the prior information .
To get rid of the dilemma , we allow to specify Œ∫ with different values in different tasks , i.e. , using a large Œ∫ ( referred to as Œ∫d ) in the word discovery stage and a small Œ∫ ( referred to as Œ∫s ) in the text segmentation stage .
Based on a wide range of experimental studies , we suggest to set Œ∫d = 0.5 and Œ∫s = 0.001 by default .
3 Experimental Study on Wikipedia web page and histograms for term length and appearance frequency of technical terms involved in TW -R .
In this section , we apply TopWORDS - Seg to process these Wikipedia corpora separately , and compare its performance to 6 existing methods , including Jieba ( Sun , 2012 ) , StanfordNLP ( Manning et al. , 2014 ) , THULAC ( Sun et al. , 2016 ) , PKUSEG ( Luo et al. , 2019 ) , LTP ( Che et al. , 2021 ) , and TopWORDS ( Deng et al. , 2016 ) itself , from various aspects .
3.1 Composed of over 10 billion Chinese character tokens from 3.6 million webpages , Chinese Wikipedia ( https://dumps.wikimedia.org/ ) is one of the largest open - source Chinese corpus .
Containing rich contents of various domains and millions of technical terms highlighted by hyperlinks , the Chinese Wikipedia is an ideal corpus for studying CNLP in open domain .
Considering that it ‚Äôs computationally expensive to processing all webpages in Chinese Wikipedia , we randomly picked up 1,500 webpages involving 8 million Chinese character tokens ( referred to as Chinese Wiki - Rand , or TW -R ) as the representative samples of the general texts in Chinese Wikipedia .
Moreover , we selected two collections of special webpages from Chinese Wikipedia with label ‚Äú Áîµ ÂΩ± " ( referred to as Chinese Wiki - Film , or TW -F ) or ‚Äú Áâ©ÁêÜ " ( referred to as Chinese Wiki - Physics , or TW -P ) , involving ‚àº5 million Chinese character tokens for each , as the representatives of the domain - specific texts in Chinese Wikipedia .
Figure 2 ( a ) and ( b ) demonstrates a typical Wikipedia Performance Evaluation Criteria Due to the lack of gold standard , it is not straightforward to evaluate and compare the performance of different methods on open - domain corpus like Chinese Wikipedia .
Here , we propose a cocktail strategy for method evaluation by measuring the overall performance of each method on both opendomain corpuora and benchmark corpus .
Let Vt be the collection of frequent technical terms in a particular Wikipedia corpus ( terms with hyperlinks appear at least 2 times ) , with nw be the number of occurrences for each w ‚àà Vt .
Suppose V is the discovered vocabulary reported by a particular method M , and mw is the number of successful catches of w by M. Taking advantage of the self - labelled technical terms with hyperlinks in Wikipedia webpages , it is straightforward to measure discovery recall Rd and segmentation recall Rs for technical terms in Vt as below : P mw |Vt ‚à© V | Rd = and Rs = Pw‚ààVt .
( 13 )
|Vt | w‚ààVt nw
Together , Rd and Rs reflect the ability of method 162  M to deal with technical terms in open - domain texts .
Because it is difficult to directly evaluate the perform of a method M on segmenting non - technical contents of the Wikipedia corpus , we retreat to indirect evaluation by evaluating its performance on segmenting the PKU corpus TP , a benchmark corpus with gold standard released by SIGHAN 2005 Bake - Off ( Emerson , 2005 ) , instead .
Let Fs be the F1 score of method M on text segmentation for the PKU corpus .
Score Fs reflects M ‚Äôs ability to process general Chinese texts without technical contents .
Apparently , Rd , Rs and Fs measure the strength of a method comprehensively from various aspects , with both word discovery and text segmentation considered for technical as well as non - technical texts .
Such a cocktail strategy provide us a principle to evaluate and compare the overall performance of different CNLP methods in open domains .
If a method enjoys high Rd , Rs and Fs values across different corpora stably , we would feel comfortable to claim it as a robust tools for CNLP in open domains .
3.2 Results Figure 2 ( c ) summarizes the performance of TopWORDS - Seg ( with the default setting ) and the 6 competing methods on the Wikipedia and PKU corpora in terms of Rd , Rs and Fs , with the size of discovered vocabulary |V | reported as well .
Comparing these results , we find that TopWORDSSeg enjoys robust performance on segmenting classic benchmark corpus ( Fs = 82.2 % for TP ) , open - domain corpus ( Rs = 76.5 % for TW -R ) and domain - specific corpus ( Rs = 76.8 % and 70.8 % for TW -F and TW -P respectively ) , and high efficiency on discovering technical terms ( Rd > 82 % for all three Wikipedia corpora ) .
The other methods , however , all suffer from either missing too many technical terms in the Wikipedia corpora ( Rd ranging from 45 % to 77 % as in supervised methods ) , or segmenting the PKU corpus poorly ( Fs = 50.4 % as in TopWORDS ) .
Considering that TopWORDS - Seg reports a vocabulary that is 16 K smaller than TopWORDS , it actually outperforms TopWORDS significantly in all dimensions .
Moreover , considering that both TopWORDS and TopWORDS - Seg tend to segment Chinese texts at coarser granularity with technical terms and phrases preserved as composite words instead of cutting them into smaller language units , the text segmentation standard adopted by the PKU corpus , which tends to segment Chinese texts at finer granularity , may over - punish them .
To ease the impact on performance evaluation due to segmentation granularity , we choose to mask part of the PKU corpus TP where method M is not consistent with the standard segmentation only on granularity ( with the concrete criteria detailed in Appendix C ) , and measure the F1 score of method M on the masked version of TP only , leading to a masked version of Fs referred to as Fm .
The proportion of masked corpus ( i.e. , mask rate ) is also calculated for each method and reported in Figure 2 ( c ) .
TopwORDSSeg achieves an improved Fm = 93.7 % with a mask rate of 16.6 % , suggesting that TopwORDSSeg actually segments the PKU corpus very well .
Meanwhile , a much higher mask rate of 50.4 % is obtained for TopWORDS , which is consistent to our impression that TopWORDS tends to preserve too many sub - phrases in text segmentation .
In addition , because some methods based on supervised learning , e.g. , Jieba , THULAC and PKUSEG , can receive external vocabulary for processing open - domain corpus , there exists an alternative strategy to integrate TopWORDS with thses methods by simply forwarding the vocabulary discovered by TopWORDS to them .
We refer to approaches based on this strategy as TopWORDSJieba / THULAC / PKUSEG , and report their performance on both Chinese Wikipedia corpus and PKU corpus in Figure 2 ( c ) as well .
Unfortunately , although this family of approaches achieve a higher Rd in general , they tend to report an over - large vocabulary and segment texts with coarser granularity like TopWORDS does .
These results indicate that simply concatenating TopWORDS to other methods does not necessarily lead to an improved approach , and thus imply that the proposed strategy based on Bayesian inference is not trivial .
The heatmaps in Figure 2 ( d ) demonstrate the similarity on text segmentation of different methods on four different target corpora , where the similarity between any two methods Mi and Mj is measured by P ( i ) ( j ) T ‚ààTD sum(BT ‚àß BT ) œÜij = P , ( i ) ( j ) T ‚ààTD sum(BT ‚à® BT ) ( i ) with BT denoting the predicted word boundary vector of text sequence T by method Mi .
From the figure , we can see clearly that text segmentation 163  ( b ) ( a ) ChineseWiki - Rand |V | Rd Rs Jieba 110k 60.2 % 72.6 % StanfordNLP 100 K 58.1 % 64.1 % THULAC 101 K 59.4 % 64.8 % PKUSEG 105k
56.9 % 63.8 % LTP 130k 76.4 % 67.2 % TopWORDS 165k 86.8 % 71.9 % TopWORDS - Seg 149 K 86.9 % 76.5 % TopWORDS - Jieba 201
K 91.4 % 72.8 % TopWORDS - THULAC 193 K 91.8 % 71.8 % TopWORDS - PKUSEG 214 K 90.0 % 69.5 % Method ChineseWiki - Film |V | Rd
Rs 67k 48.6 % 60.0 % 64k 47.7 % 55.3 % 60k 46.4 % 52.6 % 63k 46.2 % 53.7 % 78k 65.1 % 63.5 % 103k 82.5 % 72.7 % 92k 82.0 % 76.8 % 120k 85.1 % 73.1 % 116k 85.2 % 73.2 % 127k 85.0 % 71.4 % ( c ) ChineseWiki - Physics |V | Rd Rs 43k 47.0 % 59.9 % 43k 45.6 % 49.1 % 42k 47.1 % 49.2 % 43k 45.3 % 49.9 % 63k 72.4 % 59.3 % 92k 85.7 % 61.6 % 80k 85.1 % 70.8 % 104k 89.1 % 60.9 % 103k 89.5 % 61.5 % 117k 89.1 % 57.7 % PKU Fs Fm Mask Rate 81.2 % 98.6 % 22.4 % 85.8 % 93.9 % 11.4 % 92.4 % 95.6 % 4.5 % 95.4 % 99.5 % 5.5 % 88.7 % 99.8 % 14.7 % 50.4 % 85.8 % 50.4 % 82.2 % 93.7 % 16.6 % 50.9 % 95.8 % 55.0 % 54.9 % 98.4 % 52.6 % 44.5 % 77.2 % 51.3 % Chinese Wiki - Rand PKU Target text : Á¢≥ÁöÑÂêÑÁßçÂêåÁ¥†ÂºÇÂΩ¢‰ΩìÁöÑ Áâ©ÁêÜÁâπÊÄßÂ∑ÆÂºÇÊûÅÂ§ß (
The physical properties of various allotropes of carbon are extremely different )
Method Segmented text PKUSEG Á¢≥|ÁöÑ|ÂêÑÁßç|Âêå|Á¥†ÂºÇ|ÂΩ¢‰Ωì| ÁöÑ Áâ©ÁêÜ|ÁâπÊÄß|Â∑ÆÂºÇ|ÊûÅ|Â§ß TopWORDS Á¢≥ÁöÑ|ÂêÑÁßç|ÂêåÁ¥†ÂºÇÂΩ¢‰Ωì| ÁöÑ|Áâ©ÁêÜÁâπÊÄß|Â∑ÆÂºÇÊûÅÂ§ß TopWORDS - Seg Á¢≥|ÁöÑ|ÂêÑÁßç|ÂêåÁ¥†ÂºÇÂΩ¢‰Ωì| ÁöÑ|Áâ©ÁêÜ|ÁâπÊÄß|Â∑ÆÂºÇ|ÊûÅ|Â§ß Chinese Wiki - Physics Chinese Wiki - Film ( e ) ( d ) Figure 2 : Experimental study on PKU corpus and 3 Chinese Wikipedia corpora .
( a ) A typical web page in Chinese Wikipedia .
( b ) Key characteristics of technical terms involved in Chines Wikipedia .
( c ) Results on PKU , Chinese Wiki - Rand , Chinese Wiki - Film and Chinese Wiki - Physics datasets of different methods .
( d ) Similarity on text segmentation of different methods on four different target corpora .
( e ) Segmentation results on a typical sentence reported by TopWORDS - Seg is very similar to the results reported by supervised methods , but is significantly different from the result reported by TopWORDS for all four corpora .
Such results confirm the strength of TopWORDS - Seg on text segmentation in addition to word discovery , and provide strong evidences to support TopWORDSSeg as a powerful tool for processing open - domain Chinese texts .
Figure 2 ( e ) shows an illustrative example of text segmentation of PKUSEG , TopWORDS and TopWORDS - Seg for a piece of target text , respec tively .
Apparently , PKUSEG segments the target text almost perfectly except for chopping the technical term allotropes ( ÂêåÁ¥†ÂºÇÂΩ¢‰Ωì ) into three substrings by mistake , due to the lack of ability to recognize unregistered words .
TopWORDS , however , successfully recognizes and segments the technical term allotropes correctly , but segments the other part of the target text with coarser granularity leaving phrases like physical properties ( Áâ© ÁêÜÁâπÊÄß ) and extremely different ( Â∑ÆÂºÇÊûÅÂ§ß ) as unsegmented language units .
TopWORDS - Seg , as expected , segments the target text perfectly , with 164  Method |V | Rd Jieba StanfordNLP THULAC PKUSEG LTP TopWORDS TopWORDS - Seg 7.0 K 7.0 K 6.8 K 6.8 K 12.2 K 12.8 K 10.7 K 32.1 % 36.0 % 35.4 % 36.0 % 69.2 % 85.0 % 84.1 % ( a ) ( b ) ( c ) ( d ) ( e ) ( f ) Figure 3 : Real application on the full text of the Chinese version of Deep Learning .
( a ) Cover page of the book .
( b ) Performance on word discovery of different methods .
( c ) Similarity on text segmentation of different methods .
( d ) 100 most frequent words discovered by TopWORDS - Seg .
( e ) Technical terms captured by TopWORDS - Seg but missed by all supervised methods .
( f ) Typical pseudo words and phrases reported by TopWORDS but eliminated by TopWORDS - Seg .
the technical term allotropes correctly recognized and the rest part segmented with proper granularity .
4 Processing the Book of Deep Learning Written by Goodfellow et al. ( 2016 ) , the book Deep Learning has become a classic tutorial for deep learning .
In 2017 , its Chinese version was published in China ( see Figure 3 ( a ) for the book ‚Äôs cover ) , which is composed of more than 400,000 Chinese character tokens ( referred to as TD ) .
Covering rich technical contents in the domain of machine learning , including over 800 technical terms as listed in the Index Table at the end of the book , such a book is an ideal target for testing the performance of the proposed TopWORDS - Seg in real application .
Feeding full text of the book to TopWORDS - Seg and competing methods respectively , we obtained results as summarized in Figure 3 .
Figure 3 ( b ) shows that TopWORDS - Seg discovers 84.1 % technical terms listed in the Index Table of the book with a vocabulary of 10.7 K discovered words .
TopWORDS achieves a slightly higher Rd = 85.0 % at the price of a larger vocabulary with 12.8 K discovered words .
Other methods based on supervised learning result in much lower Rd with the vocabulary size varying between 6.8 K to 12.2K. Figure 3 ( d ) shows the most frequent words discovered by TopWORDS - Seg .
Figure 3 ( e ) displays part of the technical terms captured by TopWORDS - Seg but missed by all supervised methods , which are all meaningful technical terms like unsupervised learning ( Êó†ÁõëÁù£Â≠¶‰π† ) and stochastic gradient decent ( ÈöèÊú∫Ê¢ØÂ∫¶‰∏ãÈôç ) .
Figure 3 ( f ) summarizes typical pseudo words and phrases reported by TopWORDS but eliminated by TopWORDS - Seg , which are all common collocations widely used but usually not treated as words in Chinese , e.g. , in the model ( Ê®° Âûã‰∏≠ ) and it is because of ( ÊòØÂõ†‰∏∫ ) .
These results suggest that TopWORDS - Seg is indeed more effective than competing methods on word discovery .
In terms of text segmentation , the heatmap in Figure 3 ( c ) visualizes the similarity between TopWORDS - Seg and other approaches on this corpus in a similar fashion as in Figure 2 ( d ) .
Again , the performance of TopWORDS - Seg is very similar to the supervised methods , and demonstrates significant difference from TopWORDS , suggesting that TopWORDS - Seg is a robust tool with balanced ability on processing open - domain Chinese texts .
5 Conclusions and Discussions In this paper , we proposed TopWORDS - Seg , a powerful tool for processing open - domain Chi 165  nese texts based on Bayesian inference with balanced ability on text segmentation and word discovery .
A series of experimental studies confirm that TopWORDS - Seg can discover unregistered technical terms in open - domain texts effectively , and achieve high - quality text segmentation on both benchmark and open - domain corpora .
Taking advantage of the Bayesian framework , TopWORDSSeg is ready to process large scale open - domain Chinese texts without extra training corpus or pregiven domain vocabulary , leading to an ideal solution to a critical bottleneck existing in computational linguistics for decades .
Moreover , combing the strong points of PKUSEG and TopWORDS via Bayesian inference , TopWORDS - Seg enjoys transparent reasoning process , and is fully interpretable to most people .
In practical applications , such a property is very attractive to many researchers and practicers .
Meanwhile , TopWORDS - Seg also suffers from a few obvious limitations .
For example , although the current learning framework is effective to discover frequent words , it tends to miss many rare words that appear only a few times in the texts .
For another instance , because PKUSEG is more reliable on segmenting general texts , but less reliable on segmenting technical texts , in the ideal case we should adopt prior information provided by PKUSEG adaptively when processing texts of different types .
Unfortunately , TopWORDS - Seg does not take such a natural idea into consideration yet , and simply use the PKUSEG prior at the same intensity everywhere .
These deficiencies partially explain why TopWORDS - Seg still misses about 15 % technical terms in both experimental studies reported in this paper .
More research efforts are needed to fill in these gaps in future .
References Adam Berger , Stephen A Della Pietra , and Vincent J Della Pietra . 1996 .
A maximum entropy approach to natural language processing .
Computational linguistics , 22(1):39‚Äì71 .
Jason S. Chang and Tracy Lin . 2003 .
Unsupervised word segmentation without dictionary .
In ROCLING 2003 Poster Papers , pages 355‚Äì359 , Hsinchu , Taiwan .
The Association for Computational Linguistics and Chinese Language Processing ( ACLCLP ) .
Wanxiang Che , Yunlong Feng , Libo Qin , and Ting Liu . 2021 .
N - LTP : An open - source neural language technology platform for Chinese .
Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing : System Demonstrations , pages 42‚Äì49 .
Aitao Chen .
2003 .
Chinese word segmentation using minimal linguistic knowledge .
In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing , pages 148‚Äì151 , Sapporo , Japan .
Association for Computational Linguistics .
Keh - Jiann Chen and Shing - Huan Liu .
1992 .
Word identification for mandarin Chinese sentences .
In Proceedings of the 14th Conference on Computational Linguistics - Volume 1 , COLING ‚Äô 92 , page 101‚Äì107 , USA .
Association for Computational Linguistics .
Miaohong Chen , Baobao Chang , and Wenzhe Pei .
2014 .
A joint model for unsupervised Chinese word segmentation .
In Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 854‚Äì863 , Doha , Qatar .
Association for Computational Linguistics .
Xinchi Chen , Xipeng Qiu , Chenxi Zhu , Pengfei Liu , and Xuanjing Huang .
2015 .
Long short - term memory neural networks for Chinese word segmentation .
In Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing , pages 1197‚Äì1206 , Lisbon , Portugal .
Association for Computational Linguistics .
Acknowledgements
This research is partially supported by the National Scientific and Technological Innovation 2030 Major Project ( No : 2020AAA0106501 ) , the Guo Qiang Institute of Tsinghua University , the Beijing Natural Science Foundation ( Z190021 ) , and the Scientific - Technological Innovation Plan Program of Universities guided by the Ministry of Education of China .
Changzai Pan is supported by China Scholarship Council .
Arthur P Dempster , Nan M Laird , and Donald B Rubin . 1977 .
Maximum likelihood from incomplete data via the EM algorithm .
Journal of the Royal Statistical Society : Series B ( Methodological ) , 39(1):1‚Äì22 .
Ke Deng , Peter K. Bol , Kate J. Li , and Jun S. Liu . 2016 .
On the unsupervised analysis of domainspecific Chinese texts .
Proceedings of the National Academy of Sciences , 113(22):6154‚Äì6159 .
Thomas Emerson . 2005 .
The second international Chinese word segmentation bakeoff .
In Proceedings of the fourth SIGHAN workshop on Chinese language Processing . 166  Haodi Feng , Kang Chen , Xiaotie Deng , and Weimin Zheng . 2004 .
Accessor variety criteria for Chinese word extraction .
Computational Linguistics , 30(1):75‚Äì93 .
Jinlan Fu , Pengfei Liu , Qi Zhang , and Xuanjing Huang . 2020 .
RethinkCWS : Is Chinese word segmentation a solved task ?
In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 5676‚Äì5686 , Online .
Association for Computational Linguistics .
P. Geutner . 1996 .
Introducing linguistic constraints into statistical language modeling .
In Proceeding of Fourth International Conference on Spoken Language Processing . ICSLP ‚Äô 96 , volume 1 , pages 402 ‚Äì 405 .
Sharon Goldwater , Thomas L. Griffiths , and Mark Johnson . 2009 .
A Bayesian framework for word segmentation : Exploring the effects of context .
Cognition , 112(1):21‚Äì54 .
Ian Goodfellow , Yoshua Bengio , and Aaron Courville . 2016 .
Deep Learning .
MIT Press .
http://www . deeplearningbook.org . Zhiheng Huang , Wei Xu , and Kai Yu . 2015 .
Bidirectional LSTM - CRF models for sequence tagging .
arXiv preprint arXiv:1508.01991 .
Chunyu Kit and Yorick Wilks . 1999 .
Unsupervised learning of word boundary with description length gain .
In EACL 1999 : CoNLL-99 Computational Natural Language Learning .
Taku Kudo . 2018 .
Subword regularization : Improving neural network translation models with multiple subword candidates .
In Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 66‚Äì75 , Melbourne , Australia .
Association for Computational Linguistics .
John D. Lafferty , Andrew McCallum , and Fernando C. N. Pereira . 2001 .
Conditional random fields : Probabilistic models for segmenting and labeling sequence data .
In Proceedings of the Eighteenth International Conference on Machine Learning , ICML ‚Äô 01 , page 282‚Äì289 , San Francisco , CA , USA .
Morgan Kaufmann Publishers Inc.
Yang Liu and Yue Zhang . 2012 .
Unsupervised domain adaptation for joint segmentation and POS - tagging .
In Proceedings of COLING 2012 : Posters , pages 745‚Äì754 , Mumbai , India .
The COLING 2012 Organizing Committee .
Jin Kiat Low , Hwee Tou Ng , and Wenyuan Guo . 2005 .
A maximum entropy approach to Chinese word segmentation .
In Proceedings of the Fourth SIGHAN Workshop on Chinese Language Processing .
Ruixuan Luo , Jingjing Xu , Yi Zhang , Xuancheng Ren , and Xu Sun . 2019 .
PKUSEG :
A toolkit for multi - domain Chinese word segmentation .
CoRR , abs/1906.11455 .
Ji Ma , Kuzman Ganchev , and David Weiss . 2018 .
State - of - the - art Chinese word segmentation with BiLSTMs .
pages 4902‚Äì4908 .
Xuezhe Ma and Eduard Hovy . 2016 .
End - to - end sequence labeling via Bi - directional LSTM - CNNsCRF .
pages 1064‚Äì1074 .
Pierre Magistry and Beno√Æt Sagot . 2012 .
Unsupervized word segmentation : the case for mandarin Chinese .
In Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 383‚Äì387 , Jeju Island , Korea .
Association for Computational Linguistics .
Christopher Manning , Mihai Surdeanu , John Bauer , Jenny Finkel , Steven Bethard , and David McClosky . 2014 .
The Stanford CoreNLP natural language processing toolkit .
In Proceedings of 52nd Annual Meeting of the Association for Computational Linguistics : System Demonstrations , pages 55‚Äì60 , Baltimore , Maryland .
Association for Computational Linguistics .
Andrew McCallum , Dayne Freitag , and Fernando CN Pereira .
Maximum entropy markov models for information extraction and segmentation .
Daichi Mochihashi , Takeshi Yamada , and Naonori Ueda . 2009 .
Bayesian unsupervised word segmentation with nested Pitman - Yor language modeling .
In Proceedings of the Joint Conference of the 47th Annual Meeting of the ACL and the 4th International Joint Conference on Natural Language Processing of the AFNLP , pages 100‚Äì108 , Suntec , Singapore .
Association for Computational Linguistics .
Fuchun Peng , Fangfang Feng , and Andrew McCallum . 2004 .
Chinese segmentation and new word detection using conditional random fields .
In Proceedings of the 20th International Conference on Computational Linguistics , COLING ‚Äô 04 , page 562‚Äì568 , USA .
Association for Computational Linguistics .
Mike Schuster and Kaisuke Nakajima . 2012 .
Japanese and Korean voice search .
In 2012 IEEE International Conference on Acoustics , Speech and Signal Processing ( ICASSP ) , pages 5149‚Äì5152 .
Rico Sennrich , Barry Haddow , and Alexandra Birch . 2016 .
Neural machine translation of rare words with subword units .
In Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1715 ‚Äì 1725 , Berlin , Germany .
Association for Computational Linguistics .
Xinxin Shu , Junhui Wang , Xiaotong Shen , and Annie Qu .
2017 .
Word segmentation in Chinese language processing .
Statistics and Its Interface , 10(2):165 ‚Äì 173 .
R. Sproat , Chilin Shih , W. Gale , and N. Chang . 1994 .
A stochastic finite - state word - segmentation algorithm for Chinese .
Proceedings of the 32nd Annual Meeting on Association for Computational Linguistics , page 66‚Äì73 .
167  Junyi Sun . 2012 .
Jieba Chinese word segmentation tool .
Maosong Sun , Xinxiong Chen , Kaixu Zhang , Zhipeng Guo , and Zhiyuan Liu .
2016 .
THULAC :
An efficient lexical analyzer for Chinese .
Chunqi Wang and Bo Xu .
Convolutional neural network with word embeddings for Chinese word segmentation .
arXiv preprint arXiv:1711.04411 .
Hanshi Wang , Jian Zhu , Shiping Tang , and Xiaozhong Fan . 2011 .
A new unsupervised approach to word segmentation .
Computational Linguistics , 37(3):421‚Äì454 .
Xiaobin Wang , Deng Cai , Linlin Li , Guangwei Xu , Hai Zhao , and Luo Si . 2019 .
Unsupervised learning helps supervised neural word segmentation .
Proceedings of the AAAI Conference on Artificial Intelligence , 33(01):7200‚Äì7207 .
Lujun Zhao , Qi Zhang , Peng Wang , and Xiaoyu Liu .
2018 .
Neural networks incorporating unlabeled and partially - labeled data for cross - domain Chinese word segmentation .
In Proceedings of the 27th International Joint Conference on Artificial Intelligence , page 4602‚Äì4608 .
AAAI Press .
EM Algorithm for Estimating Œ∏ÃÇ A Given Œ∏ ( t ) , the current estimation of Œ∏ , the E - step computes the Q - function below : Q(Œ∏ , Œ∏ ( t ) )    = E log P(Œ∏ , B | T , D ) | T , D , Œ∏ ( t )  X = C+ log Œ∏w ¬∑ nw ( Œ∏ ( t ) ) , ( 14 ) w‚ààD where C is constant that does not change with Œ∏ , Nianwen Xue . 2003 .
Chinese word segmentation as character tagging .
International Journal of Computational Linguistics & Chinese Language Processing , 8(1):29‚Äì48 .
nw ( Œ∏ ( t ) ) = n X nw , j ( Œ∏ ( t ) ) , ( 15 ) j=1 nwj ( Œ∏ ( t ) ) =
Haiqin Yang .
2019 .
BERT meets Chinese word segmentation .
arXiv preprint arXiv:1909.09292 .
ArXiv : 1909.09292 .
= X E nw ( Bj ) | Tj , D , Œ∏(t )     nw ( Bj ) ¬∑
PŒ∫ Bj | Tj , D , Œ∏ ( t ) , ( 16 )
Bj ‚ààBj Yang Yang , Qi Li , Zhaoyang Liu , Fang Ye , and Ke Deng . 2019 .
Understanding traditional Chinese medicine via statistical learning of expert - specific electronic medical records .
Quantitative Biology , 7(3):210‚Äì232 .
PŒ∫ Bj | Tj , D , Œ∏(t ) P(Tj |
D , Œ∏(t ) , Bj ) ¬∑ œÄŒ∫(Bj ) , = P ( t ) B‚ààB P(Tj | D , Œ∏ , B ) ¬∑ œÄŒ∫ ( B )  Zheng Yuan , Yuanhao Liu , Qiuyang Yin , Boyao Li , Xiaobin Feng , Guoming Zhang , and Sheng Yu .
2020 .
Unsupervised multi - granular Chinese word segmentation and term discovery via graph partition .
Journal of Biomedical Informatics , 110:103542 . Hua - Ping Zhang , Hong - Kui Yu , De - Yi Xiong , and Qun Liu .
2003 .
HHMM - based Chinese lexical analyzer ICTCLAS .
In Proceedings of the Second SIGHAN Workshop on Chinese Language Processing , pages 184‚Äì187 , Sapporo , Japan .
Association for Computational Linguistics .
Hai Zhao and Chunyu Kit . 2007 .
Incorporating global information into supervised learning for Chinese word segmentation .
In 10th Conference of the Pacific Association for Computational Linguistics , pages 66‚Äì74 .
Hai Zhao and Chunyu Kit . 2008 .
Exploiting unlabeled text with different unsupervised segmentation criteria for Chinese word segmentation .
Research in Computing Science , 33:93‚Äì104 . 
( 17 ) j and Bj stands for the collection of all possible word boundary profiles of Tj .
The M - step updates Œ∏ ( t ) by maximizing Q(Œ∏ , Œ∏ ( t ) ) with respect to Œ∏ , leading to the updating function below : ( t+1 ) Œ∏w = P nw
( Œ∏ ( t ) ) ( t ) w‚ààD
nw ( Œ∏ ) , ‚àÄ w ‚àà D. ( 18 ) Along the updating procedure of the EM algorithm , word candidates with low estimated usage frequency ( e.g. , Œ∏ÃÇw < œÑŒ∏ = 10‚àí8 ) can be gradually removed from D to simplify the model .
When EM algorithm gets converged , we can get the estimation of posterior mode , Œ∏ÃÇ. B Computational Details
Considering that Hai Zhao and Chunyu Kit . 2011 .
Integrating unsupervised and supervised word segmentation : The role of goodness measures .
Information Sciences , 181(1):163‚Äì183 . 168 œàw = ‚àí n X j=1 log ( 1 ‚àí rwj ) ,  C where rwj PŒ∫ w ‚àº
Bj | Tj , D , Œ∏ÃÇ ( 19 ) X =
I ( w ‚àº Bj ) ¬∑
PŒ∫ ( Bj | Tj , D , Œ∏ÃÇ ) ,  =  Criteria for Masking PKU Corpus For a specific text sequence T = a1 ¬∑ ¬∑ ¬∑ aL ‚àà TW , let B ‚àó = ( b‚àó1 , ¬∑ ¬∑ ¬∑ , b‚àóL ) be the standard segmentation adopted by the PKU corpus , while B = ( b1 , ¬∑ ¬∑ ¬∑ , bL ) be its word boundary profile predicted by a segmentation method M.
For each sub - string S = ai1 ¬∑ ¬∑ ¬∑ ai2 of T , we say method M segments S with a coarser granularity with respect to B ‚àó ( denoted as S ‚àà GM , B ‚àó ) , if Bj ‚ààBj with notation ‚Äú w ‚àº Bj ‚Äù meaning that word candidate w appears in the segmented version of Tj based on Bj , we can get œàw by calculating rwj for each Tj .
Thus , to implement the TopWORDS - Seg algobi1 ‚àí1 =
b‚àói1 ‚àí1 = 1 = b‚àói2 =
bi2 , and rithm , we need to calculate nwj in ( 15 ) , rwj in ( 19 ) , BÃÇj in ( 10 ) or Œ≥jl in ( 12 ) for ‚àÄ Tj ‚àà T .
X X bl = 0 and b‚àól > 0 .
For a specific Tj = T = a1 ¬∑ ¬∑ ¬∑ aL , we define i1 < l < i2 i1 < l < i2 T[t : s ] = at ¬∑ ¬∑ ¬∑ as .
It can be showed that nwj , rwj and Œ≥jl , which are all functions of Tj , have the Masking all sub - string S ‚àà GM , B ‚àó , we obtain the formulation below : masked version of TW .
X h 1
nw ( T ) = p(T[<t ] ) ¬∑
p(T[>s ] ) p(T )
1‚â§t < s‚â§L i Y ¬∑ Œ∏w ¬∑ ( 1 ‚àí œÅl ) ¬∑
œÅs ¬∑ I(T[t : s ] = w ) , t‚â§l < s œÑL h X 1 rw ( T[>t ] ) ¬∑ I(T[1 : t ] 6= w ) + p(T ) t=1
i Y I(T[1 : t ] = w ) ¬∑ Œ∏T[‚â§t ] ¬∑ ( 1 ‚àí œÅl ) ¬∑ œÅt ¬∑ p(T[>t ] ) , rw ( T ) = 1 < l < t Œ≥l ( T ) = p(T[‚â§l ] ) ¬∑ p(T[>l ] ) , p(T )
where PŒ∫(T[t : s ] | D , Œ∏ ) X = P(T[t : s ] | B , D , Œ∏ ) ¬∑ œÄŒ∫(B ) , p(T[t : s ] ) = B‚ààB[t : s ] with B[t : s ] being the truncated version of B according to the position window [ t : s ] .
As p(T[<t ] ) and p(T[>t ] ) can be derived in linear time via dynamic programming based on the following recursion : h X p(T[<t ] ) = p(T[<t‚àís ] ) 1‚â§s‚â§min(t‚àí1,œÑL ) ¬∑ Œ∏T[t‚àís : t‚àí1 ] ¬∑ Y i ( 1 ‚àí œÅl ) ¬∑
œÅt‚àí1 , t‚àís‚â§l < t‚àí1 p(T[>t ] )
=
h X p(T[>t+s ] )
1‚â§s‚â§min(L‚àít , œÑL ) ¬∑ Œ∏T[t+1 : t+s ] ¬∑ Y i ( 1 ‚àí œÅl ) ¬∑ œÅt+s , t+1‚â§l < t+s all computation issues involved can be efficiently resolved .
169 
