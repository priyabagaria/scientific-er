Estimating the Entropy of Linguistic Distributions
Aryaman Arora
Clara Meister Ryan Cotterell
Abstract English German Mongolian Tagalog 1e1 Shannon entropy is often a quantity of interest to linguists studying the communicative capacity of human language .
However , entropy must typically be estimated from observed data because researchers do not have access to the underlying probability distribution that gives rise to these data .
While entropy estimation is a well - studied problem in other fields , there is not yet a comprehensive exploration of the efficacy of entropy estimators for use with linguistic data .
In this work , we fill this void , studying the empirical effectiveness of different entropy estimators for linguistic distributions .
In a replication of two recent information - theoretic linguistic studies , we find evidence that the reported effect size is over - estimated due to overreliance on poor entropy estimators .
Finally , we end our paper with concrete recommendations for entropy estimation depending on distribution type and data availability .
1 Dutch 1e2 1e3 1e4 1e5 1e6 Georgetown University ETH Zürich aa2190@georgetown.edu { clara.meister,ryan.cotterell}@inf.ethz.ch MSE ( nats2 )
1e-2 1e-5 1e1
Estimator 1e-2 1e-5
CS MM HT NSB JACK WW 1e2 1e3 1e4 1e5
1e6 1e2 1e3 1e4 1e5 1e6 MLE Samples Figure 1 : A comparison of several estimators of the entropy of the unigram distribution across 5 languages .
Minima in all the graphs indicate sign changes in the error of the estimate , from an under- to an over - estimate .
Introduction There is a natural connection between information theory , the mathematical study of communication systems , and linguistics , the study of human language — the primary vehicle that humans employ to communicate .
Researchers have exploited this connection since information theory ’s inception ( Shannon , 1951 ; Cherry et al. , 1953 ; Harris , 1991 ) .
With the advent of modern computing , the number of information - theoretic linguistic studies has risen , exploring claims about language such as the optimality of the lexicon ( Piantadosi et al. , 2011 ; Pimentel et al. , 2021 ) , the complexity of morphological systems ( Cotterell et al. , 2019 ; Wu et al. , 2019 ; Rathi et al. , 2021 ) , and the correlation between surprisal and language processing time ( Smith and Levy , 2013 ; Bentz et al. , 2017 ; Goodkind and Bicknell , 2018 ; Cotterell et al. , 2018 ; Meister et al. , 2021 , inter alia ) .
In information - theoretic linguistics , a fundamental quantity of research interest is entropy .
Entropy is both useful to linguists in its own right , and is necessary for estimating other useful quantities , e.g. , mutual information .
However , the estimation of entropy from raw data can be quite challenging ( Paninski , 2003 ; Nowozin , 2015 ) , e.g. , in expectation , the plug - in estimator underestimates entropy ( Miller , 1955 ) .
Linguistic distributions often present additional challenges .
For instance , many linguistic distributions , such as the unigram distribution , follow a power law ( Zipf , 1935 ;
Mitzenmacher , 2004).1 Linguistics is not the only field with such nuances , and so a large number of entropy estimators have been proposed in other fields ( Chao and Shen , 2003 ; Archer et al. , 2014 , inter alia ) .
However , no work to date has attempted a practical comparison of these estimators on natural language data .
This work fills this empirical void .
Our paper offers a large empirical comparison of the performance of 6 different entropy estimators 1
As Nemenman et al. ( 2002 ) highlight , when estimating the entropy of a distribution that follows a power law , it is often possible to get an effectively meaningless estimate that is completely determined by the estimator ’s hyperparameters .
175 Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics Volume 2 : Short Papers , pages 175 - 195 May 22 - 27 , 2022 c 2022 Association for Computational Linguistics  on both synthetic and natural language data , an example of which is shown in Figure 1 .
We find that Chao and Shen ’s ( 2003 ) is the best estimator when very few data are available , but Nemenman et al. ’s ( 2002 ) is superior as more data become available .
Both are significantly better ( in terms of meansquared error ) than the naïve plug - in estimator .
Importantly , we also show that two recent studies ( Williams et al. , 2021 ; McCarthy et al. , 2020 ) show smaller effect sizes when a better estimator is employed ; however , we are able to reproduce a significant effect in both replications .
We recommend that future studies carefully consider their choice of entropy estimators , taking into account data availability and the nature of the underlying distribution.2 2 Entropy and Language Shannon entropy is a quantification of the uncertainty in a random variable .
Given a ( discrete ) random variable X with probability distribution p over K possible outcomes X = { xk } K k=1 , the Shannon entropy of X is defined as def H(X ) = H(p ) = − K X p(xk ) log p(xk )
( 1 ) k=1
Entropy has many uses throughout science and engineering ; for instance , Shannon ( 1948 ) originally proposed entropy as a lower bound on the compressibility of a stochastic source .
Yet the application of information - theoretic techniques to linguistics is not so straightforward : Information - theoretic measures are defined over probability distributions and , in the study of natural language , we typically only have access to samples from the distribution of interest , e.g. , the phonotactic distribution in English , which permits word we can not find in a corpus , like blick , rather than the true probabilities required in the computation of Eq .
( 1 ) .
Indeed , it is often the case that not all elements of X are even observed in available data — such as words that were coined after the a corpus was collected .
Rather , p must be approximated in order to estimate H(p ) .
One solution is plug - in estimation : Given samples from p , the maximum - likelihood estimate for p is “ plugged ” into Eq .
( 1 ) .
However , as originally noted by Miller ( 1955 ) , this strategy generally yields poor estimates.3 It is thus necessary 2 Our code is available at https://github.com/ aryamanarora / entropy - estimation .
3 A proof of this result in given in full in Proposition 1 . to derive more nuanced estimators .
3 Statistical Estimation Theory Statistical estimation theory provides us with the tools for estimating various quantities of interest based on samples from a distribution .
Central to this theory is the estimator : A statistic that approximates a property of the distribution our data is drawn from .
More formally , let D = { e x(n ) } N n=1 be samples from an unknown distribution
p.
Suppose we are interested in a quantity θ that can be computed as a function of the distribub tion p.
An estimator θ(D ) for θ is then a function of the data D that provides an approximation of θ .
Two properties of an estimator are often of interest : bias — the difference between the true value of θ and the expected value of our estimator b b under p — and variance — how much θ(D ) θ(D ) fluctuates from sample set to sample set : def b b bias(θ(D ) ) =
Ep [ θ(D ) ]
−θ def ( 2 ) 2 b b b var(θ(D ) ) =
Ep [ ( θ(D ) −
Ep [ θ(D ) ] ) ]
( 3 ) It is desirable to construct an estimator that has both low bias and low variance .
However , the bias – variance trade - off tells us that we often have to pick one , and we should focus on a balance between the two .
This trade - off is evinced through mean - squared error ( MSE ) , a metric oft - employed for assessing estimator quality : 2 b b b MSE(θ(D ) )
= bias(θ(D ) )
+ var(θ(D ) )
( 4 ) To recognize the trade - oft note that , for any fixed MSE , a decrease in bias must be compensated with an increase in variance and vice versa .
Indeed , it is important to recognize that there is typically no single estimator that is seen as “ best . ”
Different estimators balance the bias – variance trade - off differently , making their perceived quality specific to one ’s use - case .
Importantly , the effectiveness of an estimator also depends on the domain of interest .
Consequently , an empirical study of various entropy estimators , which this paper provides , is necessary in order to determine which entropy estimators are best suited for linguistic distributions .
3.1 Plug - in Estimation of Entropy A simple , two - step approach for estimating entropy is plug - in estimation .
In the first step , we compute the maximum - likelihood estimate for p from our 176  dataset D as follows def pbMLE ( xk ) =
PN n=1 1{ex(n )
= xk } N ( 5 )
In the second step , we plug Eq . ( 5 ) into Eq . ( 1 ) b MLE ( D ) .
directly , which results in the estimator H
So why is this a bad idea ?
While our probability estimates themselves are unbiased , entropy is a concave function .
Consequently , by Jensen ’s inequality , this estimator is , in expectation , a lower bound on the true entropy ( see App . E.1 for proof ) .
Moreover , when N ≪ K , which is often the case in power - law distributed data , the estimate becomes quite unreliable ( Nemenman et al. , 2002 ) .
3.2 An Ensemble of Entropy Estimators MM — Miller ( 1955 ) and Madow ( 1948 ) .
The first innovation in entropy estimation known to the authors is a simple fix derived from a first - order Taylor expansion of MLE ( described above ) .
The Miller – Madow estimator only involves a simple additive correction , which is shown below : def b MM ( D ) = b MLE ( D ) + K − 1 H H 2N def b HT ( D ) = H − ( 6 ) JACK — Zahl ( 1977 ) .
Next we consider the jackknife , which is a common strategy used to correct for the bias of statistical estimators .
In the case of entropy estimation , we can apply the jackknife out of the box to correct the bias inherent in the MLE estimator .
Explicitly , this is done by averagb MLE ( D ) albeit with ing plug - in entropy estimates H th the n sample from the data removed ; we denote b \n this held - out plug - in estimator as H MLE ( D ) .
Averaging these “ held - out ” plug - in estimators results in the following simple entropy estimator def b JACK ( D ) = b MLE ( D ) − H NH N K X pbMLE ( xk ) log pbMLE ( xk ) k=1 1 − ( 1 − pbMLE ( xk ) ) N ( 8) using our MLE probability estimates pbMLE ( xk ) .
where K is size of the support of X .
The Miller – Madow correction should seem intuitive in that we add K−1 2N ≥ 0 to compensate for the negative bias of the estimator .
A full derivation of the Miller – Madow estimator is given in Proposition 2 . N N −1X more efficiently estimate a function of a random variable .
Importantly , this estimator gives us the ability to compensate for situations where the probability of an outcome is so low that it is often not observed in a sample , which is often the case for e.g. , power - law distributions .
While a full exposition of HT estimators is outside of the scope of this work , in essence , we can divide the expected probability of a class by each class ’s estimated inclusion probability to compensate for such situations .
Given the true probability of an outcome p(xk ) , the probability that it occurs at least once in a sample of size N is 1 − ( 1 − p(xk ) )
N .
The HT estimator for entropy is then defined as b \n H MLE ( D ) n=1 ( 7 ) Note that the jackknife is applicable to any estimab MLE ( D ) , and , thus , can be combined tor , not just H with any of the other approaches mentioned .
HT — Horvitz and Thompson ( 1952 ) .
Horvitz – Thompson is a general scheme for building estimators that employs importance weighting in order to CS — Chao and Shen ( 2003 ) .
Chao – Shen modifies HT by multiplying the MLE probability estimates by an estimate of sample coverage .
Formally , let f1 be the number of observed singletons4 in sample ; our sample coverage can be estimated as b = 1 − f1 .
The CS estimator is then computed as : C N def b CS ( D ) =
H − K b X b · pbMLE ( xk ) C · pbMLE ( xk ) log C k=1 b · pbMLE ( xk ) ) N 1 − ( 1 − C ( 9 )
In the case that f1 = N , we set f1 = N − 1 to ensure the estimated entropy is not 0 .
WW — Wolpert and Wolf ( 1995 ) .
One family of entropy estimators in information theory is based on Bayesian principles .
The first of these was the Wolpert – Wolf estimator , which uses a Dirichlet prior ( with concentration parameter α and a uniform base distribution ) .
This Bayesian estimator has a clean , closed form : K   X α ek def b WW ( D | α ) = e+1 − H ψ
A ψ(e αk + 1 ) e
A k=1 ( 10 ) where α ek = c(xk ) + αk ( for the histogram count c(xk ) of class k in the sample ; this is analogous to e =
PK α Laplace smoothing ) , A k=1 ek , and ψ is the digamma function .
A full derivation of Eq . ( 10 ) is given in Proposition 3 .
Unfortunately , Eq . ( 10 ) is 4 A singleton ( hapax legomenon ) is an outcome which is observed only once in the sample .
177  102 MAB 103 104 105 102 MSE 103 104 105 English German Dutch HT HT HT HT HT HT NSB NSB NSB NSB CS CS HT HT HT HT HT HT NSB NSB NSB NSB CS CS Mongolian Tagalog NSB HT HT HT NSB NSB NSB NSB NSB HT HT HT NSB NSB NSB NSB Table 1 : The best unigram entropy estimators on the corpora studied , tested on various N averaged over 100 samples .
All differences are statistically significant on the permutation test ; lighter color indicates fewer statistically significant comparisons on the Tukey test .
Scale : significantly better than 6 5 4 3 2 1 0 other estimators .
very dependent on the choice of α : For large K , α almost completely determines the final entropy estimate , an observation first made by Nemenman et al. ( 2002 ) which motivated their improved estimator described below .
NSB — Nemenman et al. ( 2002 ) .
Nemenman et al. ( NSB ) attempt to alleviate the Wolpert – Wolf estimator ’s dependence on α .
They take α = α · 1 , enforcing that the Dirichlet prior is symmetric , and develop a hyperprior over α that results in a nearuniform distribution over entropy .
The hyperprior is given by def pNSB ( α ) = Kψ1 ( Kα + 1 ) −
ψ1 ( α + 1 ) log K ( 11 ) where ψ1 is the trigamma function .
A full derivation of Eq . ( 11 ) is given in Proposition 4 .
This choice of hyperprior mitigates the effect that the chosen α has on the entropy estimate .
Nemenman et al. ’s ( 2002 ) entropy estimator is then the posterior mean of the Wolpert – Wolft estimator taken under pNSB : Z ∞ b b WW ( D | α · 1 ) pNSB
( α ) dα HNSB ( D ) = H 0 ( 12 ) Typically , numerical integration is used to quickly compute the unidimensional integral .
4 Experiments Here we provide an evaluation of the entropy estimators presented in § 3.2 on linguistic data .
4.1 Entropy of the Unigram Distribution
We start our study with a controlled experiment where we estimate the entropy of the truncated unigram distribution , the ( finite ) distribution over the frequent word tokens in a language without regard to context ( Baayen et al. , 2016 ; Diessel , 2017 ; Divjak , 2019 ; Nikkarinen et al. , 2021 ) .
We renormalize the frequency counts of corpora in English , German , and Dutch ( taken from CELEX ; Baayen et al. , 1995 ) , as well as Mongolian and Tagalog ( from Wikipedia5 ) .
We take this renormalization as a gold standard distribution , since we can not access the underlying unigram distribution .
We then draw samples of varying sizes ( N ∈ { 102 , 103 , 104 , 105 } ) from the distribution of renormalized frequency counts to test the estimators ’ ability to recover the underlying distributions ’ entropy .
While the renormalized frequency counts are not necessarily representative of the true unigram distribution , they nevertheless provide us with a controlled setting to benchmark various entropy estimators .
We evaluate the estimators on both bias and MSE , as defined in ( 2 ) and ( 4 ) , as well as mean absolute bias ( MAB ) .
To test the statistical significance of differences in metrics between entropy estimators , we use paired permutation tests ( Good , 2000 ) ( sampling 1 , 000 permutations ) between pairs of estimators , checking MAB and MSE .
We run Tukey ’s test ( 1949 ) to judge the statistical significance of differences in MAB and MSE between all pairs of estimators , which found only a few insignificant comparisons when N was large .
Results are shown in Table 1 and Figure 1 .
We find that NSB ( followed closely by CS ) converges almost to the true entropy from below using with only a few samples .
HT is the best estimator for N < 2 , 000 , but as N increases it tends to overestimate entropy to the point where its bias is greater than that of MLE .
Besides HT , all estimators at all tested sample sizes N have lower MAB and MSE than MLE . 4.2 Replication of Williams et al. ( 2021 )
Next , we turn to a replication of Williams et al. ’s ( 2021 ) information - theoretic study on the associa5
We used dumps from November 1 , 2021 : Mongolian and Tagalog ; the extracted counts are available in our repository .
178  Language n MLE CS MM JACK WW NSB Italian Polish Portuguese Spanish 16 , 856 15 , 525 7 , 409 21 , 408 20.00 % 30.52 % 27.60 % 20.50 % 15.56 % 23.48 % 20.76 % 15.17 % 16.43 % 25.49 % 22.51 % 16.44 % 14.09 % 21.75 % 18.81 % 13.80 % 19.67 % 34.68 % 33.32 % 21.04 % 11.41 % 17.07 % 14.18 % 10.50 % Arabic Croatian Greek 2 , 483 13 , 856 3 , 305 45.31 % 31.35 % 41.58 % 38.49 % 26.04 % 33.17 % 40.99 % 26.62 % 36.39 % 37.93 % 23.08 % 32.32 % 49.09 % 35.66 % 48.80 % 34.82 % 19.06 % 27.00 % Table 2 : Normalized mutual information , calculated with several estimators , between adjectives and the inanimate nouns they modify based on UD corpora .
Colored - in cell means statistically significant NMI value .
tion between gendered inanimate nouns and their modifying adjectives .
They estimate mutual information by using its familiar decomposition as the difference of two entropies : MI(X ; Y ) = H(X ) − H(X | Y ) .
The entropies H(X ) and H(X | Y ) are estimated independently and then their difference is computed .
We replicate Williams et al. ’s ( 2021 ) experiments using gold - parsed Universal Dependencies corpora , filtering out animate nouns with Multilingual WordNet ( Bond and Foster , 2013 ) .
We rerun their experimental set - up using our full suite of entropy estimators to determine whether the relationship they posit remains significant , checking 3 more languages not in the original study .
We report results for normalized mutual information ( dividing MI by maximum possible MI ) in Table 2 .
We find that using NSB ( the estimator we found most effective in § 4.1 ) instead of MLE , nearly halves the measured effect in all languages .
However , the effect remains statistically significant in 5 of 7 languages tested , including the 4 that were also in the original study .
4.3 Replication of McCarthy et al. ( 2020 )
Finally , we turn our attention to McCarthy et al. ’s ( 2020 ) study on the similarity between grammatical gender partitions between languages .
Using information - theoretic measures , they found that closely related languages have more similar gender groupings of core lexical items .
We replicate their experiment on Swadesh lists ( Swadesh , 1955 ) for 10 European languages with different estimators , and find that hierarchical clustering over both mutual ( MI ) and variational information ( VI ) produces the same trees as the original study .
In this case , using NSB , our recommended estimator , results in a reduced estimate of MI ( e.g. Croatian – Slovak : 0.54 with MLE → 0.46 with NSB ) , but significance testing with 1,000 permutations finds the same pairs were statistically significant for both MI and VI regardless of estimator : all pairs of Slavic languages and Romance languages , and Bulgarian – Spanish ( see Figure 2 ) .
Thus , we see a similar result here as in the previous replication .
5 Conclusion This work presents the first empirical study comparing the performance of various entropy estimators for use with natural language distributions .
From experiments on synthetic data ( appendix ) and natural data ( CELEX ) , and two replication studies of recent papers in information - theoretic linguistics , we find that the oft - employed plug - in estimator of entropy can cause misleading results , e.g. , the overestimates of effect sizes seen in both replication studies .
The recommendation of our paper is that researchers should carefully consider their choice of entropy estimator based on data availability and the nature of the underlying distribution .
Ethics Statement
The authors foresee no ethical concerns with the research presented in this paper .
Acknowledgments We thank Adina Williams , Lucas Torroba Hennigen , Tiago Pimentel , and the anonymous reviewers for feedback on the manuscript .
References Evan Archer , Il Memming Park , and Jonathan W. Pillow . 2014 .
Bayesian entropy estimation for countable discrete distributions .
Journal of Machine Learning Research , 15(81):2833–2868 .
R. H. Baayen , R. Piepenbrock , and L. Gulikers .
1995 .
CELEX2 .
Linguistic Data Consortium , Philadelphia .
R. Harald Baayen , Petar Milin , and Michael Ramscar . 2016 .
Frequency in lexical processing .
Aphasiology , 30(11):1174–1220 . 179  Christian Bentz , Dimitrios Alikaniotis , Michael Cysouw , and Ramon Ferrer - i Cancho .
2017 .
The entropy of words — Learnability and expressivity across more than 1000 languages .
Entropy , 19(6):275 . of the 8th Workshop on Cognitive Modeling and Computational Linguistics ( CMCL 2018 ) , pages 10–18 , Salt Lake City , Utah .
Association for Computational Linguistics .
Francis Bond and Ryan Foster .
2013 .
Linking and extending an open multilingual Wordnet .
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( Volume 1 : Long Papers ) , pages 1352–1362 , Sofia , Bulgaria .
Association for Computational Linguistics .
Charles R. Harris , K. Jarrod Millman , Stéfan J. van der Walt , Ralf Gommers , Pauli Virtanen , David Cournapeau , Eric Wieser , Julian Taylor , Sebastian Berg , Nathaniel J. Smith , Robert Kern , Matti Picus , Stephan Hoyer , Marten H. van Kerkwijk , Matthew Brett , Allan Haldane , Jaime Fernández del Río , Mark Wiebe , Pearu Peterson , Pierre Gérard - Marchant , Kevin Sheppard , Tyler Reddy , Warren Weckesser , Hameer Abbasi , Christoph Gohlke , and Travis E. Oliphant . 2020 .
Array programming with NumPy .
Nature , 585(7825):357–362 .
Cristina Bosco , Simonetta Montemagni , and Maria Simi .
2013 .
Converting Italian treebanks : Towards an Italian Stanford dependency treebank .
In Proceedings of the 7th Linguistic Annotation Workshop and Interoperability with Discourse , pages 61–69 , Sofia , Bulgaria .
Association for Computational Linguistics .
Anne Chao and Tsung - Jen Shen .
2003 .
Nonparametric estimation of Shannon ’s index of diversity when there are unseen species in sample .
Environmental and Ecological Statistics , 10(4):429–443 .
E. Colin Cherry , Morris Halle , and Roman Jakobson . 1953 .
Toward the logical description of languages in their phonemic aspect .
Language , pages 34–46 .
Ryan Cotterell , Christo Kirov , Mans Hulden , and Jason Eisner . 2019 .
On the complexity and typology of inflectional morphological systems .
Transactions of the Association for Computational Linguistics , 7:327 – 342 .
Ryan Cotterell , Sabrina J. Mielke , Jason Eisner , and Brian Roark . 2018 .
Are all languages equally hard to language - model ?
In Proceedings of the 2018 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , Volume 2 ( Short Papers ) , pages 536–541 , New Orleans , Louisiana .
Association for Computational Linguistics .
D. G. Horvitz and D. J. Thompson . 1952 .
A generalization of sampling without replacement from a finite universe .
Journal of the American Statistical Association , 47(260):663–685 .
William G. Madow . 1948 .
On the limiting distributions of estimates based on samples from finite universes .
The Annals of Mathematical Statistics , pages 535 – 545 .
Simone Marsili . 2016 .
simomarsili / ndd :
Bayesian entropy estimation in Python - via the NemenmanSchafee - Bialek algorithm .
Arya D. McCarthy , Adina Williams , Shijia Liu , David Yarowsky , and Ryan Cotterell . 2020 .
Measuring the similarity of grammatical gender systems by comparing partitions .
In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing ( EMNLP ) , pages 5664–5675 , Online .
Association for Computational Linguistics .
Holger Diessel .
2017 .
Usage - based linguistics .
In Oxford Research Encyclopedia of Linguistics .
Oxford University Press .
Dagmar Divjak .
2019 .
Frequency in Language : Memory , Attention and Learning .
Cambridge University Press .
Herwig Friedl and Erwin Stampfer . 2002 .
Jackknife resampling .
In Encyclopedia of Environmetrics , volume 2 , pages 1089–1098 .
Wiley Chichester .
I. J. Good .
1953 .
The population frequencies of species and the estimation of population parameters .
Biometrika , 40:237–264 .
Phillip I. Good . 2000 .
Permutation Tests : A Practical Guide to Resampling Methods for Testing Hypotheses , 2nd edition .
Springer .
Adam Goodkind and Klinton Bicknell .
2018 .
Predictive power of word surprisal for reading times is a linear function of language model quality .
In Proceedings Zellig Harris . 1991 .
A Theory of Language and Information : A Mathematical Approach , 1 edition .
Clarendon Press .
Ryan McDonald , Joakim Nivre , Yvonne QuirmbachBrundage , Yoav Goldberg , Dipanjan Das , Kuzman Ganchev , Keith Hall , Slav Petrov , Hao Zhang , Oscar Täckström , Claudia Bedini , Núria Bertomeu Castelló , and Jungmee Lee . 2013 .
Universal Dependency annotation for multilingual parsing .
In Proceedings of the 51st Annual Meeting of the Association for Computational Linguistics ( Volume 2 : Short Papers ) , pages 92–97 , Sofia , Bulgaria .
Association for Computational Linguistics .
Clara Meister , Tiago Pimentel , Patrick Haller , Lena Jäger , Ryan Cotterell , and Roger Levy . 2021 .
Revisiting the Uniform Information Density hypothesis .
In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 963 – 980 , Online and Punta Cana , Dominican Republic .
Association for Computational Linguistics .
George Miller .
1955 .
Note on the bias of information estimates .
In Information Theory in Psychology : Problems and Methods , pages 95–100 .
Free Press , Glencoe , IL .
180  Michael Mitzenmacher .
2004 .
A brief history of generative models for power law and lognormal distributions .
Internet Mathematics , 1(2):226–251 .
Claude E. Shannon . 1951 .
Prediction and entropy of printed English .
Bell System Technical Journal , 30(1):50–64 .
Ilya Nemenman , F. Shafee , and William Bialek . 2002 .
Entropy and inference , revisited .
In Advances in Neural Information Processing Systems , volume 14 .
MIT Press .
Nathaniel J. Smith and Roger Levy .
2013 .
The effect of word predictability on reading time is logarithmic .
Cognition , 128(3):302–319 . Irene Nikkarinen , Tiago Pimentel , Damián Blasi , and Ryan Cotterell . 2021 .
Modeling the unigram distribution .
In Findings of the Association for Computational Linguistics : ACL - IJCNLP 2021 , pages 3721–3729 , Online .
Association for Computational Linguistics .
Sebastian Nowozin . 2015 .
Estimating discrete entropy , part 1 .
Liam Paninski .
2003 .
Estimation of entropy and mutual information .
Neural Computation , 15:1191–1254 .
Steven T. Piantadosi , Harry Tily , and Edward Gibson . 2011 .
Word lengths are optimized for efficient communication .
Proceedings of the National Academy of Sciences , 108(9):3526–3529 .
Tiago Pimentel , Irene Nikkarinen , Kyle Mahowald , Ryan Cotterell , and Damián Blasi .
2021 .
How ( non)optimal is the lexicon ?
In Proceedings of the 2021 Conference of the North American Chapter of the Association for Computational Linguistics : Human Language Technologies , pages 4426–4438 , Online .
Association for Computational Linguistics .
Prokopis Prokopidis , Elina Desipri , Maria Koutsombogera , Harris Papageorgiou , and Stelios Piperidis . 2005 .
Theoretical and practical issues in the construction of a Greek dependency treebank .
In Proceedings of the 4th Workshop on Treebanks and Linguistic Theories ( TLT ) , pages 149–160 .
Prokopis Prokopidis and Haris Papageorgiou .
2017 .
Universal Dependencies for Greek .
In Proceedings of the NoDaLiDa 2017 Workshop on Universal Dependencies ( UDW 2017 ) , pages 102–106 , Gothenburg , Sweden .
Association for Computational Linguistics .
Alexandre Rademaker , Fabricio Chalub , Livy Real , Cláudia Freitas , Eckhard Bick , and Valeria de Paiva .
2017 .
Universal Dependencies for Portuguese .
In Proceedings of the Fourth International Conference on Dependency Linguistics ( Depling 2017 ) , pages 197–206 , Pisa , Italy .
Linköping University Electronic Press .
Neil Rathi , Michael Hahn , and Richard Futrell . 2021 .
An information - theoretic characterization of morphological fusion .
In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing , pages 10115–10120 , Online and Punta Cana , Dominican Republic .
Association for Computational Linguistics .
Claude E. Shannon . 1948 .
A mathematical theory of communication .
The Bell System Technical Journal , 27(3):379–423 .
Otakar Smrž , Viktor Bielický , Iveta Kouřilová , Jakub Kráčmar , Jan Hajič , and Petr Zemánek . 2008 .
Prague Arabic Dependency Treebank : A word on the million words .
In Proceedings of the Workshop on Arabic and Local Languages , pages 16–23 , Marrakech , Morocco .
Morris Swadesh .
1955 .
Towards greater accuracy in lexicostatistic dating .
International Journal of American Linguistics , 21(2):121–137 .
Dima Taji , Nizar Habash , and Daniel Zeman .
2017 .
Universal Dependencies for Arabic .
In Proceedings of the Third Arabic Natural Language Processing Workshop , pages 166–176 , Valencia , Spain .
Association for Computational Linguistics .
Mariona Taulé , M. Antònia Martí , and Marta Recasens . 2008 .
AnCora :
Multilevel annotated corpora for Catalan and Spanish .
In Proceedings of the Sixth International Conference on Language Resources and Evaluation ( LREC’08 ) , Marrakech , Morocco .
European Language Resources Association ( ELRA ) .
Sara Tonelli , Rodolfo Delmonte , and Antonella Bristot . 2008 .
Enriching the venice Italian treebank with dependency and grammatical relations .
In Proceedings of the Sixth International Conference on Language Resources and Evaluation ( LREC’08 ) , Marrakech , Morocco .
European Language Resources Association ( ELRA ) .
John Tukey . 1949 .
Comparing individual means in the analysis of variance .
Biometrics , 5(2):99–114 .
Tim Vieira .
2017 .
Estimating means in a finite universe .
Adina Williams , Ryan Cotterell , Lawrence WolfSonkin , Damián Blasi , and Hanna Wallach . 2021 .
On the relationships between the grammatical genders of inanimate nouns and their co - occurring adjectives and verbs .
Transactions of the Association for Computational Linguistics , 9:139–159 .
David H. Wolpert and David R. Wolf . 1995 .
Estimating functions of probability distributions from a finite set of samples .
Physical Review E , 52(6):6841 .
Alina Wróblewska . 2018 .
Extended and enhanced Polish dependency bank in Universal Dependencies format .
In Proceedings of the Second Workshop on Universal Dependencies ( UDW 2018 ) , pages 173–182 , Brussels , Belgium .
Association for Computational Linguistics .
Shijie Wu , Ryan Cotterell , and Timothy O’Donnell . 2019 .
Morphological irregularity correlates with frequency .
In Proceedings of the 57th Annual Meeting of the Association for Computational Linguistics , 181  pages 5117–5126 , Florence , Italy .
Association for Computational Linguistics .
Samuel Zahl . 1977 .
Jackknifing an index of diversity .
Ecology , 58(4):907–913 .
Zhiyi Zhang .
2012 .
Entropy estimation in Turing ’s perspective .
Neural Computation , 24(5):1368–1389 .
George Kingsley Zipf .
1935 .
The Psycho - Biology of Language .
Houghton - Mifflin , New York , NY , USA .
182  101 2 5 10 100 1000 HT MM JACK CS CS MAB 102 103 WW WW CS CS HT WW WW WW JACK CS 104 101
WW JACK MM WW JACK WW MM JACK CS CS MSE 102 103 WW WW WW JACK HT 104
WW WW WW JACK CS JACK MM MLE WW JACK Table 3 : Estimators with least MAB ( mean absolute bias ) and MSE ( mean squared error ) for various combinations of N and K sampling from symmetric Dirichlet .
The lighter the color the fewer estimators the best estimator was found to be statistically significantly better than .
100 1000 101 MAB 102 103 CS NSB CS HT CS NSB 104 101 MSE 102 103
J J CS CS CS HT CS NSB 104 J J Table 4 : Estimators with least MAB ( mean absolute bias ) and MSE ( mean squared error ) for various combinations of N and K sampling from Zipfian distributions .
A Implementation
The code for each of the entropy estimators is implemented in Python using numpy ( Harris et al. , 2020 ) , except for NSB which was taken from an existing efficient implementation in the ndd module ( Marsili , 2016 ) .
We calculated entropies with base e ( in nats ) .
B Experiments with simulated data In our experiments with simulated data , we explore distributions sampled from a symmetric Dirichlet prior with varying number of classes K and known distributions of Zipfian form with various parameters .
Words in natural languages have a roughly Zipfian distribution , with probability inversely proportional to rank ( Zipf , 1935 ) , and a symmetric Dirichlet distribution is analogous to e.g. POS tag label distributions in natural language .
Thus , studying synthetic data from such distributions as a start is useful .
B.1 Experiment 1 : Symmetric Dirichlet distributions We sample 1 , 000 distributions from a symmetric Dirichlet distribution with variable number of classes K , i.e. with paramater α =
[ α1 , . .
.
, αK ] =
[ 1 , . . .
, 1 ] .
We calculate entropy estimates on different sample sizes N .
Since we know the parameters of the true distribution , we can compare estimates with the true entropy .
We do pairwise comparisons of the MAB and MSE of estimators , using paired permutation tests to establish significance .
Table 3 shows our results , including significance tests .
It is clear that when N ≫ K , all of the estimators have nearly converged to the true value and estimator choice does not matter .
However , in the low - sample regime some estimators are indeed significantly better at approximating the true entropy .
Our results are mixed as to which estimator is best in what context ; the one found to be most frequently significantly better than other estimators was Chao – Shen .
What is clear is that MLE is never the best choice .
B.2 Experiment 2 : Zipfian distributions We sample 1 , 000 finite Zipfian distributions with K classes which obey Zipf ’s law , that the probability of an outcome is inverse proportional to its rank .
The experimental setup is the same as in Experiment 1 .
A Zipfian distribution approximates ( but is not a perfect model of ) the distribution of tokens in natural language text in some languages , including English , which was the basis for the law being proposed .
Compare similar experiments on infinite Zipf distributions by Zhang ( 2012 ) .
Results are in Table 4 .
C Replication of Williams et al. ( 2021 )
We used the following UD treebanks : 183  • Arabic : PADT ( Smrž et al. , 2008 ; Taji et al. , 2017 ) ; • Greek : GDT ( Prokopidis et al. , 2005 ; Prokopidis and Papageorgiou , 2017 ) ; • Italian : ISDT ( Bosco et al. , 2013 ) , VIT ( Tonelli et al. , 2008 ) ; • Polish : PDB ( Wróblewska , 2018 ) ; • Portuguese : GSD ( McDonald et al. , 2013 ) , Bosque ( Rademaker et al. , 2017 ) ; • Spanish :
AnCora ( Taulé et al. , 2008 ) , GSD ( McDonald et al. , 2013 ) .
Additional Figures HT JACK MLE NSB 0.5 0.4 0.3 0.2 0.1 0 hr sk ru bg uk fr ca it es pt MM hr sk ru bg uk fr ca it
es pt MI
hr sk ru bg uk fr ca it es pt Lang2
CS pt es it ca fr uk bg ru sk hr WW pt es
it ca fr uk bg ru sk hr hr sk ru bg uk fr ca it es pt D
Lang1 Figure 2 : Mutual information between the gender partitions of language pairs with various estimators , replicating McCarthy et al. ( 2020 ) .
Estimator bias HT JACK MLE MM NSB WW 1 0 200 150 50 100 200 150 50 100 200 150 50 100 200 150 50 100 200 150 50 100 200 150 50 100 200 150 100 −1 50 Bias ( nats ) CS 2 Samples Figure 3 : The distribution of bias for entropy over several estimators given variable sample size N , sampling from 100 distributions taken from a symmetric Dirichlet prior with K = 100 .
184  Pairwise MAB p - values ( Dirichlet ) K : 2 K : 5 K : 10 K : 100 K :
1000 WW NSB MM MLE JACK HT CS N : 100 MAB 0.75 0.50 WW NSB MM MLE JACK HT CS N : 1000 Estimator 2 N :
10 WW NSB MM MLE JACK HT CS CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW N : 10000 WW NSB MM MLE JACK HT CS
0.25 Estimator 1 Pairwise MSE p - values ( Dirichlet ) K : 2 K : 5 K : 10 K : 100 K :
1000 WW NSB MM MLE JACK HT CS N : 100 MSE 0.75 0.50 WW NSB MM MLE JACK HT CS N : 1000 Estimator 2 N : 10 WW NSB MM MLE JACK
HT CS CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW CS HT JACK MLE MM NSB WW N : 10000 WW NSB MM MLE JACK HT CS 0.25 Estimator 1 Figure 4 : The heatmaps display the p - values calculated between pairs of estimators for mean absolute bias ( MAB ) and mean squared error ( MSE ) for Experiment 1 .
More purple values mean the estimator on the y - axis ( Estimator 2 ) is better than the estimator on the x - axis ( Estimator 1 ) .
Comparisons tend to become non - significant as N increases , since all the estimators gradually converge to the true entropy .
185  E Derivation of the Entropy Estimators Let X = { xk } K k=1 be a finite set .
Let p be a distribution over X .
The entropy of p is defined as def H(p ) = − K X ( 13 ) pk log pk k=1
Given a dataset of N samples D sampled i.i.d .
from p , our goal is to estimate the entropy H(p ) from samples n D from othe true distribution p.
We will denote the count of an item xk as PN c(xk ) =
n=1 1 xk = x e(n ) .
The maximum - likelihood estimate ( MLE ) of p given D is denoted PN 1{ex(n )
= xk } .
The plug - in estimate of H(p ) is defined to be the estimate of H(p ) obtained by plugging the MLE estimate pbMLE directly into the definition of entropy , i.e. , n=1 N b MLE ( D ) = H(b H pMLE ) =
− K X k=1 pbMLE ( xk ) log pbMLE ( xk ) =
− K X c(xk ) k=1 N log c(xk ) N ( 14 )
This section discusses the problems with Eq . ( 14 ) as an estimator and provides detailed derivations of improved estimators found in the literature .
E.1
The Plug - in Estimator is Negatively Biased Proposition 1 .
The MLE entropy estimator in expectation underestimates true entropy , i.e. , " K # X b MLE ( D ) =
E H −b pMLE ( xk ) log pbMLE ( xk ) ≤ H(p ) ( 15 ) k=1 Proof .
The result is a simple consequence of Jensen ’s inequality and some basic manipulations : " K # K X X E −b pMLE ( xk ) log pbMLE ( xk ) = E[−b pMLE ( xk ) log pbMLE ( xk ) ] ( linearity of expectation ) k=1
k=1
K X ≤− = − k=1 K X E[b pMLE ( xk ) ] log E[b pMLE ( xk ) ] p(xk ) log p(xk )
( Jensen ’s inequality ) ( E[b pMLE ( xk ) ]
= p(xk ) )
k=1 ( definition of entropy ) = H(p )
This completes the result .
E.2 Miller – Madow Proposition 2 .
Let p be a categorical distribution over X = { x1 , . . .
, xK } , i.e. , a categorical distribution with support K. Let D be our dataset of size N sampled from p.
Finally , let pbMLE be the maximumlikelihood estimate computed on D. Then , we have  
h
i def b MLE ( D ) = b MLE ( D ) − H(p ) bias H Ep H ( 16 ) = − 
K −1 + o N −1 2N ( 17 ) Proof .
We start by taking a first - order Taylor expansion and take an expectation of both sides .
( Lemma 1 ) b MLE ( D ) = H(b H pMLE , p ) −KL(b pMLE || p ) | { z } cross - entropy 186 ( 18 ) 
h i b MLE ( D ) =
Ep [ H(b Ep H pMLE , p ) ]
− Ep [ KL(b pMLE || p ) ] " K # X = Ep − pbMLE ( xk ) log p(xk )
− Ep [ KL(b pMLE || p ) ] ( expectation ) ( 19 ) ( defn .
H(p , q ) ) ( 20 ) Ep [ b pMLE ( xk ) log p(xk ) ]
− Ep [ KL(b pMLE || p ) ] ( linearity ) ( 21 ) Ep [ b pMLE ( xk ) ] log p(xk )
− Ep [ KL(b pMLE || p ) ] ( algebra ) ( 22 ) p(xk ) log p(xk )
− Ep [ KL(b pMLE || p ) ] ( unbiased ) ( 23 ) ( defn .
of H(p ) ) ( 24 ) k=1
= − = − = − K X k=1 K X k=1 K X k=1 = H(p ) − Ep [ KL(b pMLE || p ) ]
( 25 ) This gives us : i h b MLE ( D ) − H(p ) = −Ep
[ KL(b pMLE || p ) ]
Ep H ( subtract H(p ) ) ( 26 ) ( definition of bias ) ( 27 ) ( above computation ) ( 28 ) ( non - negativity of KL ) ( 29 )
Thus , we may compactly write the bias as :   b bias HMLE ( D ) =
Ep [ H(b pMLE ) ]
− H(p ) = −Ep
[ KL(b pMLE || p ) ]
≤0 Now , we find a simpler expression for the remainder Ep [ KL(b pMLE || p ) ] .
Again , we start with a secondorder Taylor expansion KL(p || q ) = X ∆(x)2 x∈X 2q(x )
+ o ∆(x)2 
( Lemma 2 ) ( 30 ) k ) around the point ∆(x ) = p(x ) − q(x ) .
Define pbMLE ( xk ) = c(x N where c(xk ) is the count of xk in the training set .
We now simplify the first term : " K # " K # X ∆(xk ) 2 X ( b pMLE ( xk ) − p(xk ) ) 2 Ep = Ep ( definition of ∆(xk ) )
( 31 ) 2q(xk ) 2p(xk ) k=1 k=1
"
K c(x ) # X ( k − p(xk ) ) 2 N ( definition of MLE ) ( 32 ) =
Ep 2p(xk )
k=1
" K # X ( c(xk ) − N p(xk ) ) 2 = Ep ( ×N / N ) ( 33 ) 2N 2 p(xk ) k=1
" K # X ( c(xk ) − N p(xk ) ) 2 1 Ep ( pulling out 1/2N ) ( 34 ) = 2N N p(xk ) k=1
  c(xk ) 2 − 2c(xk ) N p(xk )
K X 1 + N 2 p(xk ) 2    = Ep  ( exp .
the binomial ) ( 35 )  2N N p(xk ) k=1
  Ep c(xk )
2 − 2N p(xk )
Ep [ c(xk ) ]
K 1 X + N 2 p(xk ) 2 = 2N N p(xk ) k=1 187 ( lin .
of expect . )
( 36 )  = = 1 2N 1 2N K X k=1
K X k=1 N pk ( 1 − p(xk ) )
+ N 2 p(xk ) 2 − 2N 2 p(xk ) 2
+ N 2 p(xk ) 2 N p(xk ) ( moments of MLE ) ( 37 ) N pk ( 1 − p(xk ) )
N p(xk ) ( 38 ) K 1 X N 2 p(xk ) 2 − 2N 2 p(xk ) 2
+ N 2 p(xk ) 2
+ 2N N p(xk )
k=1
| { z } = 0 K X k N p(x ) ( 1 − p(xk ) )
1  = k 2N N p(x )  k=1 ( 39 ) K 1 X = ( 1 − p(xk ) )
2N 1 = 2N k=1
K X ( algebra ) ( 40 ) K 1 X 1− p(xk )
2N k=1
k=1
| { z } | { z } = K ( algebra ) ( 41 ) = 1 K −1 = 2N ( 42 ) 
Next , we simplify the second term , o ∆(x)2 , in the MLE case :     pMLE ( xk ) − p(xk ) ) 2 Ep o ∆(x)2 = Ep o ( b "  2 !
# c(xk ) =
Ep o − p(xk ) N 
  ( c(xk ) − N p(xk ) ) 2 = Ep o N2    c(xk ) 2 − 2c(xk ) N p(xk ) + N 2 p(xk ) 2 =
Ep o N2   !
Ep c(xk ) 2 − 2c(xk ) N p(xk ) + N 2 p(xk ) 2 = o N2 
 N pk ( 1 − p(xk ) )
+ N 2 p(xk ) 2  − 2N 2 p(xk ) 2
+ N 2 p(xk ) 2   =
o
 
N2 N p(xk ) ( 1 − p(xk ) )
= o N2   p(xk ) ( 1 − p(xk ) )
= o N  −1 = o N   ( definition of ∆ ) ( 43 ) ( definition of MLE ) ( 44 ) ( ×N / N ) ( 45 ) ( 46 ) ( push exp .
through ) ( 47 ) ( 48 ) ( cancel terms ) ( 49 ) ( cancel N in fraction ) ( 50 ) ( ignore constants ) ( 51 )  −1 which is the desired result .
Putting it all together , we get that bias ( H(b pMLE ) )
= − K−1 2N + o N
Interestingly , it can be seen that the negative bias of the MLE gets worse as the number of classes K grows .
Distributions with large K pop up frequently when dealing with natural language .
Corollary 1 .
The plug - in estimator of entropy is consistent .
188   −1 .
Clearly , as N → 0 , we have Proof .
From Proposition 2 , we have bias ( H(b pMLE ) )
= − K−1 2N + o N bias ( H(b pMLE ) )
→ 0 , so the estimator is consistent .
One could also prove consistency through a simple application of the continuous mapping theorem .
Estimator 1 ( Miller – Madow ) .
Let p be a categorical over K categories .
We seek to estimate the entropy H(p ) .
Let D be our dataset of size N sampled from p.
Then , the Miller – Madow estimator of H(p ) is given by def b MM ( D ) = b MLE ( D ) +
K − 1 ( 52 ) H H 2N
The Miller – Madow estimator is biased , however it is consistent .
b MLE ( D ) around the distribution p is given by Lemma 1 .
The the first - order Taylor approximation of H ( 53 ) b MLE ( D ) = H(b
H pMLE , p ) + R(p , pbMLE ) where the remainder R is given by ( 54 ) R(p , pbMLE ) = −KL(b pMLE || p ) Proof .
The result follows from direct computation .
We start by taking the Taylor expansion of H(b pMLE ) around H(p ): b MLE ( D ) = H(p ) + H K X k=1 i  ∂ h H(p ) pbMLE ( xk ) − p(xk ) + R(p , pbMLE ) | { z } ∂p(xk )
( 55 ) remainder
Our first order term can then be rewritten as follows : K X i  ∂ h H(p ) pbMLE ( xk ) − p(xk ) ∂p(xk )
k=1
#
"
K K   X X ∂ = −p(xk′ ) log p(xk′ ) pbMLE ( xk ) − p(xk )
∂p(xk )
′ k=1
k = 1 # " K K   X X ∂ p(xk′ ) log p(xk′ ) pbMLE ( xk ) − p(xk )
= − ∂p(xk ) k=1 k′ = 1 " # K K   X X ∂ = p(xk′ ) log p(xk′ ) p(xk ) − pbMLE ( xk ) ∂p(xk )
′ = = = k=1
K  X K  X k=1 K X k=1
| = ( 57 ) ( linearity ) ( 58 ) ( sign ) ( 59 ) k=1
k = 1 K  X k=1 = ( 56 ) K X k=1
  1 + log p(xk ) p(xk ) − pbMLE ( xk ) ( 60 )  p(xk ) − pbMLE
(
xk )
+
log p(xk ) ( p(xk ) − pbMLE ( xk ) ) ( 61 ) K  X p(xk ) − pbMLE ( xk ) + log p(xk )
( p(xk ) − pbMLE ( xk ) ) ( 62 ) p(xk ) − { z = 1 } K X k=1
K X pbMLE ( xk ) + k=1
| k=1 { z = 1 log p(xk ) ( p(xk ) − pbMLE ( xk ) ) ( distrib . sum ) ( 63 ) ( simplify ) ( 64 ) } log p(xk ) ( p(xk ) − pbMLE ( xk ) ) 189  = K X k=1 log p(xk ) p(xk ) − { z | } −H(p ) K X ( distrib . sum )
log p(xk ) b pMLE ( xk ) ( 65 ) k=1
{ z | H(p , b pMLE ) } ( 66 ) = H(p , pbMLE ) − H(p )
Plugging this back into our Taylor expansion , we get the following : ( 67 )  − H(p )  + H(p , pb ) + R(p , pb ) b MLE ( D ) = H(p )   H MLE MLE  
Now , we see that this implies b MLE ( D ) − H(b R(p , pbMLE )
= H pMLE , p ) = − = − = − = − K X k=1
K X k=1 K X k=1 K X pbMLE ( xk ) log pbMLE ( xk ) + K X ( algebra ) ( 68 ) ( defn . )
( 69 ) ( merge sums ) ( 70 ) ( factor out pbMLE ( xk ) ) ( 71 ) ( log algebra ) ( 72 ) ( defn . )
( 73 ) pbMLE ( xk ) log p(xk ) k=1
( b pMLE ( xk ) log pbMLE ( xk ) − pbMLE ( xk ) log p(xk ) )
pbMLE ( xk ) ( log pbMLE ( xk ) − log p(xk ) )
pbMLE ( xk ) log k=1 pbMLE ( xk ) p(xk ) =
−KL(b pMLE || p ) which is the desired result .
Lemma 2 .
Define ∆(x ) = p(x ) − q(x ) .
The second - order Taylor expansion of KL(p || q ) around ∆(x ) is given by X ∆(x)2
 KL(p || q ) =
+ o ∆(x)2 ( 74 ) 2q(x )
x∈X
Proof .
Now we compute the series expansion of the KL - divergence .
We first make a tricky substitution : p(x ) q(x ) + p(x ) − q(x ) p(x ) − q(x ) ∆(x ) = = 1 + = 1 + q(x ) q(x ) q(x ) q(x ) ( 75 )
Now , we proceed with the derivation : KL(p || q ) = X p(x ) log x∈X p(x ) q(x )   ∆(x ) = ( q(x ) + ∆(x ) ) log 1 + q(x )
x∈X  
X  ∆(x ) ∆(x)2 2 = −
+ o ∆(x ) ( q(x ) + ∆(x ) ) q(x ) 2q(x)2 X ( defn . of KL divergence ) ( 76 ) ( Eq . ( 75 ) ) ( 77 ) ( Taylor expansion ) ( 78 ) x∈X =
X x∈X = X x∈X = X x∈X ∆(x ) −  ∆(x)2 ∆(x)2 ∆(x)3 + − + o ∆(x)2 2 2q(x ) q(x ) 2q(x )
( distribute ) ( 79 ) ∆(x ) −  ∆(x)2 ∆(x)2 + + o ∆(x)2 2q(x ) q(x ) ( defn . of o ) ( 80 ) ∆(x ) +  ∆(x)2 + o ∆(x)2 2q(x ) ( algebra ) ( 81 ) 190  = X ∆(x ) + x∈X
x∈X
| = { z = 0 2q(x )
+ o ∆(x)2 ( split sums )  ( 82 ) } X ∆(x)2 x∈X X ∆(x)2 2q(x )
+ o ∆(x)2 ( 83 )  which is the desired result .
E.3 Jackknife
The jackknife resampling method is used to estimate the bias of an estimator and correct for it , by sampling all subsamples of size N − 1 from the available sample of size N , computing their average for the statistic being estimated .
Generally , this reduces the order of the bias of an estimator from O(N −1 ) to at most O(N −2 ) ( Friedl and Stampfer , 2002 ) .
Estimator 2 ( Jackknife ) .
Let p be a categorical over K categories .
We seek to estimate the entropy H(p ) .
b \n ( D ) be an estimate of the entropy from a sample Let D be our dataset of size N sampled from
p.
Let H th with the n observation held out .
Then , the Jackknife estimator is given by def b JACK ( D ) = b MLE ( D ) − N − 1 H NH N N X b \n H MLE ( D ) ( 84 ) n=1
This estimator is derived from the jackknife - resampled estimate of the bias of the MLE estimator , multiplied by N − 1 . !
N X 1 b JACK ( D ) − H b MLE ( D ) = ( N − 1 ) H b MLE ( D ) − b \n H H ( 85 ) MLE ( D ) N n=1
E.4 Horvitz – Thompson Horvitz and Thompson ( HT ; 1952 ) is a common estimator given a finite universe , which is our case as K is finite .
We omit a derivation a full here as it is well documented in other places ( Vieira , 2017 ) .
However , we note that , in contrast to many applications of HT , the application of HT to entropy estimation results in a biased estimator as the function whose mean we seek to estimate is log p(xk ) , which is dependent on the unknown distribution
p. Estimator 3 ( Horvitz – Thompson ) .
Let p be a categorical over K categories .
We seek to estimate the entropy H(p ) .
Let D be our dataset of size N sampled from p.
Then the Horvitz – Thompson estimator is defined as K X pbMLE ( xk ) log pbMLE ( xk ) def b HHT ( D ) =
− ( 86 ) 1 − ( 1 − pbMLE ( xk ) ) N k=1 where 1 − ( 1 − pbMLE ( xk ) ) N is an estimate of the inclusion probability , i.e. , the probability that xk appears in a random sample D of size N .
We do not know of a simple expression for the bias entropy estimator , but   of the Horvitz – Thompson  one observation is that Ep ( 1 − pbMLE ( xk ) )
N > Ep ( 1 − p(xk ) ) N when N > 1 ( justified by Jensen ’s inequality , since xN , N > 1 is convex over [ 0 , 1 ] ) ; this is an overestimate of the true inclusion probability .
E.5 Chao – Shen
The Chao – Shen estimator builds upon Horvitz – Thompson by noting that that estimator does not correct for underestimation of number of classes K and resulting effect on estimates of p(xk ) ; i.e. 1−(1−b pMLE ( xk ) ) N is always 0 for a class not included in the sample even if the class is present in the true distribution .
We can reweight the sample probabilities to compensate for missing classes using the notion of sample coverage .
191  Definition 1 ( Sample coverage ) .
We define the sample coverage as C= K X k=1 n o p(xk ) 1 xk ∈ D ( 87 ) Definitionally , ( 1 − C ) is then the probability of sampling an xk not observed in the sample Xe .
However , exact computation of Eq . ( 88 ) is impossible as we do not know the true distribution
p.
Thus , Chao and Shen ( 2003 ) fall back on a well - known estimator of C that uses a technique from Good – Turing ( 1953 ) smoothing .
Let f1 be the number of classes with only one observation in the current sample , i.e , the number of singletons , then we can estimate the sample coverage as f1 def b= C 1− N ( 88 )
The Chao – Shen estimator , described below , simply re - scales the MLE estimate of probability pbMLE ( xk ) b
This corrects for the observed underestimation of p ’s entropy by HT .
in the HT estimator by C. Estimator 4 ( Chao – Shen ) .
Let p be a categorical over K categories .
We seek to estimate the entropy b an estimate of sample coverage , be defined H(p ) .
Let D be our dataset of size N sampled from p. Let C , as in Eq . ( 88 ) .
The Chao – Shen estimator is then defined as def b CS ( D ) = H − E.6 K b X b · pbMLE ( xk ) )
C · pbMLE ( xk ) log ( C k=1 ( 89 ) b · pbMLE ( xk ) ) N 1 − ( 1 − C Wolpert – Wolf Fact 1 ( Derivative of an exponent ) .
d a x = xa log x da Fact 2 ( Normalizer of a Dirichlet ) .
The normalizer of a Dirichlet distribution is !
K
QK Z K X Y Γ(αk )  xk − 1 xαk dx = k=1
δ PK Γ α k=1 k=1 k=1 k ( 90 ) ( 91 )
A relatively easy proof of this fact makes use of a Laplace transform .
Estimator 5 ( Wolpert – Wolf ) .
Let p be a categorical over K categories .
We seek to estimate the entropy H(p ) .
Let D be our dataset of size N sampled from p.
Then , the Wolpert – Wolf estimator is given by K   X α ek def b WW ( D | α ) = e+1 − H ψ
A ψ(e αk + 1 ) e
A k=1 def where c(xk ) =
PN n=1 def def e= 1{exn = xk } , and we additionally define αek = c(xk )
+
αk
and A ( 92 ) PK ek .
k=1
α Proposition 3 ( Wolpert – Wolf ) .
The expectation of entropy under a Dirichlet posterior Dirichlet(α ) where parameter α is given by !
Z K K X Y Γ ( A ) def E
[ H(p ) | α ] = H(p ) δ p(xk )
− 1 Q p(xk ) αk −1 dp ( 93 ) Γ(α )
k k=1 k=1
K X = ψ ( A + 1 ) − def where A = k=1 k=1 αk ψ(αk + 1 ) A PK k=1 αk .
192 ( 94 )  Proof .
Let Dirichlet(α1 , . . .
, αK ) be a Dirichlet posterior .
The result follows by a series of manipulations : E [ H(p ) | α ] = K X Z H(p ) δ Γ ( A ) = Q k=1 Γ(αk ) Z Γ ( A ) = Q k=1 Γ(αk ) Z H(p ) δ p(xk ) −
1 k=1 K X k=1 = −Q Γ ( A ) k=1 Γ(αk ) =
−Q Γ ( A ) k=1 Γ(αk ) − K X K !
Q k=1
K Z X p(xk ) − 1 !
K Y K X ! p(xk )
log p(xk )
δ !
K Y p(xk ) − 1 p(xk ) αk −1
dp ( linear . )
( 98 ) k=1 K X p(xk ) log p(xk ) δ p(xk ) αk log p(xk )
δ K Z k=1 !
K Y ( defn .
H ) ( 97 ) k=1
K X p(xk ) − 1 k=1 k=1 !
K Y p(xk ) − 1 p(xj ) αj −1 dp ( fact # 1 ) ( 100 ) !
K K X Y d αk p(xk ) − 1 p(xk )
δ p(xj ) αj −1 dp dαk ( algebra ) ( 101 ) j=1 , j̸=k j=1 , j̸=k k=1 K X d Γ ( A ) =
−Q dαk k=1 Γ(αk ) k=1
Z δ K X k=1 !
p(xk )
− 1 p(xk )
αk K Y p(xj ) αj −1
dp Q Γ(αk + 1 ) K j=1 , Γ(αj )
Γ ( A ) d j̸=k P  = −Q K dα Γ(α )
k k k=1 Γ
α + 1 k=1 j=1 j K
( 102 ) j=1 , j̸=k K X = −Q ( algebra ) ( 99 ) j=1 , j̸=k !
K K X Y d p(xk ) − 1 p(xk ) αk δ p(xj ) αj −1 dp dαk k=1 k=1 ( 96 ) pαk k −1 dp k=1 Z X Γ ( A ) =
−Q k=1 Γ(αk ) p(xk ) αk −1
dp k=1 k=1
X Γ ( A ) =
−Q k=1 Γ(αk ) ( defn . )
( 95 ) k=1 k=1
K Z X K Y Γ ( A ) p(xk ) αk −1 dp Γ(α ) k k=1 ( fact # 2 ) ( 103 )
K XY Γ(α + 1 ) Γ ( A ) d P k 
Γ(αj )
K dαk Γ k=1 Γ(αk ) α +1 k=1 j=1 , j̸=k j=1 ( 104 ) j P 
K ψ(α + 1)Γ(α + 1)Γ
α + 1 j k k j=1 Γ ( A ) =
−Q Γ(αj )
P 2 K k=1 Γ(αk ) k=1
j=1 , Γ j=1 αj + 1 K Y K X ( derivative ) ( 105 ) j̸=k P PK ψ
( K j=1 αj + 1)Γ(αk + 1)Γ
( j=1 αk + 1 ) − 2 P
K Γ
α + 1 j j=1 ψ(αk + 1)Γ(αk + 1 ) P − ψ
( K αj + 1)Γ(αk + 1 ) Γ
( A ) j=1  = −Q Γ(αj )
P K k=1 Γ(αk ) k=1
j=1 , Γ j=1 αj + 1 ( simplify ) ( 106 ) ψ(αk + 1)Γ(αk ) αk P − ψ
( K αj + 1)Γ(αk ) αk Γ ( A ) j=1 
= −Q Γ(αj )
P K k=1 Γ(αk ) k=1
j=1 , Γ α
j=1 j A ( defn .
Γ ) ( 107 )
K Y K X j̸=k K Y K X j̸=k 193  = − = − = − = − K X k=1 K  X k=1
K X k=1 QK k=1 Γ(αk ) ! !
( distrib . )
( 108 ) ( cancel ) ( 109 ) k=1  αk αk ψ(αk + 1 ) − ψ
( A + 1 ) A A ( defn .
A ) ( 110 ) K X αk αk ψ(αk + 1 ) + ψ
( A + 1 ) A A ( distrib . ) ( 111 ) k=1
K X αk k=1
K X K X αk αk ψ(αk + 1 ) − ψ
αk + 1 Γ ( A ) A A k=1 k=1 ! !
K X αk αk ψ(αk + 1 ) − ψ
αk + 1
A A Γ ( A ) =
−Q k=1 Γ(αk ) A ( ψ(αk + 1 ) + ψ
( A + 1 ) = ψ
( A + 1 ) − K X αk k=1
A P ak = A ) ( 112 ) ( rearr . ) ( 113 ) ψ(αk + 1 ) which proves the result .
E.7 Nemenman – Shafee – Bialek Estimator 6 ( Nemenman – Shafee – Bialek ) .
Let p be a categorical over K categories .
We seek to estimate the entropy H(p ) .
Let D be our dataset of size N sampled from p.
Define the NSB density as def pNSB ( α ) = Kψ1 ( Kα + 1 ) −
ψ1 ( α + 1 ) log K ( 114 ) where ψ1 is the trigramma function .
Then , the NSB estimator is given by Z ∞ def b NSB ( D ) = b WW ( D | α · 1 ) pNSB
( α ) dα H H ( 115 ) 0
The integral in Eq . ( 115 ) is typically computed by numerical integration .
To derive the Nemenman – Shafee – Bialek ( NSB ) estimator , we start with the idea that we would like a prior over distributions such that the distribution over expected entropy is uniform .
In other words , we are looking for a pNSB such that for α ∼ pNSB , the values of Ep [ H(p ) | α ] are uniformly distributed over [ 0 , log K ] .
This is a good idea since , a - priori , we do not know entropy of p and , in the absence of any insight , we should assume the entropy could be anywhere in the range [ 0 , log K ] .
We make the above intuition formal with the following proposition .
Proposition 4 .
Let pNSB be the NSB density given in Eq . ( 114 ) .
Then the following conditional expectation def Ep [ H(p ) | α ] = Z H(p ) δ K X k=1 !
p(xk )
− 1 K Γ ( Kα ) Y p(xk )
α−1 dp Γ(α)K ( 116 ) k=1 = ψ
( Kα + 1 ) − ψ(α + 1 ) ( Proposition 3 ) ( 117 ) is uniformly distributed over [ 0 , log K ] when α ∼ pNSB ( · ) , defined in Eq . ( 114 ) .
Proof .
First , we note that Ep [ H(p ) | α ] is a continuous , increasing function in α .
We will not prove this formally , but it should make intuitive sense : α is a smoothing parameter and the more the distribution is smoothed , the more entropic it should be .
From basic analysis , we know that a strictly continuous , increasing function has an inverse .
The above means that we can view Ep [ H(p ) | α ] as a bijection from R≥0 to the interval [ 0 , log K ] .
Our goal is to reparameterize the Uniform distribution in terms of α .
To that end , we 194  def define the function g −1
( α ) =
Ep [ H(p ) | α ] : R≥0 → [ 0 , log K ] and perform a change - of - variables transform on Eq . ( 118 ) using g −1 .
We start with the continuous uniform over [ 0 , log K ] , which is show below def p(H ) = n o 1 1 H ∈
[ 0 , log K ] log K | { z } ( defn .
of uniform dist ) ( 118 ) uniform over [ 0 , log K ]
Note H is a random variable and unrelated to the functional H ( · ) ; the choice of letter intentionally reminds one that the variable represents the expected entropy of under a random distribution .
Now we apply the change - of - variables formula at H = g −1
( α ) and manipulate : p(H ) =
p(g −1 ( α ) ) dg −1 ( α ) dα n o dg −1 1 1 g−1 ( α ) ∈
[ 0 , log K ] ( α ) log K dα dg −1 1 = ( α ) log K
dα 1 dg −1 = ( α ) log K dα Kψ1 ( Kα + 1 ) −
ψ1 ( α + 1 ) = log K = ( change of variable ) ( 119 ) ( definition of p ) ( 120 ) ( redundant indicator ) ( 121 ) ( derivative is positive ) ( 122 ) ( Lemma 3 ) ( 123 ) ( definition ) ( 124 ) def = pNSB ( α ) By construction , the prior pNSB ( α ) has the property that the expected entropy Ep [ H(p ) | α ] where α ∼ pNSB ( · ) is uniformly distributed over [ 0 , log K ] , which we can see by reversing the above derivation .
This proves the result .
Nemenman et al. ( 2002 ) interpreted Proposition 4 in the following manner : As the variance of Ep [ H(p ) | α ] , which is treated as a random variable since α is random , approaches 0 , then the the NSB estimator implies a uniform prior over the entropy .
Lemma 3 ( NSB Derivative ) .
d [ ψ(Kα + 1 ) − ψ(α + 1 ) ]
= Kψ1 ( Kα + 1 ) −
ψ1 ( α + 1 ) dα Proof .
The proof follows by a straightforward computation : ( 125 ) d d d [ ψ(Kα + 1 ) − ψ(α + 1 ) ]
= [ ψ(Kα + 1 ) ] −
[ ψ(α + 1 ) ]
dα dα dα = Kψ1 ( Kα + 1 ) −
ψ1
(
α + 1 ) ( linearity ) ( 126 ) ( definition ) ( 127 ) def d where ψ1 ( x ) = dx ψ(x ) .
195 
